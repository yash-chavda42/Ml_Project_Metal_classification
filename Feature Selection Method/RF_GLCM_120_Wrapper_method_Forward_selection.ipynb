{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdXu_EJSWcpK"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage.filters import sobel\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from skimage.measure import shannon_entropy\n",
        "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxN_21vgaBAg"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbxjlC4taCOk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEi-Muz5aEIe",
        "outputId": "dff47882-ca85-4cb5-ae8b-fba7376d2479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cgxS4uWaFpQ",
        "outputId": "e2ce9fa3-1f21-4b0a-bf22-b63b4d5b3ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml/Augmented_dataset\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/\"MyDrive/ml/Augmented_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXqCaAloaMuE",
        "outputId": "f0b7a291-4f7f-43b5-d705-3d297bdd2612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "500\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "image = list()\n",
        "classes = list()\n",
        "count=0\n",
        "path = \"0/*.jpg\"\n",
        "for file in glob.glob(path):     \n",
        "  img=cv2.imread(file)\n",
        "  img2=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  if(count>=500):\n",
        "     break\n",
        "  image.append(img2)#append all the image in imge\n",
        "  classes.append(0)#append all the class in classes\n",
        "  count+=1\n",
        "\n",
        "print(len(image))\n",
        "print(len(classes))\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP2QCu-Smgwc",
        "outputId": "bdb3aef6-4d1e-49d7-c362-a42fabb0bc24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "count=0\n",
        "path = \"1/*.jpg\"\n",
        "for file in glob.glob(path):     \n",
        "  img=cv2.imread(file)\n",
        "  img2=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  if(count>=500):\n",
        "     break\n",
        "  image.append(img2)#append all the image in imge\n",
        "  classes.append(1)#append all the class in classes\n",
        "  count+=1\n",
        "print(len(image))\n",
        "print(len(classes))\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTNYkKDTmgyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da112116-7fe5-4288-cb27-2eea11f7f76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "1500\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "count=0\n",
        "path = \"2/*.jpg\"\n",
        "for file in glob.glob(path):     \n",
        "  img=cv2.imread(file)\n",
        "  img2=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  if(count>=500):\n",
        "     break\n",
        "  image.append(img2)#append all the image in imge\n",
        "  classes.append(2)#append all the class in classes\n",
        "  count+=1\n",
        "print(len(image))\n",
        "print(len(classes))\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15AeexyKmloV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58527768-40c5-4279-dc00-ab7a78cb2cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "2000\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "count=0\n",
        "path = \"3/*.jpg\"\n",
        "for file in glob.glob(path):     \n",
        "  img=cv2.imread(file)\n",
        "  img2=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  if(count>=500):\n",
        "     break\n",
        "  image.append(img2)#append all the image in imge\n",
        "  classes.append(3)#append all the class in classes\n",
        "  count+=1\n",
        "print(len(image))\n",
        "print(len(classes))\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoYkZoOkoLp-"
      },
      "outputs": [],
      "source": [
        "image=np.array(image)\n",
        "classes=np.array(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmzocvW0LuQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7322f6-9534-4b49-c51e-b3c53338750a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 256, 1600)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q21eqJxjaTPU"
      },
      "outputs": [],
      "source": [
        "# classes=list()\n",
        "# for i in range(0,4) :\n",
        "#     for j in range(0,245):\n",
        "#         classes.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0oHxcZIfDUN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on6h3s7ObE1J"
      },
      "outputs": [],
      "source": [
        "def feature_extractor(dataset):\n",
        "    image_dataset = pd.DataFrame()\n",
        "    for image in range(dataset.shape[0]):  #iterate through each file \n",
        "        #print(image)\n",
        "        \n",
        "        df = pd.DataFrame()  #Temporary data frame to capture information for each loop.\n",
        "        #Reset dataframe to blank after each loop.\n",
        "        \n",
        "        img = dataset[image, :,:]\n",
        "    ################################################################\n",
        "    #START ADDING DATA TO THE DATAFRAME\n",
        "  \n",
        "                \n",
        "         #Full image\n",
        "        #GLCM = greycomatrix(img, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4])\n",
        "        GLCM = greycomatrix(img, [1], [0])       \n",
        "        GLCM_Energy = greycoprops(GLCM, 'energy')[0]\n",
        "        df['Energy'] = GLCM_Energy\n",
        "        GLCM_corr = greycoprops(GLCM, 'correlation')[0]\n",
        "        df['Corr'] = GLCM_corr       \n",
        "        GLCM_diss = greycoprops(GLCM, 'dissimilarity')[0]\n",
        "        df['Diss_sim'] = GLCM_diss       \n",
        "        GLCM_hom = greycoprops(GLCM, 'homogeneity')[0]\n",
        "        df['Homogen'] = GLCM_hom       \n",
        "        GLCM_contr = greycoprops(GLCM, 'contrast')[0]\n",
        "        df['Contrast'] = GLCM_contr\n",
        "\n",
        "        # GLCM = greycomatrix(img, [1], np.pi/4)#0,pi/4,pi/2,3pi/4,pi,5pi/4,3pi/2,7pi/4       \n",
        "        # GLCM_Energy = greycoprops(GLCM, 'energy')[0]\n",
        "        # df['Energy'] = GLCM_Energy\n",
        "        # GLCM_corr = greycoprops(GLCM, 'correlation')[0]\n",
        "        # df['Corr'] = GLCM_corr       \n",
        "        # GLCM_diss = greycoprops(GLCM, 'dissimilarity')[0]\n",
        "        # df['Diss_sim'] = GLCM_diss       \n",
        "        # GLCM_hom = greycoprops(GLCM, 'homogeneity')[0]\n",
        "        # df['Homogen'] = GLCM_hom       \n",
        "        # GLCM_contr = greycoprops(GLCM, 'contrast')[0]\n",
        "        # df['Contrast'] = GLCM_contr\n",
        "\n",
        "        GLCM2 = greycomatrix(img, [1], [np.pi/4])       \n",
        "        GLCM_Energy2 = greycoprops(GLCM2, 'energy')[0]\n",
        "        df['Energy2'] = GLCM_Energy2\n",
        "        GLCM_corr2 = greycoprops(GLCM2, 'correlation')[0]\n",
        "        df['Corr2'] = GLCM_corr2       \n",
        "        GLCM_diss2 = greycoprops(GLCM2, 'dissimilarity')[0]\n",
        "        df['Diss_sim2'] = GLCM_diss2       \n",
        "        GLCM_hom2 = greycoprops(GLCM2, 'homogeneity')[0]\n",
        "        df['Homogen2'] = GLCM_hom2       \n",
        "        GLCM_contr2 = greycoprops(GLCM2, 'contrast')[0]\n",
        "        df['Contrast2'] = GLCM_contr2\n",
        "\n",
        "        GLCM3 = greycomatrix(img, [1], [np.pi/2])       \n",
        "        GLCM_Energy3 = greycoprops(GLCM3, 'energy')[0]\n",
        "        df['Energy3'] = GLCM_Energy3\n",
        "        GLCM_corr3 = greycoprops(GLCM3, 'correlation')[0]\n",
        "        df['Corr3'] = GLCM_corr3       \n",
        "        GLCM_diss3 = greycoprops(GLCM3, 'dissimilarity')[0]\n",
        "        df['Diss_sim3'] = GLCM_diss3       \n",
        "        GLCM_hom3 = greycoprops(GLCM3, 'homogeneity')[0]\n",
        "        df['Homogen3'] = GLCM_hom3       \n",
        "        GLCM_contr3 = greycoprops(GLCM3, 'contrast')[0]\n",
        "        df['Contrast3'] = GLCM_contr3\n",
        "\n",
        "        GLCM4 = greycomatrix(img, [1], [3*np.pi/4])       \n",
        "        GLCM_Energy4 = greycoprops(GLCM4, 'energy')[0]\n",
        "        df['Energy4'] = GLCM_Energy4\n",
        "        GLCM_corr4 = greycoprops(GLCM4, 'correlation')[0]\n",
        "        df['Corr4'] = GLCM_corr4       \n",
        "        GLCM_diss4 = greycoprops(GLCM4, 'dissimilarity')[0]\n",
        "        df['Diss_sim4'] = GLCM_diss4       \n",
        "        GLCM_hom4 = greycoprops(GLCM4, 'homogeneity')[0]\n",
        "        df['Homogen4'] = GLCM_hom4       \n",
        "        GLCM_contr4 = greycoprops(GLCM4, 'contrast')[0]\n",
        "        df['Contrast4'] = GLCM_contr4\n",
        "        \n",
        "        GLCM5 = greycomatrix(img, [1], [np.pi])       \n",
        "        GLCM_Energy5 = greycoprops(GLCM5, 'energy')[0]\n",
        "        df['Energy5'] = GLCM_Energy5\n",
        "        GLCM_corr5 = greycoprops(GLCM5, 'correlation')[0]\n",
        "        df['Corr5'] = GLCM_corr5       \n",
        "        GLCM_diss5 = greycoprops(GLCM5, 'dissimilarity')[0]\n",
        "        df['Diss_sim5'] = GLCM_diss5       \n",
        "        GLCM_hom5 = greycoprops(GLCM5, 'homogeneity')[0]\n",
        "        df['Homogen5'] = GLCM_hom5       \n",
        "        GLCM_contr5 = greycoprops(GLCM5, 'contrast')[0]\n",
        "        df['Contrast5'] = GLCM_contr5\n",
        "        \n",
        "        GLCM6 = greycomatrix(img, [1], [5*np.pi/4])       \n",
        "        GLCM_Energy6 = greycoprops(GLCM6, 'energy')[0]\n",
        "        df['Energy6'] = GLCM_Energy6\n",
        "        GLCM_corr6 = greycoprops(GLCM6, 'correlation')[0]\n",
        "        df['Corr6'] = GLCM_corr6       \n",
        "        GLCM_diss6 = greycoprops(GLCM6, 'dissimilarity')[0]\n",
        "        df['Diss_sim6'] = GLCM_diss6       \n",
        "        GLCM_hom6 = greycoprops(GLCM6, 'homogeneity')[0]\n",
        "        df['Homogen6'] = GLCM_hom6    \n",
        "        GLCM_contr6 = greycoprops(GLCM6, 'contrast')[0]\n",
        "        df['Contrast6'] = GLCM_contr6\n",
        "\n",
        "        GLCM7 = greycomatrix(img, [1], [3*np.pi/2])       \n",
        "        GLCM_Energy7 = greycoprops(GLCM7, 'energy')[0]\n",
        "        df['Energy7'] = GLCM_Energy7\n",
        "        GLCM_corr7 = greycoprops(GLCM7, 'correlation')[0]\n",
        "        df['Corr7'] = GLCM_corr7      \n",
        "        GLCM_diss7 = greycoprops(GLCM7, 'dissimilarity')[0]\n",
        "        df['Diss_sim7'] = GLCM_diss7       \n",
        "        GLCM_hom7 = greycoprops(GLCM7, 'homogeneity')[0]\n",
        "        df['Homogen7'] = GLCM_hom7       \n",
        "        GLCM_contr7 = greycoprops(GLCM7, 'contrast')[0]\n",
        "        df['Contrast7'] = GLCM_contr7\n",
        "\n",
        "        GLCM8 = greycomatrix(img, [1], [7*np.pi/4])       \n",
        "        GLCM_Energy8 = greycoprops(GLCM8, 'energy')[0]\n",
        "        df['Energy8'] = GLCM_Energy8\n",
        "        GLCM_corr8 = greycoprops(GLCM8, 'correlation')[0]\n",
        "        df['Corr8'] = GLCM_corr8       \n",
        "        GLCM_diss8 = greycoprops(GLCM8, 'dissimilarity')[0]\n",
        "        df['Diss_sim8'] = GLCM_diss8       \n",
        "        GLCM_hom8 = greycoprops(GLCM8, 'homogeneity')[0]\n",
        "        df['Homogen8'] = GLCM_hom8       \n",
        "        GLCM_contr8 = greycoprops(GLCM8, 'contrast')[0]\n",
        "        df['Contrast8'] = GLCM_contr8\n",
        "\n",
        "        GLCM9 = greycomatrix(img, [3], [0])       \n",
        "        GLCM_Energy9 = greycoprops(GLCM9, 'energy')[0]\n",
        "        df['Energy9'] = GLCM_Energy9\n",
        "        GLCM_corr9 = greycoprops(GLCM9, 'correlation')[0]\n",
        "        df['Corr9'] = GLCM_corr9       \n",
        "        GLCM_diss9 = greycoprops(GLCM9, 'dissimilarity')[0]\n",
        "        df['Diss_sim9'] = GLCM_diss9       \n",
        "        GLCM_hom9 = greycoprops(GLCM9, 'homogeneity')[0]\n",
        "        df['Homogen9'] = GLCM_hom9       \n",
        "        GLCM_contr9 = greycoprops(GLCM9, 'contrast')[0]\n",
        "        df['Contrast9'] = GLCM_contr9\n",
        "\n",
        "        GLCM10 = greycomatrix(img, [3], [np.pi/4])       \n",
        "        GLCM_Energy10 = greycoprops(GLCM10, 'energy')[0]\n",
        "        df['Energy10'] = GLCM_Energy10\n",
        "        GLCM_corr10 = greycoprops(GLCM10, 'correlation')[0]\n",
        "        df['Corr10'] = GLCM_corr10    \n",
        "        GLCM_diss10 = greycoprops(GLCM10, 'dissimilarity')[0]\n",
        "        df['Diss_sim10'] = GLCM_diss10       \n",
        "        GLCM_hom10 = greycoprops(GLCM10, 'homogeneity')[0]\n",
        "        df['Homogen10'] = GLCM_hom10       \n",
        "        GLCM_contr10 = greycoprops(GLCM10, 'contrast')[0]\n",
        "        df['Contrast10'] = GLCM_contr10\n",
        "\n",
        "        GLCM11 = greycomatrix(img, [3], [np.pi/2])       \n",
        "        GLCM_Energy11 = greycoprops(GLCM11, 'energy')[0]\n",
        "        df['Energy11'] = GLCM_Energy11\n",
        "        GLCM_corr11 = greycoprops(GLCM11, 'correlation')[0]\n",
        "        df['Corr11'] = GLCM_corr11   \n",
        "        GLCM_diss11 = greycoprops(GLCM11, 'dissimilarity')[0]\n",
        "        df['Diss_sim11'] = GLCM_diss11       \n",
        "        GLCM_hom11 = greycoprops(GLCM11, 'homogeneity')[0]\n",
        "        df['Homogen11'] = GLCM_hom11       \n",
        "        GLCM_contr11 = greycoprops(GLCM11, 'contrast')[0]\n",
        "        df['Contrast11'] = GLCM_contr11\n",
        "\n",
        "        GLCM12 = greycomatrix(img, [3], [3*np.pi/4])       \n",
        "        GLCM_Energy12 = greycoprops(GLCM12, 'energy')[0]\n",
        "        df['Energy12'] = GLCM_Energy12\n",
        "        GLCM_corr12 = greycoprops(GLCM12, 'correlation')[0]\n",
        "        df['Corr12'] = GLCM_corr12       \n",
        "        GLCM_diss12 = greycoprops(GLCM12, 'dissimilarity')[0]\n",
        "        df['Diss_sim12'] = GLCM_diss12       \n",
        "        GLCM_hom12 = greycoprops(GLCM12, 'homogeneity')[0]\n",
        "        df['Homogen12'] = GLCM_hom12   \n",
        "        GLCM_contr12 = greycoprops(GLCM12, 'contrast')[0]\n",
        "        df['Contrast12'] = GLCM_contr12\n",
        "        \n",
        "        GLCM13 = greycomatrix(img, [3], [np.pi])       \n",
        "        GLCM_Energy13 = greycoprops(GLCM13, 'energy')[0]\n",
        "        df['Energy13'] = GLCM_Energy13\n",
        "        GLCM_corr13 = greycoprops(GLCM13, 'correlation')[0]\n",
        "        df['Corr13'] = GLCM_corr13\n",
        "        GLCM_diss13 = greycoprops(GLCM13, 'dissimilarity')[0]\n",
        "        df['Diss_sim13'] = GLCM_diss13       \n",
        "        GLCM_hom13 = greycoprops(GLCM13, 'homogeneity')[0]\n",
        "        df['Homogen13'] = GLCM_hom13   \n",
        "        GLCM_contr13 = greycoprops(GLCM13, 'contrast')[0]\n",
        "        df['Contrast13'] = GLCM_contr13\n",
        "        \n",
        "        GLCM14 = greycomatrix(img, [3], [5*np.pi/4])       \n",
        "        GLCM_Energy14 = greycoprops(GLCM14, 'energy')[0]\n",
        "        df['Energy14'] = GLCM_Energy14\n",
        "        GLCM_corr14 = greycoprops(GLCM14, 'correlation')[0]\n",
        "        df['Corr14'] = GLCM_corr14\n",
        "        GLCM_diss14 = greycoprops(GLCM14, 'dissimilarity')[0]\n",
        "        df['Diss_sim14'] = GLCM_diss14       \n",
        "        GLCM_hom14 = greycoprops(GLCM14, 'homogeneity')[0]\n",
        "        df['Homogen14'] = GLCM_hom14 \n",
        "        GLCM_contr14 = greycoprops(GLCM14, 'contrast')[0]\n",
        "        df['Contrast14'] = GLCM_contr14\n",
        "\n",
        "        GLCM15 = greycomatrix(img, [3], [3*np.pi/2])       \n",
        "        GLCM_Energy15 = greycoprops(GLCM15, 'energy')[0]\n",
        "        df['Energy15'] = GLCM_Energy15\n",
        "        GLCM_corr15 = greycoprops(GLCM15, 'correlation')[0]\n",
        "        df['Corr15'] = GLCM_corr15\n",
        "        GLCM_diss15 = greycoprops(GLCM15, 'dissimilarity')[0]\n",
        "        df['Diss_sim15'] = GLCM_diss15     \n",
        "        GLCM_hom15 = greycoprops(GLCM15, 'homogeneity')[0]\n",
        "        df['Homogen15'] = GLCM_hom15       \n",
        "        GLCM_contr15 = greycoprops(GLCM15, 'contrast')[0]\n",
        "        df['Contrast15'] = GLCM_contr15\n",
        "\n",
        "        GLCM16 = greycomatrix(img, [3], [7*np.pi/4])       \n",
        "        GLCM_Energy16 = greycoprops(GLCM16, 'energy')[0]\n",
        "        df['Energy16'] = GLCM_Energy16\n",
        "        GLCM_corr16 = greycoprops(GLCM16, 'correlation')[0]\n",
        "        df['Corr16'] = GLCM_corr16\n",
        "        GLCM_diss16 = greycoprops(GLCM16, 'dissimilarity')[0]\n",
        "        df['Diss_sim16'] = GLCM_diss16     \n",
        "        GLCM_hom16 = greycoprops(GLCM16, 'homogeneity')[0]\n",
        "        df['Homogen16'] = GLCM_hom16 \n",
        "        GLCM_contr16 = greycoprops(GLCM16, 'contrast')[0]\n",
        "        df['Contrast16'] = GLCM_contr16\n",
        "\n",
        "        GLCM17 = greycomatrix(img, [5], [0])       \n",
        "        GLCM_Energy17 = greycoprops(GLCM17, 'energy')[0]\n",
        "        df['Energy17'] = GLCM_Energy17\n",
        "        GLCM_corr17 = greycoprops(GLCM17, 'correlation')[0]\n",
        "        df['Corr17'] = GLCM_corr17 \n",
        "        GLCM_diss17 = greycoprops(GLCM17, 'dissimilarity')[0]\n",
        "        df['Diss_sim17'] = GLCM_diss17       \n",
        "        GLCM_hom17 = greycoprops(GLCM17, 'homogeneity')[0]\n",
        "        df['Homogen17'] = GLCM_hom17   \n",
        "        GLCM_contr17 = greycoprops(GLCM17, 'contrast')[0]\n",
        "        df['Contrast17'] = GLCM_contr17\n",
        "\n",
        "        GLCM18 = greycomatrix(img, [5], [np.pi/4])       \n",
        "        GLCM_Energy18 = greycoprops(GLCM18, 'energy')[0]\n",
        "        df['Energy18'] = GLCM_Energy18\n",
        "        GLCM_corr18 = greycoprops(GLCM18, 'correlation')[0]\n",
        "        df['Corr18'] = GLCM_corr18\n",
        "        GLCM_diss18 = greycoprops(GLCM18, 'dissimilarity')[0]\n",
        "        df['Diss_sim18'] = GLCM_diss18     \n",
        "        GLCM_hom18 = greycoprops(GLCM18, 'homogeneity')[0]\n",
        "        df['Homogen18'] = GLCM_hom18 \n",
        "        GLCM_contr18 = greycoprops(GLCM18, 'contrast')[0]\n",
        "        df['Contrast18'] = GLCM_contr18\n",
        "\n",
        "        \n",
        "        GLCM19 = greycomatrix(img, [5], [3*np.pi/4])       \n",
        "        GLCM_Energy19 = greycoprops(GLCM19, 'energy')[0]\n",
        "        df['Energy19'] = GLCM_Energy19\n",
        "        GLCM_corr19 = greycoprops(GLCM19, 'correlation')[0]\n",
        "        df['Corr19'] = GLCM_corr19\n",
        "        GLCM_diss19 = greycoprops(GLCM19, 'dissimilarity')[0]\n",
        "        df['Diss_sim19'] = GLCM_diss19       \n",
        "        GLCM_hom19 = greycoprops(GLCM19, 'homogeneity')[0]\n",
        "        df['Homogen19'] = GLCM_hom19   \n",
        "        GLCM_contr19 = greycoprops(GLCM19, 'contrast')[0]\n",
        "        df['Contrast19'] = GLCM_contr19\n",
        "        \n",
        "        GLCM20 = greycomatrix(img, [5], [np.pi])       \n",
        "        GLCM_Energy20 = greycoprops(GLCM20, 'energy')[0]\n",
        "        df['Energy20'] = GLCM_Energy20\n",
        "        GLCM_corr20 = greycoprops(GLCM20, 'correlation')[0]\n",
        "        df['Corr20'] = GLCM_corr20\n",
        "        GLCM_diss20 = greycoprops(GLCM20, 'dissimilarity')[0]\n",
        "        df['Diss_sim20'] = GLCM_diss20     \n",
        "        GLCM_hom20 = greycoprops(GLCM20, 'homogeneity')[0]\n",
        "        df['Homogen20'] = GLCM_hom20 \n",
        "        GLCM_contr20 = greycoprops(GLCM20, 'contrast')[0]\n",
        "        df['Contrast20'] = GLCM_contr20\n",
        "        \n",
        "        GLCM21 = greycomatrix(img, [5], [5*np.pi/4])       \n",
        "        GLCM_Energy21 = greycoprops(GLCM21, 'energy')[0]\n",
        "        df['Energy21'] = GLCM_Energy21\n",
        "        GLCM_corr21 = greycoprops(GLCM21, 'correlation')[0]\n",
        "        df['Corr21'] = GLCM_corr21\n",
        "        GLCM_diss21 = greycoprops(GLCM21, 'dissimilarity')[0]\n",
        "        df['Diss_sim21'] = GLCM_diss21     \n",
        "        GLCM_hom21 = greycoprops(GLCM21, 'homogeneity')[0]\n",
        "        df['Homogen21'] = GLCM_hom21 \n",
        "        GLCM_contr21 = greycoprops(GLCM21, 'contrast')[0]\n",
        "        df['Contrast21'] = GLCM_contr21\n",
        "\n",
        "        GLCM22 = greycomatrix(img, [5], [3*np.pi/2])       \n",
        "        GLCM_Energy22 = greycoprops(GLCM22, 'energy')[0]\n",
        "        df['Energy22'] = GLCM_Energy22\n",
        "        GLCM_corr22 = greycoprops(GLCM22, 'correlation')[0]\n",
        "        df['Corr22'] = GLCM_corr22\n",
        "        GLCM_diss22 = greycoprops(GLCM22, 'dissimilarity')[0]\n",
        "        df['Diss_sim22'] = GLCM_diss22     \n",
        "        GLCM_hom22 = greycoprops(GLCM22, 'homogeneity')[0]\n",
        "        df['Homogen22'] = GLCM_hom22 \n",
        "        GLCM_contr22 = greycoprops(GLCM22, 'contrast')[0]\n",
        "        df['Contrast22'] = GLCM_contr22\n",
        "\n",
        "        GLCM23 = greycomatrix(img, [5], [7*np.pi/4])       \n",
        "        GLCM_Energy23 = greycoprops(GLCM23, 'energy')[0]\n",
        "        df['Energy23'] = GLCM_Energy23\n",
        "        GLCM_corr23 = greycoprops(GLCM23, 'correlation')[0]\n",
        "        df['Corr23'] = GLCM_corr23  \n",
        "        GLCM_diss23 = greycoprops(GLCM5, 'dissimilarity')[0]\n",
        "        df['Diss_sim23'] = GLCM_diss23     \n",
        "        GLCM_hom23 = greycoprops(GLCM23, 'homogeneity')[0]\n",
        "        df['Homogen23'] = GLCM_hom23   \n",
        "        GLCM_contr23 = greycoprops(GLCM5, 'contrast')[0]\n",
        "        df['Contrast23'] = GLCM_contr23\n",
        "\n",
        "        GLCM24 = greycomatrix(img, [5], [np.pi/2])       \n",
        "        GLCM_Energy24 = greycoprops(GLCM24, 'energy')[0]\n",
        "        df['Energy24'] = GLCM_Energy24\n",
        "        GLCM_corr24 = greycoprops(GLCM24, 'correlation')[0]\n",
        "        df['Corr24'] = GLCM_corr24\n",
        "        GLCM_diss24 = greycoprops(GLCM24, 'dissimilarity')[0]\n",
        "        df['Diss_sim24'] = GLCM_diss24    \n",
        "        GLCM_hom24 = greycoprops(GLCM24, 'homogeneity')[0]\n",
        "        df['Homogen24'] = GLCM_hom24\n",
        "        GLCM_contr24 = greycoprops(GLCM24, 'contrast')[0]\n",
        "        df['Contrast24'] = GLCM_contr24\n",
        "\n",
        "        #Add more filters as needed\n",
        "        #entropy = shannon_entropy(img)\n",
        "        #df['Entropy'] = entropy\n",
        "\n",
        "        \n",
        "        #Append features from current image to the dataset\n",
        "        image_dataset = image_dataset.append(df)\n",
        "        \n",
        "    return image_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmiKUNrEoJz2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYuF4l5favBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7f7676-74a4-42a5-a59e-b857c8979b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:271: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:273: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:277: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:279: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:283: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:285: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:287: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:289: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:299: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:301: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:303: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:307: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:309: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:311: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:313: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:315: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        }
      ],
      "source": [
        "image_features = feature_extractor(image)\n",
        "X_for_ML =image_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFGV25bQIZlg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "e8776f33-0685-4090-ecd7-e43962faf509"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Energy      Corr  Diss_sim   Homogen   Contrast   Energy2     Corr2  \\\n",
              "0   0.019777  0.990054  4.277598  0.247156  36.444905  0.015949  0.978066   \n",
              "0   0.016515  0.975922  6.669503  0.147883  76.860101  0.014711  0.962917   \n",
              "0   0.019613  0.977942  5.333277  0.190679  53.503486  0.016877  0.962830   \n",
              "0   0.032360  0.916951  4.430577  0.232241  45.871854  0.027141  0.850907   \n",
              "0   0.185430  0.997988  2.362763  0.483637  16.949028  0.179079  0.995940   \n",
              "..       ...       ...       ...       ...        ...       ...       ...   \n",
              "0   0.032551  0.983889  2.914644  0.331691  19.504085  0.026984  0.969194   \n",
              "0   0.023281  0.959195  5.437246  0.196526  62.139897  0.020272  0.936261   \n",
              "0   0.017407  0.974226  6.418169  0.154440  71.949698  0.014794  0.954254   \n",
              "0   0.026672  0.959417  4.932778  0.233624  56.668208  0.021573  0.886251   \n",
              "0   0.026288  0.986783  3.517030  0.255693  21.937366  0.022720  0.976691   \n",
              "\n",
              "    Diss_sim2  Homogen2   Contrast2  ...  Energy23    Corr23  Diss_sim23  \\\n",
              "0    6.535472  0.166722   80.384235  ...  0.013144  0.952928    4.277598   \n",
              "0    8.351634  0.119170  118.402612  ...  0.012613  0.929095    6.669503   \n",
              "0    7.083770  0.142722   90.136433  ...  0.015222  0.940995    5.333277   \n",
              "0    6.198573  0.167969   82.380608  ...  0.023766  0.758917    4.430577   \n",
              "0    3.527165  0.409657   34.319278  ...  0.159459  0.984795    2.362763   \n",
              "..        ...       ...         ...  ...       ...       ...         ...   \n",
              "0    4.097218  0.242583   37.290645  ...  0.024601  0.957342    2.914644   \n",
              "0    6.990720  0.152456   97.043208  ...  0.016084  0.756143    5.437246   \n",
              "0    8.743813  0.112510  127.676489  ...  0.013670  0.935901    6.418169   \n",
              "0    8.004159  0.161809  158.685423  ...  0.017104  0.682753    4.932778   \n",
              "0    4.739526  0.195467   38.641901  ...  0.018431  0.935921    3.517030   \n",
              "\n",
              "    Homogen23  Contrast23  Energy24    Corr24  Diss_sim24  Homogen24  \\\n",
              "0    0.116075   36.444905  0.013461  0.958588    9.062958   0.120719   \n",
              "0    0.087605   76.860101  0.013225  0.940476   10.481345   0.097133   \n",
              "0    0.118471   53.503486  0.014601  0.932951    9.543376   0.107447   \n",
              "0    0.128228   45.871854  0.024025  0.765533    7.989064   0.131949   \n",
              "0    0.336759   16.949028  0.164346  0.984211    6.419888   0.341094   \n",
              "..        ...         ...       ...       ...         ...        ...   \n",
              "0    0.200316   19.504085  0.024147  0.948401    5.233098   0.195785   \n",
              "0    0.095861   62.139897  0.015792  0.769672   12.681215   0.092473   \n",
              "0    0.094663   71.949698  0.013657  0.935585   10.387191   0.095386   \n",
              "0    0.102597   56.668208  0.017026  0.664312   13.970127   0.101692   \n",
              "0    0.130058   21.937366  0.018136  0.929569    7.895637   0.127296   \n",
              "\n",
              "    Contrast24  \n",
              "0   152.274537  \n",
              "0   190.141444  \n",
              "0   162.435408  \n",
              "0   129.923675  \n",
              "0   137.554156  \n",
              "..         ...  \n",
              "0    62.479522  \n",
              "0   350.278630  \n",
              "0   179.732166  \n",
              "0   466.521736  \n",
              "0   116.225144  \n",
              "\n",
              "[2000 rows x 120 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7bbb6a47-cbab-4ba1-b2ed-2073c0af768e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Energy</th>\n",
              "      <th>Corr</th>\n",
              "      <th>Diss_sim</th>\n",
              "      <th>Homogen</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>Energy2</th>\n",
              "      <th>Corr2</th>\n",
              "      <th>Diss_sim2</th>\n",
              "      <th>Homogen2</th>\n",
              "      <th>Contrast2</th>\n",
              "      <th>...</th>\n",
              "      <th>Energy23</th>\n",
              "      <th>Corr23</th>\n",
              "      <th>Diss_sim23</th>\n",
              "      <th>Homogen23</th>\n",
              "      <th>Contrast23</th>\n",
              "      <th>Energy24</th>\n",
              "      <th>Corr24</th>\n",
              "      <th>Diss_sim24</th>\n",
              "      <th>Homogen24</th>\n",
              "      <th>Contrast24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.019777</td>\n",
              "      <td>0.990054</td>\n",
              "      <td>4.277598</td>\n",
              "      <td>0.247156</td>\n",
              "      <td>36.444905</td>\n",
              "      <td>0.015949</td>\n",
              "      <td>0.978066</td>\n",
              "      <td>6.535472</td>\n",
              "      <td>0.166722</td>\n",
              "      <td>80.384235</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013144</td>\n",
              "      <td>0.952928</td>\n",
              "      <td>4.277598</td>\n",
              "      <td>0.116075</td>\n",
              "      <td>36.444905</td>\n",
              "      <td>0.013461</td>\n",
              "      <td>0.958588</td>\n",
              "      <td>9.062958</td>\n",
              "      <td>0.120719</td>\n",
              "      <td>152.274537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.016515</td>\n",
              "      <td>0.975922</td>\n",
              "      <td>6.669503</td>\n",
              "      <td>0.147883</td>\n",
              "      <td>76.860101</td>\n",
              "      <td>0.014711</td>\n",
              "      <td>0.962917</td>\n",
              "      <td>8.351634</td>\n",
              "      <td>0.119170</td>\n",
              "      <td>118.402612</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012613</td>\n",
              "      <td>0.929095</td>\n",
              "      <td>6.669503</td>\n",
              "      <td>0.087605</td>\n",
              "      <td>76.860101</td>\n",
              "      <td>0.013225</td>\n",
              "      <td>0.940476</td>\n",
              "      <td>10.481345</td>\n",
              "      <td>0.097133</td>\n",
              "      <td>190.141444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.019613</td>\n",
              "      <td>0.977942</td>\n",
              "      <td>5.333277</td>\n",
              "      <td>0.190679</td>\n",
              "      <td>53.503486</td>\n",
              "      <td>0.016877</td>\n",
              "      <td>0.962830</td>\n",
              "      <td>7.083770</td>\n",
              "      <td>0.142722</td>\n",
              "      <td>90.136433</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015222</td>\n",
              "      <td>0.940995</td>\n",
              "      <td>5.333277</td>\n",
              "      <td>0.118471</td>\n",
              "      <td>53.503486</td>\n",
              "      <td>0.014601</td>\n",
              "      <td>0.932951</td>\n",
              "      <td>9.543376</td>\n",
              "      <td>0.107447</td>\n",
              "      <td>162.435408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.032360</td>\n",
              "      <td>0.916951</td>\n",
              "      <td>4.430577</td>\n",
              "      <td>0.232241</td>\n",
              "      <td>45.871854</td>\n",
              "      <td>0.027141</td>\n",
              "      <td>0.850907</td>\n",
              "      <td>6.198573</td>\n",
              "      <td>0.167969</td>\n",
              "      <td>82.380608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023766</td>\n",
              "      <td>0.758917</td>\n",
              "      <td>4.430577</td>\n",
              "      <td>0.128228</td>\n",
              "      <td>45.871854</td>\n",
              "      <td>0.024025</td>\n",
              "      <td>0.765533</td>\n",
              "      <td>7.989064</td>\n",
              "      <td>0.131949</td>\n",
              "      <td>129.923675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.185430</td>\n",
              "      <td>0.997988</td>\n",
              "      <td>2.362763</td>\n",
              "      <td>0.483637</td>\n",
              "      <td>16.949028</td>\n",
              "      <td>0.179079</td>\n",
              "      <td>0.995940</td>\n",
              "      <td>3.527165</td>\n",
              "      <td>0.409657</td>\n",
              "      <td>34.319278</td>\n",
              "      <td>...</td>\n",
              "      <td>0.159459</td>\n",
              "      <td>0.984795</td>\n",
              "      <td>2.362763</td>\n",
              "      <td>0.336759</td>\n",
              "      <td>16.949028</td>\n",
              "      <td>0.164346</td>\n",
              "      <td>0.984211</td>\n",
              "      <td>6.419888</td>\n",
              "      <td>0.341094</td>\n",
              "      <td>137.554156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.032551</td>\n",
              "      <td>0.983889</td>\n",
              "      <td>2.914644</td>\n",
              "      <td>0.331691</td>\n",
              "      <td>19.504085</td>\n",
              "      <td>0.026984</td>\n",
              "      <td>0.969194</td>\n",
              "      <td>4.097218</td>\n",
              "      <td>0.242583</td>\n",
              "      <td>37.290645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024601</td>\n",
              "      <td>0.957342</td>\n",
              "      <td>2.914644</td>\n",
              "      <td>0.200316</td>\n",
              "      <td>19.504085</td>\n",
              "      <td>0.024147</td>\n",
              "      <td>0.948401</td>\n",
              "      <td>5.233098</td>\n",
              "      <td>0.195785</td>\n",
              "      <td>62.479522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.023281</td>\n",
              "      <td>0.959195</td>\n",
              "      <td>5.437246</td>\n",
              "      <td>0.196526</td>\n",
              "      <td>62.139897</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>0.936261</td>\n",
              "      <td>6.990720</td>\n",
              "      <td>0.152456</td>\n",
              "      <td>97.043208</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016084</td>\n",
              "      <td>0.756143</td>\n",
              "      <td>5.437246</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>62.139897</td>\n",
              "      <td>0.015792</td>\n",
              "      <td>0.769672</td>\n",
              "      <td>12.681215</td>\n",
              "      <td>0.092473</td>\n",
              "      <td>350.278630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.017407</td>\n",
              "      <td>0.974226</td>\n",
              "      <td>6.418169</td>\n",
              "      <td>0.154440</td>\n",
              "      <td>71.949698</td>\n",
              "      <td>0.014794</td>\n",
              "      <td>0.954254</td>\n",
              "      <td>8.743813</td>\n",
              "      <td>0.112510</td>\n",
              "      <td>127.676489</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013670</td>\n",
              "      <td>0.935901</td>\n",
              "      <td>6.418169</td>\n",
              "      <td>0.094663</td>\n",
              "      <td>71.949698</td>\n",
              "      <td>0.013657</td>\n",
              "      <td>0.935585</td>\n",
              "      <td>10.387191</td>\n",
              "      <td>0.095386</td>\n",
              "      <td>179.732166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026672</td>\n",
              "      <td>0.959417</td>\n",
              "      <td>4.932778</td>\n",
              "      <td>0.233624</td>\n",
              "      <td>56.668208</td>\n",
              "      <td>0.021573</td>\n",
              "      <td>0.886251</td>\n",
              "      <td>8.004159</td>\n",
              "      <td>0.161809</td>\n",
              "      <td>158.685423</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017104</td>\n",
              "      <td>0.682753</td>\n",
              "      <td>4.932778</td>\n",
              "      <td>0.102597</td>\n",
              "      <td>56.668208</td>\n",
              "      <td>0.017026</td>\n",
              "      <td>0.664312</td>\n",
              "      <td>13.970127</td>\n",
              "      <td>0.101692</td>\n",
              "      <td>466.521736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026288</td>\n",
              "      <td>0.986783</td>\n",
              "      <td>3.517030</td>\n",
              "      <td>0.255693</td>\n",
              "      <td>21.937366</td>\n",
              "      <td>0.022720</td>\n",
              "      <td>0.976691</td>\n",
              "      <td>4.739526</td>\n",
              "      <td>0.195467</td>\n",
              "      <td>38.641901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018431</td>\n",
              "      <td>0.935921</td>\n",
              "      <td>3.517030</td>\n",
              "      <td>0.130058</td>\n",
              "      <td>21.937366</td>\n",
              "      <td>0.018136</td>\n",
              "      <td>0.929569</td>\n",
              "      <td>7.895637</td>\n",
              "      <td>0.127296</td>\n",
              "      <td>116.225144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 120 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bbb6a47-cbab-4ba1-b2ed-2073c0af768e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7bbb6a47-cbab-4ba1-b2ed-2073c0af768e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7bbb6a47-cbab-4ba1-b2ed-2073c0af768e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_for_ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8jLLFLCS7L"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(X_for_ML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLP3a3UhCxV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "4367fdbc-d5c6-464c-8934-ed5c142800dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Energy      Corr  Diss_sim   Homogen   Contrast   Energy2     Corr2  \\\n",
              "0   0.019777  0.990054  4.277598  0.247156  36.444905  0.015949  0.978066   \n",
              "0   0.016515  0.975922  6.669503  0.147883  76.860101  0.014711  0.962917   \n",
              "0   0.019613  0.977942  5.333277  0.190679  53.503486  0.016877  0.962830   \n",
              "0   0.032360  0.916951  4.430577  0.232241  45.871854  0.027141  0.850907   \n",
              "0   0.185430  0.997988  2.362763  0.483637  16.949028  0.179079  0.995940   \n",
              "..       ...       ...       ...       ...        ...       ...       ...   \n",
              "0   0.032551  0.983889  2.914644  0.331691  19.504085  0.026984  0.969194   \n",
              "0   0.023281  0.959195  5.437246  0.196526  62.139897  0.020272  0.936261   \n",
              "0   0.017407  0.974226  6.418169  0.154440  71.949698  0.014794  0.954254   \n",
              "0   0.026672  0.959417  4.932778  0.233624  56.668208  0.021573  0.886251   \n",
              "0   0.026288  0.986783  3.517030  0.255693  21.937366  0.022720  0.976691   \n",
              "\n",
              "    Diss_sim2  Homogen2   Contrast2  ...    Corr23  Diss_sim23  Homogen23  \\\n",
              "0    6.535472  0.166722   80.384235  ...  0.952928    4.277598   0.116075   \n",
              "0    8.351634  0.119170  118.402612  ...  0.929095    6.669503   0.087605   \n",
              "0    7.083770  0.142722   90.136433  ...  0.940995    5.333277   0.118471   \n",
              "0    6.198573  0.167969   82.380608  ...  0.758917    4.430577   0.128228   \n",
              "0    3.527165  0.409657   34.319278  ...  0.984795    2.362763   0.336759   \n",
              "..        ...       ...         ...  ...       ...         ...        ...   \n",
              "0    4.097218  0.242583   37.290645  ...  0.957342    2.914644   0.200316   \n",
              "0    6.990720  0.152456   97.043208  ...  0.756143    5.437246   0.095861   \n",
              "0    8.743813  0.112510  127.676489  ...  0.935901    6.418169   0.094663   \n",
              "0    8.004159  0.161809  158.685423  ...  0.682753    4.932778   0.102597   \n",
              "0    4.739526  0.195467   38.641901  ...  0.935921    3.517030   0.130058   \n",
              "\n",
              "    Contrast23  Energy24    Corr24  Diss_sim24  Homogen24  Contrast24  target  \n",
              "0    36.444905  0.013461  0.958588    9.062958   0.120719  152.274537       0  \n",
              "0    76.860101  0.013225  0.940476   10.481345   0.097133  190.141444       0  \n",
              "0    53.503486  0.014601  0.932951    9.543376   0.107447  162.435408       0  \n",
              "0    45.871854  0.024025  0.765533    7.989064   0.131949  129.923675       0  \n",
              "0    16.949028  0.164346  0.984211    6.419888   0.341094  137.554156       0  \n",
              "..         ...       ...       ...         ...        ...         ...     ...  \n",
              "0    19.504085  0.024147  0.948401    5.233098   0.195785   62.479522       3  \n",
              "0    62.139897  0.015792  0.769672   12.681215   0.092473  350.278630       3  \n",
              "0    71.949698  0.013657  0.935585   10.387191   0.095386  179.732166       3  \n",
              "0    56.668208  0.017026  0.664312   13.970127   0.101692  466.521736       3  \n",
              "0    21.937366  0.018136  0.929569    7.895637   0.127296  116.225144       3  \n",
              "\n",
              "[2000 rows x 121 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3010cf78-7768-4291-87e6-502a54730a7a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Energy</th>\n",
              "      <th>Corr</th>\n",
              "      <th>Diss_sim</th>\n",
              "      <th>Homogen</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>Energy2</th>\n",
              "      <th>Corr2</th>\n",
              "      <th>Diss_sim2</th>\n",
              "      <th>Homogen2</th>\n",
              "      <th>Contrast2</th>\n",
              "      <th>...</th>\n",
              "      <th>Corr23</th>\n",
              "      <th>Diss_sim23</th>\n",
              "      <th>Homogen23</th>\n",
              "      <th>Contrast23</th>\n",
              "      <th>Energy24</th>\n",
              "      <th>Corr24</th>\n",
              "      <th>Diss_sim24</th>\n",
              "      <th>Homogen24</th>\n",
              "      <th>Contrast24</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.019777</td>\n",
              "      <td>0.990054</td>\n",
              "      <td>4.277598</td>\n",
              "      <td>0.247156</td>\n",
              "      <td>36.444905</td>\n",
              "      <td>0.015949</td>\n",
              "      <td>0.978066</td>\n",
              "      <td>6.535472</td>\n",
              "      <td>0.166722</td>\n",
              "      <td>80.384235</td>\n",
              "      <td>...</td>\n",
              "      <td>0.952928</td>\n",
              "      <td>4.277598</td>\n",
              "      <td>0.116075</td>\n",
              "      <td>36.444905</td>\n",
              "      <td>0.013461</td>\n",
              "      <td>0.958588</td>\n",
              "      <td>9.062958</td>\n",
              "      <td>0.120719</td>\n",
              "      <td>152.274537</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.016515</td>\n",
              "      <td>0.975922</td>\n",
              "      <td>6.669503</td>\n",
              "      <td>0.147883</td>\n",
              "      <td>76.860101</td>\n",
              "      <td>0.014711</td>\n",
              "      <td>0.962917</td>\n",
              "      <td>8.351634</td>\n",
              "      <td>0.119170</td>\n",
              "      <td>118.402612</td>\n",
              "      <td>...</td>\n",
              "      <td>0.929095</td>\n",
              "      <td>6.669503</td>\n",
              "      <td>0.087605</td>\n",
              "      <td>76.860101</td>\n",
              "      <td>0.013225</td>\n",
              "      <td>0.940476</td>\n",
              "      <td>10.481345</td>\n",
              "      <td>0.097133</td>\n",
              "      <td>190.141444</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.019613</td>\n",
              "      <td>0.977942</td>\n",
              "      <td>5.333277</td>\n",
              "      <td>0.190679</td>\n",
              "      <td>53.503486</td>\n",
              "      <td>0.016877</td>\n",
              "      <td>0.962830</td>\n",
              "      <td>7.083770</td>\n",
              "      <td>0.142722</td>\n",
              "      <td>90.136433</td>\n",
              "      <td>...</td>\n",
              "      <td>0.940995</td>\n",
              "      <td>5.333277</td>\n",
              "      <td>0.118471</td>\n",
              "      <td>53.503486</td>\n",
              "      <td>0.014601</td>\n",
              "      <td>0.932951</td>\n",
              "      <td>9.543376</td>\n",
              "      <td>0.107447</td>\n",
              "      <td>162.435408</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.032360</td>\n",
              "      <td>0.916951</td>\n",
              "      <td>4.430577</td>\n",
              "      <td>0.232241</td>\n",
              "      <td>45.871854</td>\n",
              "      <td>0.027141</td>\n",
              "      <td>0.850907</td>\n",
              "      <td>6.198573</td>\n",
              "      <td>0.167969</td>\n",
              "      <td>82.380608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.758917</td>\n",
              "      <td>4.430577</td>\n",
              "      <td>0.128228</td>\n",
              "      <td>45.871854</td>\n",
              "      <td>0.024025</td>\n",
              "      <td>0.765533</td>\n",
              "      <td>7.989064</td>\n",
              "      <td>0.131949</td>\n",
              "      <td>129.923675</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.185430</td>\n",
              "      <td>0.997988</td>\n",
              "      <td>2.362763</td>\n",
              "      <td>0.483637</td>\n",
              "      <td>16.949028</td>\n",
              "      <td>0.179079</td>\n",
              "      <td>0.995940</td>\n",
              "      <td>3.527165</td>\n",
              "      <td>0.409657</td>\n",
              "      <td>34.319278</td>\n",
              "      <td>...</td>\n",
              "      <td>0.984795</td>\n",
              "      <td>2.362763</td>\n",
              "      <td>0.336759</td>\n",
              "      <td>16.949028</td>\n",
              "      <td>0.164346</td>\n",
              "      <td>0.984211</td>\n",
              "      <td>6.419888</td>\n",
              "      <td>0.341094</td>\n",
              "      <td>137.554156</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.032551</td>\n",
              "      <td>0.983889</td>\n",
              "      <td>2.914644</td>\n",
              "      <td>0.331691</td>\n",
              "      <td>19.504085</td>\n",
              "      <td>0.026984</td>\n",
              "      <td>0.969194</td>\n",
              "      <td>4.097218</td>\n",
              "      <td>0.242583</td>\n",
              "      <td>37.290645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.957342</td>\n",
              "      <td>2.914644</td>\n",
              "      <td>0.200316</td>\n",
              "      <td>19.504085</td>\n",
              "      <td>0.024147</td>\n",
              "      <td>0.948401</td>\n",
              "      <td>5.233098</td>\n",
              "      <td>0.195785</td>\n",
              "      <td>62.479522</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.023281</td>\n",
              "      <td>0.959195</td>\n",
              "      <td>5.437246</td>\n",
              "      <td>0.196526</td>\n",
              "      <td>62.139897</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>0.936261</td>\n",
              "      <td>6.990720</td>\n",
              "      <td>0.152456</td>\n",
              "      <td>97.043208</td>\n",
              "      <td>...</td>\n",
              "      <td>0.756143</td>\n",
              "      <td>5.437246</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>62.139897</td>\n",
              "      <td>0.015792</td>\n",
              "      <td>0.769672</td>\n",
              "      <td>12.681215</td>\n",
              "      <td>0.092473</td>\n",
              "      <td>350.278630</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.017407</td>\n",
              "      <td>0.974226</td>\n",
              "      <td>6.418169</td>\n",
              "      <td>0.154440</td>\n",
              "      <td>71.949698</td>\n",
              "      <td>0.014794</td>\n",
              "      <td>0.954254</td>\n",
              "      <td>8.743813</td>\n",
              "      <td>0.112510</td>\n",
              "      <td>127.676489</td>\n",
              "      <td>...</td>\n",
              "      <td>0.935901</td>\n",
              "      <td>6.418169</td>\n",
              "      <td>0.094663</td>\n",
              "      <td>71.949698</td>\n",
              "      <td>0.013657</td>\n",
              "      <td>0.935585</td>\n",
              "      <td>10.387191</td>\n",
              "      <td>0.095386</td>\n",
              "      <td>179.732166</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026672</td>\n",
              "      <td>0.959417</td>\n",
              "      <td>4.932778</td>\n",
              "      <td>0.233624</td>\n",
              "      <td>56.668208</td>\n",
              "      <td>0.021573</td>\n",
              "      <td>0.886251</td>\n",
              "      <td>8.004159</td>\n",
              "      <td>0.161809</td>\n",
              "      <td>158.685423</td>\n",
              "      <td>...</td>\n",
              "      <td>0.682753</td>\n",
              "      <td>4.932778</td>\n",
              "      <td>0.102597</td>\n",
              "      <td>56.668208</td>\n",
              "      <td>0.017026</td>\n",
              "      <td>0.664312</td>\n",
              "      <td>13.970127</td>\n",
              "      <td>0.101692</td>\n",
              "      <td>466.521736</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026288</td>\n",
              "      <td>0.986783</td>\n",
              "      <td>3.517030</td>\n",
              "      <td>0.255693</td>\n",
              "      <td>21.937366</td>\n",
              "      <td>0.022720</td>\n",
              "      <td>0.976691</td>\n",
              "      <td>4.739526</td>\n",
              "      <td>0.195467</td>\n",
              "      <td>38.641901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.935921</td>\n",
              "      <td>3.517030</td>\n",
              "      <td>0.130058</td>\n",
              "      <td>21.937366</td>\n",
              "      <td>0.018136</td>\n",
              "      <td>0.929569</td>\n",
              "      <td>7.895637</td>\n",
              "      <td>0.127296</td>\n",
              "      <td>116.225144</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 121 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3010cf78-7768-4291-87e6-502a54730a7a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3010cf78-7768-4291-87e6-502a54730a7a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3010cf78-7768-4291-87e6-502a54730a7a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "df['target'] = classes.tolist()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import sys\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "from sklearn.linear_model import LogisticRegression as LGR\n",
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
        "from mlxtend.plotting import plot_sequential_feature_selection\n"
      ],
      "metadata": {
        "id": "CopW1CEsTGCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "from sklearn.linear_model import LogisticRegression as LGR\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc"
      ],
      "metadata": {
        "id": "0C-taJrsTO6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qte19B0QQXYE"
      },
      "outputs": [],
      "source": [
        "#select the first 13 columns as features\n",
        "x = df.iloc[:,:120]\n",
        "#Select the last column for target \n",
        "y = df.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2PhELMCQear",
        "outputId": "7132119b-8e42-4245-88d3-124e63bc464b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 120), (2000,))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "x.shape,y.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tuple(x.columns)\n",
        "feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V4l_NZaS9vi",
        "outputId": "aade4680-529c-489d-8ede-5c96ed87e5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Energy',\n",
              " 'Corr',\n",
              " 'Diss_sim',\n",
              " 'Homogen',\n",
              " 'Contrast',\n",
              " 'Energy2',\n",
              " 'Corr2',\n",
              " 'Diss_sim2',\n",
              " 'Homogen2',\n",
              " 'Contrast2',\n",
              " 'Energy3',\n",
              " 'Corr3',\n",
              " 'Diss_sim3',\n",
              " 'Homogen3',\n",
              " 'Contrast3',\n",
              " 'Energy4',\n",
              " 'Corr4',\n",
              " 'Diss_sim4',\n",
              " 'Homogen4',\n",
              " 'Contrast4',\n",
              " 'Energy5',\n",
              " 'Corr5',\n",
              " 'Diss_sim5',\n",
              " 'Homogen5',\n",
              " 'Contrast5',\n",
              " 'Energy6',\n",
              " 'Corr6',\n",
              " 'Diss_sim6',\n",
              " 'Homogen6',\n",
              " 'Contrast6',\n",
              " 'Energy7',\n",
              " 'Corr7',\n",
              " 'Diss_sim7',\n",
              " 'Homogen7',\n",
              " 'Contrast7',\n",
              " 'Energy8',\n",
              " 'Corr8',\n",
              " 'Diss_sim8',\n",
              " 'Homogen8',\n",
              " 'Contrast8',\n",
              " 'Energy9',\n",
              " 'Corr9',\n",
              " 'Diss_sim9',\n",
              " 'Homogen9',\n",
              " 'Contrast9',\n",
              " 'Energy10',\n",
              " 'Corr10',\n",
              " 'Diss_sim10',\n",
              " 'Homogen10',\n",
              " 'Contrast10',\n",
              " 'Energy11',\n",
              " 'Corr11',\n",
              " 'Diss_sim11',\n",
              " 'Homogen11',\n",
              " 'Contrast11',\n",
              " 'Energy12',\n",
              " 'Corr12',\n",
              " 'Diss_sim12',\n",
              " 'Homogen12',\n",
              " 'Contrast12',\n",
              " 'Energy13',\n",
              " 'Corr13',\n",
              " 'Diss_sim13',\n",
              " 'Homogen13',\n",
              " 'Contrast13',\n",
              " 'Energy14',\n",
              " 'Corr14',\n",
              " 'Diss_sim14',\n",
              " 'Homogen14',\n",
              " 'Contrast14',\n",
              " 'Energy15',\n",
              " 'Corr15',\n",
              " 'Diss_sim15',\n",
              " 'Homogen15',\n",
              " 'Contrast15',\n",
              " 'Energy16',\n",
              " 'Corr16',\n",
              " 'Diss_sim16',\n",
              " 'Homogen16',\n",
              " 'Contrast16',\n",
              " 'Energy17',\n",
              " 'Corr17',\n",
              " 'Diss_sim17',\n",
              " 'Homogen17',\n",
              " 'Contrast17',\n",
              " 'Energy18',\n",
              " 'Corr18',\n",
              " 'Diss_sim18',\n",
              " 'Homogen18',\n",
              " 'Contrast18',\n",
              " 'Energy19',\n",
              " 'Corr19',\n",
              " 'Diss_sim19',\n",
              " 'Homogen19',\n",
              " 'Contrast19',\n",
              " 'Energy20',\n",
              " 'Corr20',\n",
              " 'Diss_sim20',\n",
              " 'Homogen20',\n",
              " 'Contrast20',\n",
              " 'Energy21',\n",
              " 'Corr21',\n",
              " 'Diss_sim21',\n",
              " 'Homogen21',\n",
              " 'Contrast21',\n",
              " 'Energy22',\n",
              " 'Corr22',\n",
              " 'Diss_sim22',\n",
              " 'Homogen22',\n",
              " 'Contrast22',\n",
              " 'Energy23',\n",
              " 'Corr23',\n",
              " 'Diss_sim23',\n",
              " 'Homogen23',\n",
              " 'Contrast23',\n",
              " 'Energy24',\n",
              " 'Corr24',\n",
              " 'Diss_sim24',\n",
              " 'Homogen24',\n",
              " 'Contrast24')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niRhLWkrayGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a086c5f6-7747-48dd-86cf-8419dac6c539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:   49.9s finished\n",
            "\n",
            "[2022-05-12 10:41:50] Features: 1/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 119 out of 119 | elapsed:   46.5s finished\n",
            "\n",
            "[2022-05-12 10:42:37] Features: 2/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 118 out of 118 | elapsed:   45.1s finished\n",
            "\n",
            "[2022-05-12 10:43:22] Features: 3/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 117 out of 117 | elapsed:  1.0min finished\n",
            "\n",
            "[2022-05-12 10:44:23] Features: 4/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 116 out of 116 | elapsed:   59.7s finished\n",
            "\n",
            "[2022-05-12 10:45:23] Features: 5/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 115 out of 115 | elapsed:   59.0s finished\n",
            "\n",
            "[2022-05-12 10:46:22] Features: 6/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 114 out of 114 | elapsed:   58.6s finished\n",
            "\n",
            "[2022-05-12 10:47:21] Features: 7/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 113 out of 113 | elapsed:   58.2s finished\n",
            "\n",
            "[2022-05-12 10:48:19] Features: 8/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 112 out of 112 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:49:31] Features: 9/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 111 out of 111 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:50:42] Features: 10/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 110 out of 110 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:51:53] Features: 11/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 109 out of 109 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:53:03] Features: 12/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:54:13] Features: 13/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 107 out of 107 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:55:22] Features: 14/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 106 out of 106 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 10:56:32] Features: 15/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 105 out of 105 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 10:57:54] Features: 16/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 104 out of 104 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 10:59:15] Features: 17/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 103 out of 103 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:00:35] Features: 18/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 102 out of 102 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:01:54] Features: 19/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 101 out of 101 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:03:13] Features: 20/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:04:31] Features: 21/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:05:48] Features: 22/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:07:04] Features: 23/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:08:19] Features: 24/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:  1.5min finished\n",
            "\n",
            "[2022-05-12 11:09:47] Features: 25/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:11:13] Features: 26/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:12:38] Features: 27/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:14:02] Features: 28/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:15:26] Features: 29/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:16:49] Features: 30/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:18:11] Features: 31/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:19:32] Features: 32/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:20:51] Features: 33/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:22:10] Features: 34/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:23:29] Features: 35/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed:  1.5min finished\n",
            "\n",
            "[2022-05-12 11:24:57] Features: 36/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:  1.5min finished\n",
            "\n",
            "[2022-05-12 11:26:24] Features: 37/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:27:50] Features: 38/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:29:16] Features: 39/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:30:40] Features: 40/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:32:03] Features: 41/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:33:25] Features: 42/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:34:46] Features: 43/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:36:07] Features: 44/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:37:26] Features: 45/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:38:45] Features: 46/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:40:02] Features: 47/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:41:19] Features: 48/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:42:44] Features: 49/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:44:07] Features: 50/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:  1.4min finished\n",
            "\n",
            "[2022-05-12 11:45:29] Features: 51/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:46:50] Features: 52/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:48:09] Features: 53/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:49:27] Features: 54/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:50:44] Features: 55/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:  1.3min finished\n",
            "\n",
            "[2022-05-12 11:51:59] Features: 56/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 11:53:14] Features: 57/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 11:54:27] Features: 58/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 11:55:39] Features: 59/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 11:56:50] Features: 60/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 11:58:00] Features: 61/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 11:59:08] Features: 62/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 12:00:15] Features: 63/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 12:01:28] Features: 64/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 12:02:40] Features: 65/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 12:03:51] Features: 66/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:  1.2min finished\n",
            "\n",
            "[2022-05-12 12:05:01] Features: 67/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 12:06:09] Features: 68/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 12:07:16] Features: 69/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 12:08:22] Features: 70/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 12:09:26] Features: 71/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:  1.1min finished\n",
            "\n",
            "[2022-05-12 12:10:29] Features: 72/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  1.0min finished\n",
            "\n",
            "[2022-05-12 12:11:31] Features: 73/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:  1.0min finished\n",
            "\n",
            "[2022-05-12 12:12:32] Features: 74/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:   59.1s finished\n",
            "\n",
            "[2022-05-12 12:13:31] Features: 75/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   58.0s finished\n",
            "\n",
            "[2022-05-12 12:14:29] Features: 76/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:   56.5s finished\n",
            "\n",
            "[2022-05-12 12:15:25] Features: 77/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:   55.5s finished\n",
            "\n",
            "[2022-05-12 12:16:21] Features: 78/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   53.9s finished\n",
            "\n",
            "[2022-05-12 12:17:15] Features: 79/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:   52.7s finished\n",
            "\n",
            "[2022-05-12 12:18:07] Features: 80/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   56.6s finished\n",
            "\n",
            "[2022-05-12 12:19:04] Features: 81/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:   55.1s finished\n",
            "\n",
            "[2022-05-12 12:19:59] Features: 82/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   53.7s finished\n",
            "\n",
            "[2022-05-12 12:20:53] Features: 83/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:   52.3s finished\n",
            "\n",
            "[2022-05-12 12:21:45] Features: 84/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   50.9s finished\n",
            "\n",
            "[2022-05-12 12:22:36] Features: 85/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:   49.7s finished\n",
            "\n",
            "[2022-05-12 12:23:26] Features: 86/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   48.3s finished\n",
            "\n",
            "[2022-05-12 12:24:14] Features: 87/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   46.9s finished\n",
            "\n",
            "[2022-05-12 12:25:01] Features: 88/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   45.7s finished\n",
            "\n",
            "[2022-05-12 12:25:47] Features: 89/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:   44.0s finished\n",
            "\n",
            "[2022-05-12 12:26:31] Features: 90/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   42.6s finished\n",
            "\n",
            "[2022-05-12 12:27:13] Features: 91/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:   41.1s finished\n",
            "\n",
            "[2022-05-12 12:27:54] Features: 92/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:   39.9s finished\n",
            "\n",
            "[2022-05-12 12:28:34] Features: 93/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   38.4s finished\n",
            "\n",
            "[2022-05-12 12:29:13] Features: 94/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:   36.9s finished\n",
            "\n",
            "[2022-05-12 12:29:49] Features: 95/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   35.4s finished\n",
            "\n",
            "[2022-05-12 12:30:25] Features: 96/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   33.9s finished\n",
            "\n",
            "[2022-05-12 12:30:59] Features: 97/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   32.6s finished\n",
            "\n",
            "[2022-05-12 12:31:31] Features: 98/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   31.2s finished\n",
            "\n",
            "[2022-05-12 12:32:03] Features: 99/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   32.4s finished\n",
            "\n",
            "[2022-05-12 12:32:35] Features: 100/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   30.8s finished\n",
            "\n",
            "[2022-05-12 12:33:06] Features: 101/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   29.4s finished\n",
            "\n",
            "[2022-05-12 12:33:35] Features: 102/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   27.9s finished\n",
            "\n",
            "[2022-05-12 12:34:03] Features: 103/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:   26.3s finished\n",
            "\n",
            "[2022-05-12 12:34:29] Features: 104/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:   24.7s finished\n",
            "\n",
            "[2022-05-12 12:34:54] Features: 105/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   23.2s finished\n",
            "\n",
            "[2022-05-12 12:35:17] Features: 106/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:   21.6s finished\n",
            "\n",
            "[2022-05-12 12:35:39] Features: 107/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   20.1s finished\n",
            "\n",
            "[2022-05-12 12:35:59] Features: 108/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   18.6s finished\n",
            "\n",
            "[2022-05-12 12:36:18] Features: 109/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   17.0s finished\n",
            "\n",
            "[2022-05-12 12:36:35] Features: 110/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   15.4s finished\n",
            "\n",
            "[2022-05-12 12:36:50] Features: 111/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   13.9s finished\n",
            "\n",
            "[2022-05-12 12:37:04] Features: 112/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   12.3s finished\n",
            "\n",
            "[2022-05-12 12:37:16] Features: 113/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   10.8s finished\n",
            "\n",
            "[2022-05-12 12:37:27] Features: 114/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    9.3s finished\n",
            "\n",
            "[2022-05-12 12:37:36] Features: 115/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.8s finished\n",
            "\n",
            "[2022-05-12 12:37:44] Features: 116/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    6.2s finished\n",
            "\n",
            "[2022-05-12 12:37:50] Features: 117/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.6s finished\n",
            "\n",
            "[2022-05-12 12:37:55] Features: 118/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.1s finished\n",
            "\n",
            "[2022-05-12 12:37:58] Features: 119/120 -- score: 1.0[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n",
            "\n",
            "[2022-05-12 12:38:00] Features: 120/120 -- score: 1.0"
          ]
        }
      ],
      "source": [
        "sfs1 = SFS(#knn(n_neighbors=3),\n",
        "           #rfc(n_jobs=8),\n",
        "           rfc(),\n",
        "           k_features='best', \n",
        "           forward=True, \n",
        "           floating=False, \n",
        "           verbose=2,\n",
        "           #scoring = 'neg_mean_squared_error',  # sklearn regressors\n",
        "           scoring='accuracy',  # sklearn classifiers\n",
        "           cv=0)\n",
        "\n",
        "sfs1 = sfs1.fit(x, y,custom_feature_names=feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCvu1PAwmHtL"
      },
      "outputs": [],
      "source": [
        "sfs1.subsets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPR-sOZpccfb"
      },
      "outputs": [],
      "source": [
        "sfs1.get_metric_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "0g_gCpBAm7d0",
        "outputId": "99d882fc-6acc-43d0-a9b1-0615d3d0ceeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims, where=where)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "fig1 = plot_sfs(sfs1.get_metric_dict(confidence_interval=0.95), kind='std_err')\n",
        "\n",
        "plt.title('Sequential Forward Selection')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets access the indices of the best features directly via the k_feature_idx_ attribute:\n",
        "sfs1.k_feature_names_, sfs1.k_feature_idx_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qfnh36wXGDM",
        "outputId": "e467067c-f939-4a4f-87ba-21031580fff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Homogen',), (3,))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame.from_dict(sfs1.get_metric_dict()).T\n",
        "#print(df[[\"feature_idx\",\"avg_score\"]])\n",
        "names = df1['avg_score'].tolist()\n",
        "print(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peZsmkkAXJsy",
        "outputId": "38ea4f7c-fffe-4750-da7e-9c1ec41fad81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims, where=where)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:,3:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "M-XPkNvmYs86",
        "outputId": "bb646e2d-f1ba-4c1c-db5f-7c27d5d76629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Homogen\n",
              "0   0.247156\n",
              "0   0.147883\n",
              "0   0.190679\n",
              "0   0.232241\n",
              "0   0.483637\n",
              "..       ...\n",
              "0   0.331691\n",
              "0   0.196526\n",
              "0   0.154440\n",
              "0   0.233624\n",
              "0   0.255693\n",
              "\n",
              "[2000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faf5b9b6-a140-4c89-9263-cbf41f87a985\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Homogen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.247156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.147883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.190679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.232241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.483637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.331691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.196526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.154440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.233624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.255693</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faf5b9b6-a140-4c89-9263-cbf41f87a985')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-faf5b9b6-a140-4c89-9263-cbf41f87a985 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-faf5b9b6-a140-4c89-9263-cbf41f87a985');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G9aERoibjvq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train_, x_test_, y_train, y_test =0,0,0,0\n",
        "x_train_, x_test_, y_train, y_test =train_test_split(x,classes,test_size=0.15,random_state=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53MWDYSPb7AF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNWPoQotprl9",
        "outputId": "2ed0d933-ae78-49ba-8617-6b5eed3de627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1700, 1)\n",
            "(1700,)\n",
            "(300, 1)\n",
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "print(x_train_.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test_.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAV1Gi3sXWZt"
      },
      "outputs": [],
      "source": [
        "x_train=x_train_\n",
        "x_test=x_test_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHeqfy6JcjZE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JKptajHcAiw",
        "outputId": "11c01af2-1c98-46cc-ce71-00e8d72e0fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estimator: 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.72      0.73       426\n",
            "           1       0.75      0.81      0.78       426\n",
            "           2       0.78      0.70      0.74       431\n",
            "           3       0.73      0.77      0.75       417\n",
            "\n",
            "    accuracy                           0.75      1700\n",
            "   macro avg       0.75      0.75      0.75      1700\n",
            "weighted avg       0.75      0.75      0.75      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.26      0.26        74\n",
            "           1       0.37      0.41      0.38        74\n",
            "           2       0.29      0.29      0.29        69\n",
            "           3       0.37      0.35      0.36        83\n",
            "\n",
            "    accuracy                           0.33       300\n",
            "   macro avg       0.32      0.33      0.32       300\n",
            "weighted avg       0.33      0.33      0.33       300\n",
            "\n",
            "[[19 15 14 26]\n",
            " [17 30 14 13]\n",
            " [18 20 20 11]\n",
            " [17 17 20 29]]\n",
            "estimator: 6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91       426\n",
            "           1       0.90      0.92      0.91       426\n",
            "           2       0.87      0.86      0.87       431\n",
            "           3       0.94      0.83      0.88       417\n",
            "\n",
            "    accuracy                           0.89      1700\n",
            "   macro avg       0.89      0.89      0.89      1700\n",
            "weighted avg       0.89      0.89      0.89      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.24      0.24        74\n",
            "           1       0.40      0.41      0.40        74\n",
            "           2       0.23      0.26      0.24        69\n",
            "           3       0.35      0.28      0.31        83\n",
            "\n",
            "    accuracy                           0.30       300\n",
            "   macro avg       0.30      0.30      0.30       300\n",
            "weighted avg       0.30      0.30      0.30       300\n",
            "\n",
            "[[18 17 18 21]\n",
            " [20 30 15  9]\n",
            " [19 19 18 13]\n",
            " [22  9 29 23]]\n",
            "estimator: 11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.94       426\n",
            "           1       0.93      0.95      0.94       426\n",
            "           2       0.94      0.91      0.92       431\n",
            "           3       0.95      0.93      0.94       417\n",
            "\n",
            "    accuracy                           0.94      1700\n",
            "   macro avg       0.94      0.94      0.94      1700\n",
            "weighted avg       0.94      0.94      0.94      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.26      0.25        74\n",
            "           1       0.44      0.46      0.45        74\n",
            "           2       0.25      0.25      0.25        69\n",
            "           3       0.35      0.31      0.33        83\n",
            "\n",
            "    accuracy                           0.32       300\n",
            "   macro avg       0.32      0.32      0.32       300\n",
            "weighted avg       0.32      0.32      0.32       300\n",
            "\n",
            "[[19 14 16 25]\n",
            " [19 34 10 11]\n",
            " [18 21 17 13]\n",
            " [22  9 26 26]]\n",
            "estimator: 16\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       426\n",
            "           1       0.95      0.97      0.96       426\n",
            "           2       0.97      0.95      0.96       431\n",
            "           3       0.98      0.95      0.97       417\n",
            "\n",
            "    accuracy                           0.96      1700\n",
            "   macro avg       0.96      0.96      0.96      1700\n",
            "weighted avg       0.96      0.96      0.96      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.22      0.21        74\n",
            "           1       0.42      0.41      0.41        74\n",
            "           2       0.27      0.28      0.27        69\n",
            "           3       0.36      0.35      0.36        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.32      0.31      0.31       300\n",
            "\n",
            "[[16 15 16 27]\n",
            " [24 30  9 11]\n",
            " [17 20 19 13]\n",
            " [20  7 27 29]]\n",
            "estimator: 21\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98       426\n",
            "           1       0.97      0.98      0.98       426\n",
            "           2       0.98      0.97      0.98       431\n",
            "           3       0.97      0.98      0.97       417\n",
            "\n",
            "    accuracy                           0.98      1700\n",
            "   macro avg       0.98      0.98      0.98      1700\n",
            "weighted avg       0.98      0.98      0.98      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.22      0.22        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.33      0.34        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[16 16 16 26]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 29 27]]\n",
            "estimator: 26\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       426\n",
            "           1       0.98      0.98      0.98       426\n",
            "           2       0.98      0.98      0.98       431\n",
            "           3       0.98      0.97      0.98       417\n",
            "\n",
            "    accuracy                           0.98      1700\n",
            "   macro avg       0.98      0.98      0.98      1700\n",
            "weighted avg       0.98      0.98      0.98      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.22      0.21        74\n",
            "           1       0.41      0.41      0.41        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.36      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[16 16 16 26]\n",
            " [22 30 11 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 31\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       426\n",
            "           1       0.97      0.99      0.98       426\n",
            "           2       0.99      0.98      0.98       431\n",
            "           3       1.00      0.99      0.99       417\n",
            "\n",
            "    accuracy                           0.99      1700\n",
            "   macro avg       0.99      0.99      0.99      1700\n",
            "weighted avg       0.99      0.99      0.99      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.22      0.22        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.24      0.26      0.25        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[16 16 16 26]\n",
            " [19 32 12 11]\n",
            " [17 20 18 14]\n",
            " [20  7 28 28]]\n",
            "estimator: 36\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       426\n",
            "           1       1.00      0.99      1.00       426\n",
            "           2       0.99      0.99      0.99       431\n",
            "           3       0.99      0.99      0.99       417\n",
            "\n",
            "    accuracy                           0.99      1700\n",
            "   macro avg       0.99      0.99      0.99      1700\n",
            "weighted avg       0.99      0.99      0.99      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.22      0.22        74\n",
            "           1       0.42      0.42      0.42        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[16 16 15 27]\n",
            " [21 31 11 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 41\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      0.99      1.00       431\n",
            "           3       0.99      1.00      0.99       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       426\n",
            "           1       1.00      0.99      0.99       426\n",
            "           2       0.99      1.00      0.99       431\n",
            "           3       1.00      0.99      1.00       417\n",
            "\n",
            "    accuracy                           0.99      1700\n",
            "   macro avg       0.99      0.99      0.99      1700\n",
            "weighted avg       0.99      0.99      0.99      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       426\n",
            "           1       1.00      0.99      0.99       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 56\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.36      0.35      0.36        83\n",
            "\n",
            "    accuracy                           0.32       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.32      0.32      0.32       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 27 29]]\n",
            "estimator: 61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.33      0.34        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [21  7 28 27]]\n",
            "estimator: 76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n",
            "estimator: 96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.20      0.20        74\n",
            "           1       0.43      0.43      0.43        74\n",
            "           2       0.26      0.28      0.27        69\n",
            "           3       0.35      0.34      0.35        83\n",
            "\n",
            "    accuracy                           0.31       300\n",
            "   macro avg       0.31      0.31      0.31       300\n",
            "weighted avg       0.31      0.31      0.31       300\n",
            "\n",
            "[[15 16 16 27]\n",
            " [21 32 10 11]\n",
            " [17 20 19 13]\n",
            " [20  7 28 28]]\n"
          ]
        }
      ],
      "source": [
        "train_accuracy = list()\n",
        "test_accuracy = list()\n",
        "no_of_estimators = list()\n",
        "\n",
        "for estimator in range(1,100,5):\n",
        "    RF_model = RandomForestClassifier(n_estimators=estimator)\n",
        "    RF_model.fit(x_train,y_train)\n",
        "    #for train set\n",
        "    print(\"estimator:\",estimator)\n",
        "    pred_class_train = RF_model.predict(x_train)\n",
        "    print (classification_report(y_train, pred_class_train))\n",
        "    train_accuracy.append(metrics.accuracy_score(y_train, pred_class_train))\n",
        "    \n",
        "    #for test class\n",
        "    pred_class_test = RF_model.predict(x_test)\n",
        "    print (classification_report(y_test, pred_class_test))\n",
        "    # print(y_test)\n",
        "    # print(pred_class_test)\n",
        "    test_accuracy.append(metrics.accuracy_score(y_test, pred_class_test))\n",
        "    no_of_estimators.append(estimator)\n",
        "    results = confusion_matrix(y_test, pred_class_test)\n",
        "    print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2LGUl2pOjh",
        "outputId": "f750355d-1e76-4f8a-9efc-3276c4abd341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [ 8  7 10 58]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55 10  1  8]\n",
            " [11 49  9  5]\n",
            " [ 3  7 51  8]\n",
            " [ 8  6  8 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8152941176470588\n",
            "0.56\n",
            "\n",
            "[[42 10  9 13]\n",
            " [15 32 19  8]\n",
            " [ 2 13 45  9]\n",
            " [ 8 12 14 49]]\n",
            "estimator: 6\n",
            "0.9594117647058824\n",
            "0.65\n",
            "\n",
            "[[51  9  4 10]\n",
            " [13 42 14  5]\n",
            " [ 6  7 49  7]\n",
            " [13 10  7 53]]\n",
            "estimator: 11\n",
            "0.9876470588235294\n",
            "0.7066666666666667\n",
            "\n",
            "[[53  7  6  8]\n",
            " [13 43 10  8]\n",
            " [ 5  5 55  4]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 16\n",
            "0.9952941176470588\n",
            "0.68\n",
            "\n",
            "[[50 12  4  8]\n",
            " [ 9 45 12  8]\n",
            " [ 5  3 52  9]\n",
            " [12  7  7 57]]\n",
            "estimator: 21\n",
            "0.9982352941176471\n",
            "0.7033333333333334\n",
            "\n",
            "[[56  8  5  5]\n",
            " [12 48 10  4]\n",
            " [ 5  5 49 10]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 26\n",
            "1.0\n",
            "0.69\n",
            "\n",
            "[[54 13  2  5]\n",
            " [11 46 11  6]\n",
            " [ 3  7 50  9]\n",
            " [10  9  7 57]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 13  2  5]\n",
            " [12 48 10  4]\n",
            " [ 3  5 55  6]\n",
            " [ 7  8 11 57]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[59  6  3  6]\n",
            " [11 49 11  3]\n",
            " [ 4  6 54  5]\n",
            " [ 8  8  8 59]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55  9  4  6]\n",
            " [10 47 13  4]\n",
            " [ 3  6 52  8]\n",
            " [ 8  6  8 61]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  8  5  5]\n",
            " [ 9 47 11  7]\n",
            " [ 4  7 49  9]\n",
            " [11  4  8 60]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[56 11  2  5]\n",
            " [10 46 12  6]\n",
            " [ 4  6 52  7]\n",
            " [ 9  9  7 58]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[55 10  2  7]\n",
            " [ 8 49 13  4]\n",
            " [ 4  7 47 11]\n",
            " [ 8  8  8 59]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[59  8  3  4]\n",
            " [ 8 49 11  6]\n",
            " [ 4  8 49  8]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[60  7  2  5]\n",
            " [10 49  9  6]\n",
            " [ 4  2 55  8]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[58  7  3  6]\n",
            " [10 46 11  7]\n",
            " [ 3  6 52  8]\n",
            " [ 7  7  6 63]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 49 10  5]\n",
            " [ 3  4 53  9]\n",
            " [10  6  7 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[57  7  3  7]\n",
            " [11 48  9  6]\n",
            " [ 3  6 53  7]\n",
            " [ 7  7  6 63]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56 10  3  5]\n",
            " [10 46 11  7]\n",
            " [ 4  5 53  7]\n",
            " [ 8  8  8 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 11  3  5]\n",
            " [10 47 13  4]\n",
            " [ 3  5 54  7]\n",
            " [ 9  5  8 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  4  5]\n",
            " [11 48  9  6]\n",
            " [ 2  5 54  8]\n",
            " [ 9  7  7 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8158823529411765\n",
            "0.5766666666666667\n",
            "\n",
            "[[36 16  9 13]\n",
            " [19 35 11  9]\n",
            " [10  5 47  7]\n",
            " [ 5  9 14 55]]\n",
            "estimator: 6\n",
            "0.9682352941176471\n",
            "0.6266666666666667\n",
            "\n",
            "[[50 11  3 10]\n",
            " [19 36 11  8]\n",
            " [ 6  6 49  8]\n",
            " [11  8 11 53]]\n",
            "estimator: 11\n",
            "0.9870588235294118\n",
            "0.7166666666666667\n",
            "\n",
            "[[62  5  4  3]\n",
            " [12 46 13  3]\n",
            " [ 5  7 50  7]\n",
            " [ 8 11  7 57]]\n",
            "estimator: 16\n",
            "0.9982352941176471\n",
            "0.71\n",
            "\n",
            "[[55  8  4  7]\n",
            " [10 49 11  4]\n",
            " [ 4  6 53  6]\n",
            " [11  8  8 56]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.7\n",
            "\n",
            "[[55  9  4  6]\n",
            " [13 46 13  2]\n",
            " [ 4  7 49  9]\n",
            " [ 5  8 10 60]]\n",
            "estimator: 26\n",
            "0.9982352941176471\n",
            "0.6966666666666667\n",
            "\n",
            "[[56  9  2  7]\n",
            " [11 46 13  4]\n",
            " [ 5  7 51  6]\n",
            " [11  9  7 56]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 10  2  7]\n",
            " [10 48 10  6]\n",
            " [ 3  8 52  6]\n",
            " [12  5  6 60]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 10  3  7]\n",
            " [ 8 47 12  7]\n",
            " [ 4  5 52  8]\n",
            " [ 8 10  7 58]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[57 10  1  6]\n",
            " [11 49  9  5]\n",
            " [ 3  4 55  7]\n",
            " [10  7  7 59]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 12  2  6]\n",
            " [10 47 12  5]\n",
            " [ 7  3 52  7]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[57  9  2  6]\n",
            " [11 47 11  5]\n",
            " [ 2  7 53  7]\n",
            " [10  6  6 61]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55  8  3  8]\n",
            " [11 48 10  5]\n",
            " [ 4  5 54  6]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  9  2  6]\n",
            " [10 49 11  4]\n",
            " [ 4  6 52  7]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[59  7  3  5]\n",
            " [13 47  8  6]\n",
            " [ 4  5 52  8]\n",
            " [ 8  7  8 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[58  9  4  3]\n",
            " [12 46 11  5]\n",
            " [ 4  6 50  9]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  9  3  4]\n",
            " [ 9 49 12  4]\n",
            " [ 4  4 53  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  9  3  6]\n",
            " [11 49  9  5]\n",
            " [ 3  6 53  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  9  4  4]\n",
            " [10 50  9  5]\n",
            " [ 4  5 53  7]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  1  7]\n",
            " [11 47 11  5]\n",
            " [ 2  6 52  9]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  9  2  6]\n",
            " [12 48  8  6]\n",
            " [ 2  5 53  9]\n",
            " [ 9  7  6 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8288235294117647\n",
            "0.5666666666666667\n",
            "\n",
            "[[39 14  7 14]\n",
            " [14 36 14 10]\n",
            " [13  9 41  6]\n",
            " [ 8  8 13 54]]\n",
            "estimator: 6\n",
            "0.9629411764705882\n",
            "0.6633333333333333\n",
            "\n",
            "[[57  5  8  4]\n",
            " [13 45 11  5]\n",
            " [ 5  9 48  7]\n",
            " [12  9 13 49]]\n",
            "estimator: 11\n",
            "0.9876470588235294\n",
            "0.66\n",
            "\n",
            "[[50 10  7  7]\n",
            " [10 47 11  6]\n",
            " [ 8  7 46  8]\n",
            " [12  6 10 55]]\n",
            "estimator: 16\n",
            "0.9952941176470588\n",
            "0.6866666666666666\n",
            "\n",
            "[[54 10  3  7]\n",
            " [ 9 46 13  6]\n",
            " [ 6  6 50  7]\n",
            " [12  6  9 56]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.6933333333333334\n",
            "\n",
            "[[56 10  3  5]\n",
            " [12 45 13  4]\n",
            " [ 3  7 51  8]\n",
            " [ 6 10 11 56]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7033333333333334\n",
            "\n",
            "[[51 11  6  6]\n",
            " [13 45 12  4]\n",
            " [ 6  4 55  4]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.7366666666666667\n",
            "\n",
            "[[60  9  1  4]\n",
            " [10 52 10  2]\n",
            " [ 4  7 50  8]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  8  5  5]\n",
            " [ 9 43 14  8]\n",
            " [ 4  4 53  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 10  2  8]\n",
            " [12 48 10  4]\n",
            " [ 4  5 50 10]\n",
            " [ 8  8  8 59]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  3  5]\n",
            " [ 9 50 10  5]\n",
            " [ 4  7 52  6]\n",
            " [11  7  6 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56  9  1  8]\n",
            " [10 50 10  4]\n",
            " [ 4  7 50  8]\n",
            " [ 8  6 11 58]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 47 11  6]\n",
            " [ 5  5 50  9]\n",
            " [ 8  8 10 57]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.6966666666666667\n",
            "\n",
            "[[54 11  4  5]\n",
            " [11 46 11  6]\n",
            " [ 3  6 51  9]\n",
            " [ 8  7 10 58]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 48 10  6]\n",
            " [ 5  7 49  8]\n",
            " [10  7  6 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[59  8  3  4]\n",
            " [10 49 10  5]\n",
            " [ 4  6 51  8]\n",
            " [10  6  6 61]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  4  4]\n",
            " [ 9 47 12  6]\n",
            " [ 2  4 53 10]\n",
            " [10  6  6 61]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  9  2  6]\n",
            " [10 49 11  4]\n",
            " [ 3  4 54  8]\n",
            " [ 9 10  7 57]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54  9  4  7]\n",
            " [10 47 11  6]\n",
            " [ 1  6 54  8]\n",
            " [10 10  7 56]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56 10  1  7]\n",
            " [10 49 10  5]\n",
            " [ 2  6 52  9]\n",
            " [ 7  8  9 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[54 13  2  5]\n",
            " [10 49 10  5]\n",
            " [ 3  4 54  8]\n",
            " [ 9  6  8 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8194117647058824\n",
            "0.55\n",
            "\n",
            "[[47  9  9  9]\n",
            " [ 8 32 21 13]\n",
            " [12 11 37  9]\n",
            " [14  6 14 49]]\n",
            "estimator: 6\n",
            "0.97\n",
            "0.68\n",
            "\n",
            "[[50  9  8  7]\n",
            " [14 46 10  4]\n",
            " [ 6  6 52  5]\n",
            " [10  5 12 56]]\n",
            "estimator: 11\n",
            "0.9911764705882353\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 12  1  7]\n",
            " [12 45 13  4]\n",
            " [ 3  5 55  6]\n",
            " [ 6 11  8 58]]\n",
            "estimator: 16\n",
            "0.9947058823529412\n",
            "0.7066666666666667\n",
            "\n",
            "[[57  9  5  3]\n",
            " [11 45 13  5]\n",
            " [ 5  8 50  6]\n",
            " [11  6  6 60]]\n",
            "estimator: 21\n",
            "0.9994117647058823\n",
            "0.6866666666666666\n",
            "\n",
            "[[54  8  4  8]\n",
            " [12 48 11  3]\n",
            " [ 4  5 51  9]\n",
            " [11 11  8 53]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.72\n",
            "\n",
            "[[57  9  4  4]\n",
            " [ 9 51 11  3]\n",
            " [ 3  5 54  7]\n",
            " [10 10  9 54]]\n",
            "estimator: 31\n",
            "0.9976470588235294\n",
            "0.71\n",
            "\n",
            "[[55  8  3  8]\n",
            " [13 47 10  4]\n",
            " [ 4  6 52  7]\n",
            " [10  9  5 59]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.71\n",
            "\n",
            "[[58  6  1  9]\n",
            " [ 9 47 13  5]\n",
            " [ 3  4 50 12]\n",
            " [10  4 11 58]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.7166666666666667\n",
            "\n",
            "[[56  8  3  7]\n",
            " [10 48 10  6]\n",
            " [ 4  5 53  7]\n",
            " [10  7  8 58]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  7  5  5]\n",
            " [10 47 11  6]\n",
            " [ 3  6 55  5]\n",
            " [10  9  6 58]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  8  4  4]\n",
            " [11 50  9  4]\n",
            " [ 4  5 53  7]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 51  9  4]\n",
            " [ 3  4 56  6]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[53 12  3  6]\n",
            " [10 49 10  5]\n",
            " [ 4  5 53  7]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[57  8  4  5]\n",
            " [11 48 10  5]\n",
            " [ 4  7 51  7]\n",
            " [ 8 10  7 58]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 45 14  4]\n",
            " [ 3  5 53  8]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[53 10  4  7]\n",
            " [ 9 52  9  4]\n",
            " [ 3  5 54  7]\n",
            " [10  6  8 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[53 10  4  7]\n",
            " [ 8 48 12  6]\n",
            " [ 3  5 53  8]\n",
            " [11  6  8 58]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[53 13  4  4]\n",
            " [11 46 10  7]\n",
            " [ 2  4 54  9]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[56 10  3  5]\n",
            " [11 50 10  3]\n",
            " [ 3  6 54  6]\n",
            " [ 8  8  8 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[57  9  4  4]\n",
            " [10 49 10  5]\n",
            " [ 4  6 50  9]\n",
            " [ 8  8 10 57]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8305882352941176\n",
            "0.5866666666666667\n",
            "\n",
            "[[43 12 12  7]\n",
            " [11 41 13  9]\n",
            " [ 8  8 45  8]\n",
            " [12 10 14 47]]\n",
            "estimator: 6\n",
            "0.9729411764705882\n",
            "0.6666666666666666\n",
            "\n",
            "[[57  6  3  8]\n",
            " [11 47 10  6]\n",
            " [ 9  6 48  6]\n",
            " [12  8 15 48]]\n",
            "estimator: 11\n",
            "0.991764705882353\n",
            "0.7\n",
            "\n",
            "[[55  8  3  8]\n",
            " [10 49 11  4]\n",
            " [ 3  6 52  8]\n",
            " [ 7 10 12 54]]\n",
            "estimator: 16\n",
            "0.9941176470588236\n",
            "0.73\n",
            "\n",
            "[[58  7  3  6]\n",
            " [ 8 53  9  4]\n",
            " [ 3  7 53  6]\n",
            " [12  8  8 55]]\n",
            "estimator: 21\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[57  7  6  4]\n",
            " [14 46 13  1]\n",
            " [ 4  6 54  5]\n",
            " [10  9 10 54]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.69\n",
            "\n",
            "[[54 11  3  6]\n",
            " [ 9 45 13  7]\n",
            " [ 5  6 49  9]\n",
            " [10  9  5 59]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[53 11  4  6]\n",
            " [11 48 10  5]\n",
            " [ 3  5 55  6]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55  8  1 10]\n",
            " [ 9 50 12  3]\n",
            " [ 5  6 51  7]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.74\n",
            "\n",
            "[[58  7  3  6]\n",
            " [11 49 10  4]\n",
            " [ 2  5 55  7]\n",
            " [ 8  7  8 60]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  2  5]\n",
            " [ 9 51  8  6]\n",
            " [ 3  5 51 10]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 51\n",
            "0.9994117647058823\n",
            "0.71\n",
            "\n",
            "[[55 10  1  8]\n",
            " [11 48 12  3]\n",
            " [ 3  3 53 10]\n",
            " [ 9  7 10 57]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  7  4  6]\n",
            " [ 9 48 11  6]\n",
            " [ 2  7 54  6]\n",
            " [ 8 10  8 57]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[55 11  3  5]\n",
            " [10 51  9  4]\n",
            " [ 3  6 51  9]\n",
            " [10  6  6 61]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 48 10  6]\n",
            " [ 3  5 54  7]\n",
            " [10  6  7 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  9  1  6]\n",
            " [ 8 48 11  7]\n",
            " [ 4  4 54  7]\n",
            " [11  7  7 58]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[58  9  1  6]\n",
            " [ 9 47 12  6]\n",
            " [ 5  4 52  8]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[55  9  1  9]\n",
            " [10 50  9  5]\n",
            " [ 4  5 52  8]\n",
            " [10  6  6 61]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  8  4  4]\n",
            " [10 48 10  6]\n",
            " [ 3  4 53  9]\n",
            " [10  6  8 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56 10  3  5]\n",
            " [10 47 12  5]\n",
            " [ 4  6 52  7]\n",
            " [11  4  9 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57 10  3  4]\n",
            " [10 46 12  6]\n",
            " [ 3  4 53  9]\n",
            " [ 9  6  7 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8123529411764706\n",
            "0.5833333333333334\n",
            "\n",
            "[[45  9 12  8]\n",
            " [14 41 16  3]\n",
            " [ 6 10 41 12]\n",
            " [ 7  6 22 48]]\n",
            "estimator: 6\n",
            "0.9582352941176471\n",
            "0.6366666666666667\n",
            "\n",
            "[[53  5  3 13]\n",
            " [18 40 12  4]\n",
            " [ 7  9 47  6]\n",
            " [13 11  8 51]]\n",
            "estimator: 11\n",
            "0.9858823529411764\n",
            "0.6633333333333333\n",
            "\n",
            "[[49 11  4 10]\n",
            " [10 48  9  7]\n",
            " [ 9  4 47  9]\n",
            " [ 9  8 11 55]]\n",
            "estimator: 16\n",
            "0.9947058823529412\n",
            "0.7133333333333334\n",
            "\n",
            "[[60  7  1  6]\n",
            " [12 47  9  6]\n",
            " [ 6  5 50  8]\n",
            " [11  9  6 57]]\n",
            "estimator: 21\n",
            "0.9988235294117647\n",
            "0.71\n",
            "\n",
            "[[57  7  4  6]\n",
            " [12 44 14  4]\n",
            " [ 3  8 53  5]\n",
            " [10  4 10 59]]\n",
            "estimator: 26\n",
            "0.9994117647058823\n",
            "0.71\n",
            "\n",
            "[[54 12  3  5]\n",
            " [12 49 10  3]\n",
            " [ 3  5 53  8]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.6833333333333333\n",
            "\n",
            "[[54 10  4  6]\n",
            " [13 43 12  6]\n",
            " [ 5  5 52  7]\n",
            " [10  9  8 56]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  8  2  9]\n",
            " [10 49 10  5]\n",
            " [ 5  4 50 10]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  1  8]\n",
            " [10 50  9  5]\n",
            " [ 4  5 54  6]\n",
            " [ 8 10  7 58]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  8  2  7]\n",
            " [11 49  8  6]\n",
            " [ 4  6 52  7]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 10  2  8]\n",
            " [11 49 10  4]\n",
            " [ 4  6 52  7]\n",
            " [11  5  7 60]]\n",
            "estimator: 56\n",
            "0.9994117647058823\n",
            "0.73\n",
            "\n",
            "[[53 10  2  9]\n",
            " [ 9 52 10  3]\n",
            " [ 3  4 54  8]\n",
            " [ 7  7  9 60]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[54 12  2  6]\n",
            " [10 49 10  5]\n",
            " [ 4  5 54  6]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[57 10  1  6]\n",
            " [12 47 11  4]\n",
            " [ 3  6 51  9]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55  9  3  7]\n",
            " [11 47 10  6]\n",
            " [ 2  3 57  7]\n",
            " [10  7  8 58]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  9  1  7]\n",
            " [10 49 10  5]\n",
            " [ 4  5 51  9]\n",
            " [10  7  7 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  9  2  5]\n",
            " [ 9 49 10  6]\n",
            " [ 3  6 52  8]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  8  4  6]\n",
            " [10 49 10  5]\n",
            " [ 5  5 50  9]\n",
            " [10  7  9 57]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[57  8  3  6]\n",
            " [10 47 13  4]\n",
            " [ 1  5 53 10]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[52 13  1  8]\n",
            " [10 48 11  5]\n",
            " [ 3  6 52  8]\n",
            " [ 9  8  8 58]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8276470588235294\n",
            "0.54\n",
            "\n",
            "[[41 10  9 14]\n",
            " [18 37 13  6]\n",
            " [ 9 10 40 10]\n",
            " [13 14 12 44]]\n",
            "estimator: 6\n",
            "0.9623529411764706\n",
            "0.6733333333333333\n",
            "\n",
            "[[52  9  5  8]\n",
            " [14 46  8  6]\n",
            " [ 7  6 53  3]\n",
            " [10 10 12 51]]\n",
            "estimator: 11\n",
            "0.9888235294117647\n",
            "0.6766666666666666\n",
            "\n",
            "[[47 11  7  9]\n",
            " [11 48 12  3]\n",
            " [ 4  8 51  6]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 16\n",
            "0.9952941176470588\n",
            "0.7066666666666667\n",
            "\n",
            "[[52  7  6  9]\n",
            " [10 50  9  5]\n",
            " [ 2  6 52  9]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.7266666666666667\n",
            "\n",
            "[[59  6  3  6]\n",
            " [11 45 14  4]\n",
            " [ 5  5 52  7]\n",
            " [ 7  5  9 62]]\n",
            "estimator: 26\n",
            "0.9976470588235294\n",
            "0.7066666666666667\n",
            "\n",
            "[[53 10  2  9]\n",
            " [ 9 46 14  5]\n",
            " [ 3  5 52  9]\n",
            " [ 6  8  8 61]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.7333333333333333\n",
            "\n",
            "[[55 10  1  8]\n",
            " [10 47 10  7]\n",
            " [ 1  6 56  6]\n",
            " [10  6  5 62]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.6833333333333333\n",
            "\n",
            "[[53 10  1 10]\n",
            " [ 9 47 13  5]\n",
            " [ 5  7 48  9]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55  8  2  9]\n",
            " [14 46 12  2]\n",
            " [ 3  5 54  7]\n",
            " [ 8  9  6 60]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[55  9  5  5]\n",
            " [10 50 12  2]\n",
            " [ 3  4 54  8]\n",
            " [10  6  6 61]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55 10  1  8]\n",
            " [ 9 45 12  8]\n",
            " [ 2  5 54  8]\n",
            " [ 9  9  7 58]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[58  7  4  5]\n",
            " [11 50 11  2]\n",
            " [ 4  6 54  5]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[56 11  1  6]\n",
            " [11 50  8  5]\n",
            " [ 4  5 54  6]\n",
            " [ 9  5  9 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55 11  4  4]\n",
            " [10 49 11  4]\n",
            " [ 4  5 53  7]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[58  9  2  5]\n",
            " [11 48 10  5]\n",
            " [ 4  5 52  8]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[54  9  3  8]\n",
            " [11 49 11  3]\n",
            " [ 3  5 54  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[55 10  5  4]\n",
            " [ 9 50 10  5]\n",
            " [ 3  5 53  8]\n",
            " [ 8  6  7 62]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.75\n",
            "\n",
            "[[57  8  2  7]\n",
            " [10 51 10  3]\n",
            " [ 3  5 55  6]\n",
            " [ 9  5  7 62]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[57  8  3  6]\n",
            " [10 51 10  3]\n",
            " [ 2  4 54  9]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  8  1  7]\n",
            " [10 48 11  5]\n",
            " [ 3  5 52  9]\n",
            " [ 8  8  7 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8188235294117647\n",
            "0.57\n",
            "\n",
            "[[36 13 17  8]\n",
            " [10 48 11  5]\n",
            " [ 7  9 45  8]\n",
            " [ 8 13 20 42]]\n",
            "estimator: 6\n",
            "0.9758823529411764\n",
            "0.65\n",
            "\n",
            "[[46 11  5 12]\n",
            " [13 49 10  2]\n",
            " [ 8  7 46  8]\n",
            " [ 7 10 12 54]]\n",
            "estimator: 11\n",
            "0.9905882352941177\n",
            "0.6633333333333333\n",
            "\n",
            "[[52 12  4  6]\n",
            " [18 40 11  5]\n",
            " [ 7  6 50  6]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 16\n",
            "0.9976470588235294\n",
            "0.6833333333333333\n",
            "\n",
            "[[48 17  3  6]\n",
            " [11 46 13  4]\n",
            " [ 3  7 54  5]\n",
            " [ 8 10  8 57]]\n",
            "estimator: 21\n",
            "0.9970588235294118\n",
            "0.6833333333333333\n",
            "\n",
            "[[52  9  4  9]\n",
            " [10 45 14  5]\n",
            " [ 5  7 50  7]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 26\n",
            "0.9982352941176471\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 11  2  7]\n",
            " [12 48 11  3]\n",
            " [ 5  9 51  4]\n",
            " [10  7  8 58]]\n",
            "estimator: 31\n",
            "0.9976470588235294\n",
            "0.71\n",
            "\n",
            "[[56  8  4  6]\n",
            " [11 47 10  6]\n",
            " [ 2  5 54  8]\n",
            " [ 8 11  8 56]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7133333333333334\n",
            "\n",
            "[[58  7  5  4]\n",
            " [ 9 49  9  7]\n",
            " [ 4  4 51 10]\n",
            " [11  6 10 56]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  8  2  8]\n",
            " [ 9 50 11  4]\n",
            " [ 5  6 51  7]\n",
            " [ 6  8  9 60]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  1  7]\n",
            " [11 48 11  4]\n",
            " [ 3  5 54  7]\n",
            " [ 9 10  7 57]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 49 10  5]\n",
            " [ 4  7 51  7]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[59  6  2  7]\n",
            " [10 49 10  5]\n",
            " [ 3  5 52  9]\n",
            " [10  9  6 58]]\n",
            "estimator: 61\n",
            "0.9994117647058823\n",
            "0.7266666666666667\n",
            "\n",
            "[[55  9  3  7]\n",
            " [ 9 50 13  2]\n",
            " [ 3  5 53  8]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54  9  3  8]\n",
            " [12 47  9  6]\n",
            " [ 4  4 51 10]\n",
            " [ 7  7  8 61]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  9  3  4]\n",
            " [10 50  9  5]\n",
            " [ 2  7 53  7]\n",
            " [10  6  8 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 11  3  6]\n",
            " [ 9 51  8  6]\n",
            " [ 5  5 51  8]\n",
            " [ 8 10  6 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  8  1  8]\n",
            " [10 47 10  7]\n",
            " [ 3  6 52  8]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  8  2  7]\n",
            " [10 48 11  5]\n",
            " [ 5  6 50  8]\n",
            " [ 7  7  7 62]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  8  2  8]\n",
            " [ 9 51 11  3]\n",
            " [ 2  5 54  8]\n",
            " [ 9  9  9 56]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  9  2  6]\n",
            " [12 48 11  3]\n",
            " [ 3  4 53  9]\n",
            " [ 9  7 10 57]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8388235294117647\n",
            "0.5766666666666667\n",
            "\n",
            "[[42 14 10  8]\n",
            " [ 7 38 19 10]\n",
            " [ 2 10 49  8]\n",
            " [10 14 15 44]]\n",
            "estimator: 6\n",
            "0.9635294117647059\n",
            "0.6733333333333333\n",
            "\n",
            "[[54 10  4  6]\n",
            " [12 47 13  2]\n",
            " [ 6  7 50  6]\n",
            " [13 11  8 51]]\n",
            "estimator: 11\n",
            "0.9911764705882353\n",
            "0.7033333333333334\n",
            "\n",
            "[[55  9  3  7]\n",
            " [10 48  8  8]\n",
            " [ 4  6 51  8]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 16\n",
            "0.9923529411764705\n",
            "0.6966666666666667\n",
            "\n",
            "[[50 13  3  8]\n",
            " [11 51  9  3]\n",
            " [ 6  6 49  8]\n",
            " [ 7  8  9 59]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.6966666666666667\n",
            "\n",
            "[[54 11  3  6]\n",
            " [ 7 48 13  6]\n",
            " [ 3  5 52  9]\n",
            " [ 8 10 10 55]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  8  3  7]\n",
            " [12 48 10  4]\n",
            " [ 4  7 53  5]\n",
            " [ 9  9 10 55]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[52 12  2  8]\n",
            " [11 49  8  6]\n",
            " [ 6  5 51  7]\n",
            " [ 9  9  6 59]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 10  3  6]\n",
            " [11 48 11  4]\n",
            " [ 4  6 52  7]\n",
            " [ 9  6 10 58]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 48 10  5]\n",
            " [ 4  6 50  9]\n",
            " [ 9  5  5 64]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56  8  3  7]\n",
            " [11 48 12  3]\n",
            " [ 4  6 50  9]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[59  9  3  3]\n",
            " [11 47 11  5]\n",
            " [ 4  5 53  7]\n",
            " [10  8  7 58]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.6833333333333333\n",
            "\n",
            "[[56  9  1  8]\n",
            " [12 42 14  6]\n",
            " [ 4  5 51  9]\n",
            " [ 9  9  9 56]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  9  3  4]\n",
            " [ 8 49 10  7]\n",
            " [ 4  6 54  5]\n",
            " [10  6  8 59]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[52 11  3  8]\n",
            " [ 9 47 11  7]\n",
            " [ 2  5 54  8]\n",
            " [10  7  7 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[58  8  3  5]\n",
            " [10 48 10  6]\n",
            " [ 4  6 50  9]\n",
            " [10  9  8 56]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  9  3  5]\n",
            " [10 48 12  4]\n",
            " [ 3  6 52  8]\n",
            " [10  7  7 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  3  4]\n",
            " [10 51  9  4]\n",
            " [ 3  6 51  9]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  3  5]\n",
            " [11 46 11  6]\n",
            " [ 4  6 53  6]\n",
            " [10  6  8 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  3  5]\n",
            " [12 47 10  5]\n",
            " [ 1  8 52  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 10  1  8]\n",
            " [11 51  9  3]\n",
            " [ 2  6 52  9]\n",
            " [ 9  8  7 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8111764705882353\n",
            "0.5533333333333333\n",
            "\n",
            "[[42 12 13  7]\n",
            " [10 41 10 13]\n",
            " [13 11 34 11]\n",
            " [ 7 14 13 49]]\n",
            "estimator: 6\n",
            "0.9658823529411765\n",
            "0.6333333333333333\n",
            "\n",
            "[[51  7  6 10]\n",
            " [13 44 11  6]\n",
            " [10 11 44  4]\n",
            " [13 11  8 51]]\n",
            "estimator: 11\n",
            "0.9911764705882353\n",
            "0.6866666666666666\n",
            "\n",
            "[[49 12  2 11]\n",
            " [ 7 49 13  5]\n",
            " [ 4  7 53  5]\n",
            " [13  9  6 55]]\n",
            "estimator: 16\n",
            "0.9964705882352941\n",
            "0.6866666666666666\n",
            "\n",
            "[[48 13  4  9]\n",
            " [ 9 49 13  3]\n",
            " [ 5  6 50  8]\n",
            " [ 6  9  9 59]]\n",
            "estimator: 21\n",
            "0.9988235294117647\n",
            "0.69\n",
            "\n",
            "[[56  8  5  5]\n",
            " [12 45 13  4]\n",
            " [ 6  3 50 10]\n",
            " [ 8 10  9 56]]\n",
            "estimator: 26\n",
            "0.9964705882352941\n",
            "0.7\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 48 10  6]\n",
            " [ 6  6 50  7]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[57  9  2  6]\n",
            " [10 49 11  4]\n",
            " [ 5  6 49  9]\n",
            " [ 9  9  7 58]]\n",
            "estimator: 36\n",
            "0.9988235294117647\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  3  4]\n",
            " [11 49  9  5]\n",
            " [ 4  3 55  7]\n",
            " [10  8  8 57]]\n",
            "estimator: 41\n",
            "0.9988235294117647\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  9  2  6]\n",
            " [11 48 10  5]\n",
            " [ 4  7 49  9]\n",
            " [ 8  5  9 61]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  7  1  9]\n",
            " [10 49 11  4]\n",
            " [ 3  6 53  7]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 10  4  6]\n",
            " [10 47 12  5]\n",
            " [ 4  5 52  8]\n",
            " [ 7  7  8 61]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[58  8  3  5]\n",
            " [10 52  8  4]\n",
            " [ 3  4 55  7]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 10  2  8]\n",
            " [ 9 48 10  7]\n",
            " [ 4  5 53  7]\n",
            " [11  7  6 59]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55 10  2  7]\n",
            " [10 49 12  3]\n",
            " [ 5  5 49 10]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7433333333333333\n",
            "\n",
            "[[57  9  3  5]\n",
            " [10 51  9  4]\n",
            " [ 3  5 54  7]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[58  9  2  5]\n",
            " [10 50  9  5]\n",
            " [ 4  4 54  7]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[58  6  3  7]\n",
            " [11 49  9  5]\n",
            " [ 3  8 50  8]\n",
            " [ 6  7  8 62]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 48 11  5]\n",
            " [ 3  5 53  8]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[58  8  3  5]\n",
            " [ 9 50  9  6]\n",
            " [ 4  6 52  7]\n",
            " [ 9 10  9 55]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  7  2  8]\n",
            " [10 48 14  2]\n",
            " [ 4  5 53  7]\n",
            " [11  6  8 58]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.82\n",
            "0.5533333333333333\n",
            "\n",
            "[[42 18  3 11]\n",
            " [13 36 17  8]\n",
            " [ 8  8 42 11]\n",
            " [14  6 17 46]]\n",
            "estimator: 6\n",
            "0.9564705882352941\n",
            "0.6733333333333333\n",
            "\n",
            "[[54 10  4  6]\n",
            " [13 47 10  4]\n",
            " [ 7  7 47  8]\n",
            " [ 8 11 10 54]]\n",
            "estimator: 11\n",
            "0.9923529411764705\n",
            "0.69\n",
            "\n",
            "[[53  8  6  7]\n",
            " [ 9 51  8  6]\n",
            " [ 4  9 52  4]\n",
            " [11 12  9 51]]\n",
            "estimator: 16\n",
            "0.9952941176470588\n",
            "0.71\n",
            "\n",
            "[[55  9  3  7]\n",
            " [11 46 11  6]\n",
            " [ 4  4 55  6]\n",
            " [10  9  7 57]]\n",
            "estimator: 21\n",
            "0.9994117647058823\n",
            "0.7\n",
            "\n",
            "[[55 11  3  5]\n",
            " [14 46 10  4]\n",
            " [ 7  5 50  7]\n",
            " [10  7  7 59]]\n",
            "estimator: 26\n",
            "0.9994117647058823\n",
            "0.72\n",
            "\n",
            "[[53  9  4  8]\n",
            " [10 48 10  6]\n",
            " [ 2  5 57  5]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.7133333333333334\n",
            "\n",
            "[[56  7  2  9]\n",
            " [11 46 11  6]\n",
            " [ 2  6 55  6]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[53 11  5  5]\n",
            " [10 48 12  4]\n",
            " [ 3  4 56  6]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[59  7  4  4]\n",
            " [10 49  9  6]\n",
            " [ 2  6 53  8]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 12  2  6]\n",
            " [11 50  9  4]\n",
            " [ 4  6 49 10]\n",
            " [10  7  7 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[56 10  4  4]\n",
            " [11 46 12  5]\n",
            " [ 2  4 58  5]\n",
            " [ 7  7  7 62]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 44 15  5]\n",
            " [ 3  5 53  8]\n",
            " [10  5  8 60]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56  9  2  7]\n",
            " [11 48 11  4]\n",
            " [ 4  6 51  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56  8  2  8]\n",
            " [ 9 46 13  6]\n",
            " [ 4  5 52  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 12  3  5]\n",
            " [11 47 11  5]\n",
            " [ 3  4 54  8]\n",
            " [10  8  6 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 11  3  6]\n",
            " [11 46 10  7]\n",
            " [ 3  6 51  9]\n",
            " [10  8  5 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 50 10  4]\n",
            " [ 2  7 52  8]\n",
            " [ 7  8  9 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 49 12  3]\n",
            " [ 3  7 52  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  1  7]\n",
            " [10 46 11  7]\n",
            " [ 4  4 52  9]\n",
            " [ 8  8  6 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 10  2  7]\n",
            " [10 49 10  5]\n",
            " [ 3  6 54  6]\n",
            " [10  6  8 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8258823529411765\n",
            "0.5766666666666667\n",
            "\n",
            "[[42 11 10 11]\n",
            " [18 36 12  8]\n",
            " [ 9  7 43 10]\n",
            " [ 9 11 11 52]]\n",
            "estimator: 6\n",
            "0.9647058823529412\n",
            "0.65\n",
            "\n",
            "[[52 11  4  7]\n",
            " [13 47 12  2]\n",
            " [11 10 42  6]\n",
            " [10 10  9 54]]\n",
            "estimator: 11\n",
            "0.9882352941176471\n",
            "0.6533333333333333\n",
            "\n",
            "[[53  9  4  8]\n",
            " [10 47 13  4]\n",
            " [ 7  7 45 10]\n",
            " [10 13  9 51]]\n",
            "estimator: 16\n",
            "0.9970588235294118\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  7  2  7]\n",
            " [12 48  8  6]\n",
            " [ 3  5 54  7]\n",
            " [ 7  9  7 60]]\n",
            "estimator: 21\n",
            "0.9952941176470588\n",
            "0.7\n",
            "\n",
            "[[51 12  3  8]\n",
            " [ 8 49 12  5]\n",
            " [ 3  7 52  7]\n",
            " [ 8  7 10 58]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.72\n",
            "\n",
            "[[56  8  4  6]\n",
            " [11 48 11  4]\n",
            " [ 2  5 55  7]\n",
            " [10  7  9 57]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.7066666666666667\n",
            "\n",
            "[[53  9  5  7]\n",
            " [12 49  9  4]\n",
            " [ 4  6 54  5]\n",
            " [10  7 10 56]]\n",
            "estimator: 36\n",
            "0.9988235294117647\n",
            "0.72\n",
            "\n",
            "[[56  9  3  6]\n",
            " [11 47 12  4]\n",
            " [ 3  4 55  7]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[53 11  2  8]\n",
            " [ 9 51 10  4]\n",
            " [ 3  5 52  9]\n",
            " [ 8  5  7 63]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  2  5]\n",
            " [10 48 11  5]\n",
            " [ 3  5 54  7]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.69\n",
            "\n",
            "[[54 10  5  5]\n",
            " [11 46 11  6]\n",
            " [ 4  5 51  9]\n",
            " [ 9  9  9 56]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[57  9  3  5]\n",
            " [13 44 11  6]\n",
            " [ 4  6 51  8]\n",
            " [ 8  7  6 62]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 50 11  3]\n",
            " [ 4  5 54  6]\n",
            " [10  7  6 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55  8  1 10]\n",
            " [10 51  8  5]\n",
            " [ 4  6 50  9]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[57  6  3  8]\n",
            " [10 50  9  5]\n",
            " [ 3  5 53  8]\n",
            " [ 7  9  7 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  4  5]\n",
            " [10 49 12  3]\n",
            " [ 3  4 54  8]\n",
            " [ 7  8  9 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  8  3  5]\n",
            " [10 50  9  5]\n",
            " [ 4  3 53  9]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[53 12  2  7]\n",
            " [12 48 10  4]\n",
            " [ 3  6 52  8]\n",
            " [ 9  5  7 62]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  3  5]\n",
            " [11 49  8  6]\n",
            " [ 4  4 54  7]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  9  2  6]\n",
            " [11 48 10  5]\n",
            " [ 3  5 53  8]\n",
            " [ 8  7  7 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8129411764705883\n",
            "0.5066666666666667\n",
            "\n",
            "[[35 16  8 15]\n",
            " [17 35 12 10]\n",
            " [10  7 34 18]\n",
            " [13  8 14 48]]\n",
            "estimator: 6\n",
            "0.9641176470588235\n",
            "0.6766666666666666\n",
            "\n",
            "[[55  6  8  5]\n",
            " [10 42 16  6]\n",
            " [ 5  7 51  6]\n",
            " [ 9 10  9 55]]\n",
            "estimator: 11\n",
            "0.9870588235294118\n",
            "0.7433333333333333\n",
            "\n",
            "[[58  8  2  6]\n",
            " [11 53  7  3]\n",
            " [ 6  5 54  4]\n",
            " [10  6  9 58]]\n",
            "estimator: 16\n",
            "0.9976470588235294\n",
            "0.71\n",
            "\n",
            "[[55  9  4  6]\n",
            " [12 47 11  4]\n",
            " [ 5  3 55  6]\n",
            " [11  7  9 56]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.71\n",
            "\n",
            "[[57  8  4  5]\n",
            " [11 47 11  5]\n",
            " [ 6  7 48  8]\n",
            " [ 9  7  6 61]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7133333333333334\n",
            "\n",
            "[[55 11  4  4]\n",
            " [ 9 50 11  4]\n",
            " [ 5  6 52  6]\n",
            " [10  7  9 57]]\n",
            "estimator: 31\n",
            "0.9970588235294118\n",
            "0.7066666666666667\n",
            "\n",
            "[[58  8  2  6]\n",
            " [12 45 10  7]\n",
            " [ 2  6 51 10]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  9  2  7]\n",
            " [ 9 48 12  5]\n",
            " [ 2  8 51  8]\n",
            " [ 7 11  8 57]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 12  3  4]\n",
            " [10 48 12  4]\n",
            " [ 4  5 52  8]\n",
            " [ 7 11  7 58]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55  8  3  8]\n",
            " [10 50 11  3]\n",
            " [ 4  6 52  7]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[55  9  3  7]\n",
            " [ 9 51  9  5]\n",
            " [ 3  6 52  8]\n",
            " [10  7  5 61]]\n",
            "estimator: 56\n",
            "0.9994117647058823\n",
            "0.72\n",
            "\n",
            "[[56  9  1  8]\n",
            " [11 47 11  5]\n",
            " [ 3  5 55  6]\n",
            " [11  8  6 58]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[54 11  3  6]\n",
            " [10 48 12  4]\n",
            " [ 3  4 56  6]\n",
            " [ 8  6  8 61]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 11  2  6]\n",
            " [ 9 47 12  6]\n",
            " [ 4  5 54  6]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[58  8  1  7]\n",
            " [10 51  9  4]\n",
            " [ 3  7 51  8]\n",
            " [10  7  7 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[55 11  1  7]\n",
            " [11 46 12  5]\n",
            " [ 4  6 51  8]\n",
            " [10  8  7 58]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 10  1  9]\n",
            " [ 9 48 11  6]\n",
            " [ 4  4 52  9]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56 10  1  7]\n",
            " [ 9 52  9  4]\n",
            " [ 4  6 50  9]\n",
            " [10  7  6 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  8  3  6]\n",
            " [11 50 10  3]\n",
            " [ 4  7 49  9]\n",
            " [10  6  7 60]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[57  8  1  8]\n",
            " [10 47 13  4]\n",
            " [ 3  6 51  9]\n",
            " [ 9  7  8 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8223529411764706\n",
            "0.5166666666666667\n",
            "\n",
            "[[35 16 10 13]\n",
            " [15 34 15 10]\n",
            " [ 7  8 41 13]\n",
            " [14  8 16 45]]\n",
            "estimator: 6\n",
            "0.9711764705882353\n",
            "0.6566666666666666\n",
            "\n",
            "[[47 11  7  9]\n",
            " [11 48 12  3]\n",
            " [ 6  8 50  5]\n",
            " [14  9  8 52]]\n",
            "estimator: 11\n",
            "0.9894117647058823\n",
            "0.67\n",
            "\n",
            "[[49 10  6  9]\n",
            " [15 42 11  6]\n",
            " [ 5  3 55  6]\n",
            " [ 9  9 10 55]]\n",
            "estimator: 16\n",
            "0.9964705882352941\n",
            "0.68\n",
            "\n",
            "[[50 11  7  6]\n",
            " [10 48 13  3]\n",
            " [ 6  6 53  4]\n",
            " [10 12  8 53]]\n",
            "estimator: 21\n",
            "0.9947058823529412\n",
            "0.6933333333333334\n",
            "\n",
            "[[57  9  3  5]\n",
            " [12 45 10  7]\n",
            " [ 5  4 53  7]\n",
            " [13  8  9 53]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  8  3  6]\n",
            " [ 8 47 12  7]\n",
            " [ 6  5 53  5]\n",
            " [ 7  9  9 58]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56 11  2  5]\n",
            " [10 49 11  4]\n",
            " [ 4  7 52  6]\n",
            " [11  9  6 57]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 11  1  8]\n",
            " [11 48 11  4]\n",
            " [ 1  6 53  9]\n",
            " [ 7 11  9 56]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 48 10  5]\n",
            " [ 2  5 54  8]\n",
            " [ 6  9  7 61]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 47 10  6]\n",
            " [ 4  4 52  9]\n",
            " [ 7  7  8 61]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  2  6]\n",
            " [10 47 12  5]\n",
            " [ 3  6 52  8]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  8  3  7]\n",
            " [10 49 11  4]\n",
            " [ 2  4 55  8]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  2  6]\n",
            " [12 45 10  7]\n",
            " [ 3  5 53  8]\n",
            " [11  5  7 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  2  5]\n",
            " [11 50  9  4]\n",
            " [ 4  6 50  9]\n",
            " [ 7  8  7 61]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[58  8  2  6]\n",
            " [11 47 10  6]\n",
            " [ 5  5 51  8]\n",
            " [10  6  6 61]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[53 10  4  7]\n",
            " [ 9 51  9  5]\n",
            " [ 4  5 53  7]\n",
            " [10  7  8 58]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 46 12  5]\n",
            " [ 3  6 54  6]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[56  7  3  8]\n",
            " [11 47 10  6]\n",
            " [ 2  5 57  5]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56 10  3  5]\n",
            " [11 47 10  6]\n",
            " [ 4  5 53  7]\n",
            " [10  6  7 60]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[58  7  2  7]\n",
            " [12 46 10  6]\n",
            " [ 4  6 51  8]\n",
            " [ 9  8  8 58]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8229411764705883\n",
            "0.56\n",
            "\n",
            "[[44 10  6 14]\n",
            " [17 30 16 11]\n",
            " [ 7  9 41 12]\n",
            " [10  7 13 53]]\n",
            "estimator: 6\n",
            "0.9611764705882353\n",
            "0.67\n",
            "\n",
            "[[53 13  1  7]\n",
            " [10 47 15  2]\n",
            " [ 6  6 50  7]\n",
            " [11 11 10 51]]\n",
            "estimator: 11\n",
            "0.9858823529411764\n",
            "0.6833333333333333\n",
            "\n",
            "[[54  7  5  8]\n",
            " [14 48  9  3]\n",
            " [ 8  6 46  9]\n",
            " [ 7  9 10 57]]\n",
            "estimator: 16\n",
            "0.991764705882353\n",
            "0.7133333333333334\n",
            "\n",
            "[[54  8  3  9]\n",
            " [ 9 46 15  4]\n",
            " [ 3  6 55  5]\n",
            " [ 7  8  9 59]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.7033333333333334\n",
            "\n",
            "[[56  9  2  7]\n",
            " [12 47 10  5]\n",
            " [ 3  7 53  6]\n",
            " [ 9  9 10 55]]\n",
            "estimator: 26\n",
            "0.9982352941176471\n",
            "0.7333333333333333\n",
            "\n",
            "[[60  5  3  6]\n",
            " [10 50 12  2]\n",
            " [ 5  6 52  6]\n",
            " [ 7 10  8 58]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.73\n",
            "\n",
            "[[57  9  3  5]\n",
            " [ 8 50 13  3]\n",
            " [ 3  6 54  6]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 36\n",
            "0.9988235294117647\n",
            "0.7066666666666667\n",
            "\n",
            "[[56 11  2  5]\n",
            " [11 50  9  4]\n",
            " [ 3  6 48 12]\n",
            " [11  7  7 58]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[54 10  5  5]\n",
            " [11 44 13  6]\n",
            " [ 5  5 52  7]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.7266666666666667\n",
            "\n",
            "[[53 10  3  8]\n",
            " [ 9 50  9  6]\n",
            " [ 2  5 56  6]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[58  7  1  8]\n",
            " [12 47 10  5]\n",
            " [ 4  6 52  7]\n",
            " [11  8  7 57]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  9  1  9]\n",
            " [10 50 11  3]\n",
            " [ 4  7 49  9]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[59  8  1  6]\n",
            " [11 47 11  5]\n",
            " [ 3  7 51  8]\n",
            " [ 8  7  8 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 50  9  5]\n",
            " [ 5  6 51  7]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 11  3  6]\n",
            " [10 48  9  7]\n",
            " [ 3  6 52  8]\n",
            " [11  7  8 57]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[55 11  2  6]\n",
            " [11 48 10  5]\n",
            " [ 3  7 51  8]\n",
            " [10  6  7 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56 11  2  5]\n",
            " [11 47 10  6]\n",
            " [ 3  5 53  8]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[54 11  2  7]\n",
            " [11 50  9  4]\n",
            " [ 5  4 51  9]\n",
            " [ 9  6  6 62]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 47 11  6]\n",
            " [ 3  6 52  8]\n",
            " [10  6  6 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 10  3  7]\n",
            " [10 46 13  5]\n",
            " [ 2  5 54  8]\n",
            " [ 9  6  7 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.831764705882353\n",
            "0.5333333333333333\n",
            "\n",
            "[[37  9 18 10]\n",
            " [ 8 40 18  8]\n",
            " [ 7 10 44  8]\n",
            " [13 10 21 39]]\n",
            "estimator: 6\n",
            "0.9594117647058824\n",
            "0.6566666666666666\n",
            "\n",
            "[[53 11  5  5]\n",
            " [20 42  8  4]\n",
            " [ 9  8 48  4]\n",
            " [12 10  7 54]]\n",
            "estimator: 11\n",
            "0.9864705882352941\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  8  2  8]\n",
            " [13 47  9  5]\n",
            " [ 6  4 53  6]\n",
            " [10  8  9 56]]\n",
            "estimator: 16\n",
            "0.9952941176470588\n",
            "0.7266666666666667\n",
            "\n",
            "[[59  6  2  7]\n",
            " [12 49 11  2]\n",
            " [ 2  8 55  4]\n",
            " [ 9 10  9 55]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.7166666666666667\n",
            "\n",
            "[[52 13  2  7]\n",
            " [10 48 10  6]\n",
            " [ 4  5 54  6]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 26\n",
            "0.9982352941176471\n",
            "0.69\n",
            "\n",
            "[[53  9  4  8]\n",
            " [10 49 10  5]\n",
            " [ 4  7 49  9]\n",
            " [ 9  7 11 56]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.6933333333333334\n",
            "\n",
            "[[52 12  1  9]\n",
            " [ 8 45 14  7]\n",
            " [ 3  5 52  9]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.6966666666666667\n",
            "\n",
            "[[54 11  1  8]\n",
            " [11 48 11  4]\n",
            " [ 4  7 48 10]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.6966666666666667\n",
            "\n",
            "[[57 10  2  5]\n",
            " [13 45 10  6]\n",
            " [ 5  7 48  9]\n",
            " [10  6  8 59]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  8  3  8]\n",
            " [11 45 11  7]\n",
            " [ 3  7 51  8]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55  8  2  9]\n",
            " [12 48  9  5]\n",
            " [ 4  7 52  6]\n",
            " [10  5  8 60]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[54 11  3  6]\n",
            " [10 48 11  5]\n",
            " [ 3  4 54  8]\n",
            " [ 6  6  7 64]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[58  9  2  5]\n",
            " [11 48  9  6]\n",
            " [ 6  4 50  9]\n",
            " [11  8  5 59]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[56 11  2  5]\n",
            " [10 47 10  7]\n",
            " [ 5  5 50  9]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  3  5]\n",
            " [11 46 13  4]\n",
            " [ 3  6 52  8]\n",
            " [ 8  6  9 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 49 10  4]\n",
            " [ 3  5 52  9]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 49 11  4]\n",
            " [ 3  6 53  7]\n",
            " [10  6  5 62]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  8  2  7]\n",
            " [11 48  9  6]\n",
            " [ 5  7 51  6]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[57  9  1  7]\n",
            " [12 48 11  3]\n",
            " [ 3  5 54  7]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 11  2  7]\n",
            " [12 46 10  6]\n",
            " [ 4  5 53  7]\n",
            " [ 9  7  6 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8147058823529412\n",
            "0.55\n",
            "\n",
            "[[45  9  8 12]\n",
            " [14 33 14 13]\n",
            " [11  8 37 13]\n",
            " [ 7 13 13 50]]\n",
            "estimator: 6\n",
            "0.9641176470588235\n",
            "0.67\n",
            "\n",
            "[[58  8  2  6]\n",
            " [15 43 13  3]\n",
            " [ 8  7 47  7]\n",
            " [11  8 11 53]]\n",
            "estimator: 11\n",
            "0.9888235294117647\n",
            "0.69\n",
            "\n",
            "[[57 10  3  4]\n",
            " [ 8 46 12  8]\n",
            " [ 9  5 47  8]\n",
            " [ 8 10  8 57]]\n",
            "estimator: 16\n",
            "0.9935294117647059\n",
            "0.6766666666666666\n",
            "\n",
            "[[54  8  3  9]\n",
            " [11 44 12  7]\n",
            " [ 6  5 50  8]\n",
            " [11  9  8 55]]\n",
            "estimator: 21\n",
            "0.9988235294117647\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  4  5]\n",
            " [11 50 10  3]\n",
            " [ 3  6 53  7]\n",
            " [12  5  7 59]]\n",
            "estimator: 26\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[55 11  3  5]\n",
            " [ 9 48 12  5]\n",
            " [ 4  6 53  6]\n",
            " [13  4 12 54]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  8  4  5]\n",
            " [12 46  9  7]\n",
            " [ 3  7 53  6]\n",
            " [ 9  9  6 59]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7433333333333333\n",
            "\n",
            "[[59  6  4  5]\n",
            " [11 47 10  6]\n",
            " [ 0  4 57  8]\n",
            " [ 6  8  9 60]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[53 10  3  8]\n",
            " [12 50  9  3]\n",
            " [ 3  6 52  8]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.72\n",
            "\n",
            "[[55  9  4  6]\n",
            " [10 50 11  3]\n",
            " [ 3  6 50 10]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[52 13  2  7]\n",
            " [ 7 53  9  5]\n",
            " [ 3  6 52  8]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[57 10  2  5]\n",
            " [10 48 10  6]\n",
            " [ 5  7 51  6]\n",
            " [11  8  7 57]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 49 10  5]\n",
            " [ 4  6 51  8]\n",
            " [ 7  8  8 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56  9  3  6]\n",
            " [11 44 12  7]\n",
            " [ 2  5 54  8]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 49  9  5]\n",
            " [ 3  6 53  7]\n",
            " [10  6  8 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  8  3  6]\n",
            " [12 48  9  5]\n",
            " [ 3  6 54  6]\n",
            " [10  9  6 58]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57 10  2  5]\n",
            " [10 48 12  4]\n",
            " [ 4  6 50  9]\n",
            " [11  5  6 61]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 11  2  6]\n",
            " [11 46 10  7]\n",
            " [ 1  4 56  8]\n",
            " [10  6  7 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 12  2  6]\n",
            " [ 9 49 10  6]\n",
            " [ 3  5 54  7]\n",
            " [10 10  7 56]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 51 10  3]\n",
            " [ 4  6 51  8]\n",
            " [ 9  8  6 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8164705882352942\n",
            "0.5466666666666666\n",
            "\n",
            "[[37 12 10 15]\n",
            " [ 9 39 17  9]\n",
            " [ 8  7 43 11]\n",
            " [13 14 11 45]]\n",
            "estimator: 6\n",
            "0.9670588235294117\n",
            "0.6633333333333333\n",
            "\n",
            "[[51 12  6  5]\n",
            " [11 48 10  5]\n",
            " [ 6  3 50 10]\n",
            " [17  9  7 50]]\n",
            "estimator: 11\n",
            "0.9894117647058823\n",
            "0.6666666666666666\n",
            "\n",
            "[[52 11  2  9]\n",
            " [ 9 45 11  9]\n",
            " [ 6  8 47  8]\n",
            " [11  7  9 56]]\n",
            "estimator: 16\n",
            "0.9941176470588236\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  8  5  3]\n",
            " [ 9 52 10  3]\n",
            " [ 4  7 51  7]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 21\n",
            "0.9964705882352941\n",
            "0.6866666666666666\n",
            "\n",
            "[[54 10  2  8]\n",
            " [11 47 11  5]\n",
            " [ 5  5 49 10]\n",
            " [12  7  8 56]]\n",
            "estimator: 26\n",
            "0.9994117647058823\n",
            "0.7066666666666667\n",
            "\n",
            "[[58  6  4  6]\n",
            " [12 45 12  5]\n",
            " [ 3  6 50 10]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.7333333333333333\n",
            "\n",
            "[[54  9  4  7]\n",
            " [ 9 50 10  5]\n",
            " [ 3  5 57  4]\n",
            " [ 7 10  7 59]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  9  4  6]\n",
            " [11 46 12  5]\n",
            " [ 6  5 52  6]\n",
            " [13  4  7 59]]\n",
            "estimator: 41\n",
            "0.9988235294117647\n",
            "0.73\n",
            "\n",
            "[[57  8  1  8]\n",
            " [11 48  9  6]\n",
            " [ 3  7 52  7]\n",
            " [ 8  7  6 62]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[56 10  2  6]\n",
            " [ 9 49 11  5]\n",
            " [ 2  5 55  7]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[57  9  4  4]\n",
            " [11 49  9  5]\n",
            " [ 2  5 54  8]\n",
            " [10 11  8 54]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[53 10  3  8]\n",
            " [12 46 11  5]\n",
            " [ 5  6 51  7]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 61\n",
            "0.9994117647058823\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 10  2  8]\n",
            " [ 9 50 10  5]\n",
            " [ 4  5 52  8]\n",
            " [ 8  7 12 56]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[58  9  3  4]\n",
            " [10 49 10  5]\n",
            " [ 2  5 53  9]\n",
            " [10  6  5 62]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  8  3  5]\n",
            " [10 49 11  4]\n",
            " [ 3  5 53  8]\n",
            " [10  7  8 58]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[59  7  1  7]\n",
            " [10 50  9  5]\n",
            " [ 4  7 50  8]\n",
            " [ 9  9  6 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 48 11  5]\n",
            " [ 4  6 52  7]\n",
            " [10  6 10 57]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 10  5  4]\n",
            " [10 50  8  6]\n",
            " [ 3  6 51  9]\n",
            " [ 9  9  6 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[55  9  2  8]\n",
            " [11 47 10  6]\n",
            " [ 2  7 53  7]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[58  7  3  6]\n",
            " [10 49 11  4]\n",
            " [ 3  4 54  8]\n",
            " [ 9  6  8 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8276470588235294\n",
            "0.58\n",
            "\n",
            "[[47 14  9  4]\n",
            " [11 37 14 12]\n",
            " [ 7 12 38 12]\n",
            " [ 7 11 13 52]]\n",
            "estimator: 6\n",
            "0.9735294117647059\n",
            "0.6833333333333333\n",
            "\n",
            "[[57 10  4  3]\n",
            " [14 46  9  5]\n",
            " [ 6 10 47  6]\n",
            " [12  6 10 55]]\n",
            "estimator: 11\n",
            "0.9911764705882353\n",
            "0.7033333333333334\n",
            "\n",
            "[[53  9  3  9]\n",
            " [12 52  7  3]\n",
            " [ 6  7 48  8]\n",
            " [10  8  7 58]]\n",
            "estimator: 16\n",
            "0.9976470588235294\n",
            "0.72\n",
            "\n",
            "[[59  6  5  4]\n",
            " [ 8 49 10  7]\n",
            " [ 1  7 54  7]\n",
            " [ 8 12  9 54]]\n",
            "estimator: 21\n",
            "0.9952941176470588\n",
            "0.69\n",
            "\n",
            "[[53 10  3  8]\n",
            " [10 47 12  5]\n",
            " [ 5  5 52  7]\n",
            " [10  9  9 55]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 47 11  6]\n",
            " [ 4  5 51  9]\n",
            " [ 7  7  7 62]]\n",
            "estimator: 31\n",
            "0.9982352941176471\n",
            "0.7033333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 48 11  5]\n",
            " [ 3  7 51  8]\n",
            " [ 9  9  9 56]]\n",
            "estimator: 36\n",
            "0.9982352941176471\n",
            "0.7133333333333334\n",
            "\n",
            "[[57  7  2  8]\n",
            " [10 46 14  4]\n",
            " [ 4  5 52  8]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 10  2  8]\n",
            " [10 49 10  5]\n",
            " [ 4  6 50  9]\n",
            " [ 9  5  8 61]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[53 11  2  8]\n",
            " [ 9 50 10  5]\n",
            " [ 2  5 54  8]\n",
            " [10  9  6 58]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54  9  3  8]\n",
            " [11 49 10  4]\n",
            " [ 3  6 53  7]\n",
            " [10  7  7 59]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 11  2  6]\n",
            " [ 8 51 10  5]\n",
            " [ 4  6 50  9]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[58  8  3  5]\n",
            " [11 48 11  4]\n",
            " [ 3  6 54  6]\n",
            " [ 8  8  6 61]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[51 14  3  6]\n",
            " [ 8 52 10  4]\n",
            " [ 4  7 50  8]\n",
            " [10  7  7 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[53 12  4  5]\n",
            " [11 47 11  5]\n",
            " [ 2  5 55  7]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56  9  4  5]\n",
            " [11 46 11  6]\n",
            " [ 3  5 54  7]\n",
            " [10  7  6 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  7  3  6]\n",
            " [ 9 51  9  5]\n",
            " [ 3  6 51  9]\n",
            " [ 7  9  7 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56  8  2  8]\n",
            " [11 45 11  7]\n",
            " [ 3  5 54  7]\n",
            " [10  7  7 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  2  6]\n",
            " [10 48 10  6]\n",
            " [ 4  7 51  7]\n",
            " [10  8  6 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57 10  3  4]\n",
            " [10 47 11  6]\n",
            " [ 4  7 51  7]\n",
            " [ 8  6  7 62]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8382352941176471\n",
            "0.57\n",
            "\n",
            "[[45 11  6 12]\n",
            " [15 33 15 11]\n",
            " [ 2  6 45 16]\n",
            " [ 8 10 17 48]]\n",
            "estimator: 6\n",
            "0.9694117647058823\n",
            "0.69\n",
            "\n",
            "[[55  8  2  9]\n",
            " [15 44 10  5]\n",
            " [ 5  7 52  5]\n",
            " [12  8  7 56]]\n",
            "estimator: 11\n",
            "0.9888235294117647\n",
            "0.6733333333333333\n",
            "\n",
            "[[49 11  5  9]\n",
            " [11 45 13  5]\n",
            " [ 5  5 51  8]\n",
            " [ 7  8 11 57]]\n",
            "estimator: 16\n",
            "0.9982352941176471\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  7  2 10]\n",
            " [10 46 12  6]\n",
            " [ 3  6 54  6]\n",
            " [11  9  6 57]]\n",
            "estimator: 21\n",
            "0.9970588235294118\n",
            "0.7166666666666667\n",
            "\n",
            "[[56  8  2  8]\n",
            " [11 48 12  3]\n",
            " [ 1  4 53 11]\n",
            " [ 9  9  7 58]]\n",
            "estimator: 26\n",
            "0.9994117647058823\n",
            "0.69\n",
            "\n",
            "[[52  9  5  8]\n",
            " [12 45 12  5]\n",
            " [ 4  7 51  7]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[53 11  3  7]\n",
            " [12 43 14  5]\n",
            " [ 2  5 53  9]\n",
            " [ 8  8  6 61]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[60  7  2  5]\n",
            " [12 48 10  4]\n",
            " [ 4  6 50  9]\n",
            " [ 7  8  7 61]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  8  5  5]\n",
            " [10 48 12  4]\n",
            " [ 2  4 55  8]\n",
            " [ 7 10  8 58]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[55 10  4  5]\n",
            " [ 8 52  9  5]\n",
            " [ 3  5 54  7]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 51\n",
            "0.9994117647058823\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  2  6]\n",
            " [ 9 47 13  5]\n",
            " [ 4  7 51  7]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 48 11  5]\n",
            " [ 3  5 52  9]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[59  8  2  5]\n",
            " [ 9 50 12  3]\n",
            " [ 3  5 54  7]\n",
            " [10  9  7 57]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  9  3  5]\n",
            " [ 9 49 11  5]\n",
            " [ 4  6 52  7]\n",
            " [11  9  6 57]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[53 12  3  6]\n",
            " [10 49 11  4]\n",
            " [ 3  7 50  9]\n",
            " [ 8  9  6 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 10  4  5]\n",
            " [10 47 10  7]\n",
            " [ 3  5 54  7]\n",
            " [ 9  9  6 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  9  2  6]\n",
            " [ 9 50  9  6]\n",
            " [ 4  5 49 11]\n",
            " [ 8  9  6 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56  9  3  6]\n",
            " [12 48  9  5]\n",
            " [ 4  4 52  9]\n",
            " [ 8  6  9 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[57  9  3  5]\n",
            " [10 51  9  4]\n",
            " [ 2  7 53  7]\n",
            " [11  7  6 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 46 12  5]\n",
            " [ 4  4 54  7]\n",
            " [ 9  7  8 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8105882352941176\n",
            "0.5266666666666666\n",
            "\n",
            "[[39 13  7 15]\n",
            " [13 35 14 12]\n",
            " [11  8 41  9]\n",
            " [13  9 18 43]]\n",
            "estimator: 6\n",
            "0.97\n",
            "0.6433333333333333\n",
            "\n",
            "[[51 11  3  9]\n",
            " [15 46  9  4]\n",
            " [ 5 10 47  7]\n",
            " [11 13 10 49]]\n",
            "estimator: 11\n",
            "0.991764705882353\n",
            "0.7133333333333334\n",
            "\n",
            "[[53  9  6  6]\n",
            " [13 48 10  3]\n",
            " [ 4  5 54  6]\n",
            " [ 7  6 11 59]]\n",
            "estimator: 16\n",
            "0.9958823529411764\n",
            "0.6966666666666667\n",
            "\n",
            "[[52 12  4  6]\n",
            " [12 47 11  4]\n",
            " [ 1  4 56  8]\n",
            " [ 9 12  8 54]]\n",
            "estimator: 21\n",
            "0.9982352941176471\n",
            "0.6833333333333333\n",
            "\n",
            "[[54 11  2  7]\n",
            " [12 42 13  7]\n",
            " [ 4  7 52  6]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 26\n",
            "0.9994117647058823\n",
            "0.6966666666666667\n",
            "\n",
            "[[53 10  4  7]\n",
            " [12 45 10  7]\n",
            " [ 3  7 51  8]\n",
            " [ 7  7  9 60]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.71\n",
            "\n",
            "[[58  7  2  7]\n",
            " [12 46 12  4]\n",
            " [ 4  6 52  7]\n",
            " [ 8  8 10 57]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.6966666666666667\n",
            "\n",
            "[[53 12  2  7]\n",
            " [11 48 10  5]\n",
            " [ 4  7 49  9]\n",
            " [ 8 11  5 59]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 10  1  9]\n",
            " [11 47 12  4]\n",
            " [ 4  5 52  8]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  8  3  7]\n",
            " [10 51  8  5]\n",
            " [ 3  5 52  9]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 51\n",
            "0.9988235294117647\n",
            "0.7\n",
            "\n",
            "[[55 10  3  6]\n",
            " [11 45 13  5]\n",
            " [ 4  6 51  8]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55  9  2  8]\n",
            " [10 48 10  6]\n",
            " [ 3  5 54  7]\n",
            " [10  6  8 59]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[58  9  2  5]\n",
            " [12 46 12  4]\n",
            " [ 3  5 52  9]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 10  3  7]\n",
            " [11 47 12  4]\n",
            " [ 4  5 53  7]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 48 11  5]\n",
            " [ 4  6 52  7]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7466666666666667\n",
            "\n",
            "[[57  9  3  5]\n",
            " [11 53  7  3]\n",
            " [ 4  4 54  7]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[56 10  3  5]\n",
            " [10 47 11  6]\n",
            " [ 3  7 54  5]\n",
            " [10  4  7 62]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  9  3  4]\n",
            " [11 50  9  4]\n",
            " [ 2  5 53  9]\n",
            " [11  5  8 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 48 10  5]\n",
            " [ 4  5 52  8]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  2  6]\n",
            " [10 49 10  5]\n",
            " [ 4  5 50 10]\n",
            " [ 9  8  7 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8058823529411765\n",
            "0.55\n",
            "\n",
            "[[37 12  8 17]\n",
            " [14 32 16 12]\n",
            " [ 4  5 44 16]\n",
            " [11 12  8 52]]\n",
            "estimator: 6\n",
            "0.9711764705882353\n",
            "0.66\n",
            "\n",
            "[[56  6  2 10]\n",
            " [15 41 12  6]\n",
            " [ 4  6 50  9]\n",
            " [13  9 10 51]]\n",
            "estimator: 11\n",
            "0.991764705882353\n",
            "0.6666666666666666\n",
            "\n",
            "[[52 11  3  8]\n",
            " [13 44 13  4]\n",
            " [ 3  9 50  7]\n",
            " [10 10  9 54]]\n",
            "estimator: 16\n",
            "0.9964705882352941\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  7  5  4]\n",
            " [13 46  9  6]\n",
            " [ 2  7 53  7]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 21\n",
            "0.9964705882352941\n",
            "0.6933333333333334\n",
            "\n",
            "[[54 10  4  6]\n",
            " [13 42 12  7]\n",
            " [ 6  4 51  8]\n",
            " [11  5  6 61]]\n",
            "estimator: 26\n",
            "0.9982352941176471\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 11  4  5]\n",
            " [12 46 10  6]\n",
            " [ 3  4 55  7]\n",
            " [10  8  8 57]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.6866666666666666\n",
            "\n",
            "[[52 12  2  8]\n",
            " [11 46 13  4]\n",
            " [ 4  6 51  8]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[61  5  3  5]\n",
            " [11 47 11  5]\n",
            " [ 5  4 53  7]\n",
            " [ 9 10  8 56]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 10  3  7]\n",
            " [10 49  8  7]\n",
            " [ 2  7 52  8]\n",
            " [ 9  6 10 58]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.7333333333333333\n",
            "\n",
            "[[56  7  5  6]\n",
            " [ 8 50 11  5]\n",
            " [ 4  2 54  9]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[58  7  2  7]\n",
            " [11 47 11  5]\n",
            " [ 3  5 52  9]\n",
            " [ 9  6 10 58]]\n",
            "estimator: 56\n",
            "0.9994117647058823\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  1  6]\n",
            " [11 49  9  5]\n",
            " [ 3  6 53  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 11  2  6]\n",
            " [10 49 10  5]\n",
            " [ 4  8 51  6]\n",
            " [11  5  9 58]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[52 12  2  8]\n",
            " [10 48  9  7]\n",
            " [ 4  5 53  7]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57 10  2  5]\n",
            " [10 47 12  5]\n",
            " [ 5  7 49  8]\n",
            " [ 7  8  6 62]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 12  2  6]\n",
            " [11 46 10  7]\n",
            " [ 2  6 52  9]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57  7  3  7]\n",
            " [10 48 10  6]\n",
            " [ 2  5 53  9]\n",
            " [10  8  7 58]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56  8  3  7]\n",
            " [11 47 10  6]\n",
            " [ 3  8 50  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 49 11  4]\n",
            " [ 3  5 54  7]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [12 46 10  6]\n",
            " [ 3  5 54  7]\n",
            " [ 8  7  7 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8176470588235294\n",
            "0.56\n",
            "\n",
            "[[44 14  8  8]\n",
            " [12 35 14 13]\n",
            " [ 5  9 43 12]\n",
            " [16 13  8 46]]\n",
            "estimator: 6\n",
            "0.9652941176470589\n",
            "0.6266666666666667\n",
            "\n",
            "[[46 14  5  9]\n",
            " [10 44 16  4]\n",
            " [ 9  9 47  4]\n",
            " [17  7  8 51]]\n",
            "estimator: 11\n",
            "0.9888235294117647\n",
            "0.68\n",
            "\n",
            "[[49 12  4  9]\n",
            " [13 44 13  4]\n",
            " [ 6  6 52  5]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 16\n",
            "0.9941176470588236\n",
            "0.6833333333333333\n",
            "\n",
            "[[54 10  2  8]\n",
            " [15 43 12  4]\n",
            " [ 2  4 54  9]\n",
            " [10 11  8 54]]\n",
            "estimator: 21\n",
            "0.9964705882352941\n",
            "0.73\n",
            "\n",
            "[[58  9  1  6]\n",
            " [12 50  9  3]\n",
            " [ 2  6 52  9]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.71\n",
            "\n",
            "[[57  9  4  4]\n",
            " [ 9 47 11  7]\n",
            " [ 4  6 51  8]\n",
            " [13  6  6 58]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.71\n",
            "\n",
            "[[56  9  3  6]\n",
            " [11 46 12  5]\n",
            " [ 5  5 50  9]\n",
            " [ 8  4 10 61]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7133333333333334\n",
            "\n",
            "[[56 10  1  7]\n",
            " [10 49 11  4]\n",
            " [ 4  6 51  8]\n",
            " [10  8  7 58]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.69\n",
            "\n",
            "[[55 13  2  4]\n",
            " [ 9 46 12  7]\n",
            " [ 4  5 50 10]\n",
            " [11  7  9 56]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.71\n",
            "\n",
            "[[58  7  2  7]\n",
            " [14 44 11  5]\n",
            " [ 3  5 53  8]\n",
            " [10  7  8 58]]\n",
            "estimator: 51\n",
            "0.9994117647058823\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 10  3  7]\n",
            " [11 47 10  6]\n",
            " [ 3  5 54  7]\n",
            " [ 6 10  7 60]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[58  6  4  6]\n",
            " [10 50  9  5]\n",
            " [ 3  6 53  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[58  8  2  6]\n",
            " [10 49 11  4]\n",
            " [ 3  5 53  8]\n",
            " [ 8  5  8 62]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[58  7  3  6]\n",
            " [10 49 10  5]\n",
            " [ 3  5 52  9]\n",
            " [10  8  7 58]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  9  3  5]\n",
            " [12 48 10  4]\n",
            " [ 4  6 53  6]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[57  9  3  5]\n",
            " [10 48 11  5]\n",
            " [ 3  6 51  9]\n",
            " [10  7  7 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[59  7  2  6]\n",
            " [10 50 10  4]\n",
            " [ 3  6 50 10]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 51 11  2]\n",
            " [ 4  5 52  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 51 10  3]\n",
            " [ 4  6 50  9]\n",
            " [11  6  7 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[54 11  2  7]\n",
            " [12 47 10  5]\n",
            " [ 3  5 54  7]\n",
            " [ 9  6  7 61]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8264705882352941\n",
            "0.5733333333333334\n",
            "\n",
            "[[39 14  5 16]\n",
            " [ 9 44 14  7]\n",
            " [ 5  8 42 14]\n",
            " [13 10 13 47]]\n",
            "estimator: 6\n",
            "0.9647058823529412\n",
            "0.6966666666666667\n",
            "\n",
            "[[57  7  4  6]\n",
            " [12 44 12  6]\n",
            " [ 7  6 54  2]\n",
            " [ 8  8 13 54]]\n",
            "estimator: 11\n",
            "0.9870588235294118\n",
            "0.6733333333333333\n",
            "\n",
            "[[49 13  5  7]\n",
            " [11 49  9  5]\n",
            " [ 5  9 51  4]\n",
            " [10  6 14 53]]\n",
            "estimator: 16\n",
            "0.9958823529411764\n",
            "0.69\n",
            "\n",
            "[[52 12  4  6]\n",
            " [ 9 48 13  4]\n",
            " [ 4  7 52  6]\n",
            " [ 9 13  6 55]]\n",
            "estimator: 21\n",
            "0.9964705882352941\n",
            "0.6966666666666667\n",
            "\n",
            "[[55  9  4  6]\n",
            " [ 9 45 13  7]\n",
            " [ 3  5 55  6]\n",
            " [12  8  9 54]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7166666666666667\n",
            "\n",
            "[[58  7  5  4]\n",
            " [10 48 12  4]\n",
            " [ 3  7 51  8]\n",
            " [ 9  6 10 58]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[54 10  1  9]\n",
            " [11 51  9  3]\n",
            " [ 4  7 50  8]\n",
            " [ 9  9 10 55]]\n",
            "estimator: 36\n",
            "0.9982352941176471\n",
            "0.71\n",
            "\n",
            "[[55 10  4  5]\n",
            " [11 49  9  5]\n",
            " [ 3  6 53  7]\n",
            " [ 9 10  8 56]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 47 11  6]\n",
            " [ 4  5 52  8]\n",
            " [ 8 10  7 58]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[57  9  3  5]\n",
            " [11 45 13  5]\n",
            " [ 3  5 53  8]\n",
            " [10  9  7 57]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[55  8  2  9]\n",
            " [10 45 11  8]\n",
            " [ 2  4 54  9]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 12  1  7]\n",
            " [11 47 12  4]\n",
            " [ 3  6 53  7]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[55 12  2  5]\n",
            " [ 8 51  9  6]\n",
            " [ 4  5 52  8]\n",
            " [ 9  7 10 57]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 47 11  5]\n",
            " [ 3  5 53  8]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[58  8  2  6]\n",
            " [ 9 49 10  6]\n",
            " [ 1  7 54  7]\n",
            " [ 7  8 10 58]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  1  7]\n",
            " [11 49  9  5]\n",
            " [ 3  6 52  8]\n",
            " [ 8 10  7 58]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.75\n",
            "\n",
            "[[57  8  3  6]\n",
            " [ 9 52  9  4]\n",
            " [ 2  5 56  6]\n",
            " [10  6  7 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 11  3  6]\n",
            " [12 46 11  5]\n",
            " [ 4  6 51  8]\n",
            " [ 9  5  9 60]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56  9  1  8]\n",
            " [11 48 10  5]\n",
            " [ 4  4 53  8]\n",
            " [10  6  9 58]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[54 10  3  7]\n",
            " [ 9 48 12  5]\n",
            " [ 2  6 54  7]\n",
            " [10  7  6 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.821764705882353\n",
            "0.5766666666666667\n",
            "\n",
            "[[44 16  7  7]\n",
            " [ 7 41 17  9]\n",
            " [15  7 39  8]\n",
            " [12 13  9 49]]\n",
            "estimator: 6\n",
            "0.9647058823529412\n",
            "0.6133333333333333\n",
            "\n",
            "[[46 15  8  5]\n",
            " [16 40 10  8]\n",
            " [ 9 10 45  5]\n",
            " [11  9 10 53]]\n",
            "estimator: 11\n",
            "0.9882352941176471\n",
            "0.6833333333333333\n",
            "\n",
            "[[55  8  5  6]\n",
            " [12 45 12  5]\n",
            " [ 5  5 49 10]\n",
            " [ 9  8 10 56]]\n",
            "estimator: 16\n",
            "0.9941176470588236\n",
            "0.6833333333333333\n",
            "\n",
            "[[55  9  4  6]\n",
            " [14 44 11  5]\n",
            " [ 6  7 50  6]\n",
            " [10  9  8 56]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 48 11  5]\n",
            " [ 3  4 56  6]\n",
            " [10  7  8 58]]\n",
            "estimator: 26\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54  9  2  9]\n",
            " [ 9 46 14  5]\n",
            " [ 2  5 54  8]\n",
            " [ 9  6  8 60]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[58  7  1  8]\n",
            " [13 46 10  5]\n",
            " [ 4  4 52  9]\n",
            " [10  7  9 57]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[58  8  2  6]\n",
            " [10 49 11  4]\n",
            " [ 5  7 49  8]\n",
            " [ 9 10  6 58]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56 11  3  4]\n",
            " [10 49 11  4]\n",
            " [ 5  6 50  8]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 46\n",
            "0.9988235294117647\n",
            "0.7\n",
            "\n",
            "[[54  9  2  9]\n",
            " [11 48 11  4]\n",
            " [ 3  7 50  9]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[57  8  3  6]\n",
            " [11 45 13  5]\n",
            " [ 4  5 53  7]\n",
            " [ 8  9  8 58]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[60  7  3  4]\n",
            " [ 8 48 12  6]\n",
            " [ 4  7 49  9]\n",
            " [10  7  8 58]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 51  9  4]\n",
            " [ 4  7 51  7]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[56  9  2  7]\n",
            " [ 9 49  9  7]\n",
            " [ 3  7 51  8]\n",
            " [10 10  6 57]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[54  8  2 10]\n",
            " [10 49 11  4]\n",
            " [ 3  4 55  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[56  8  2  8]\n",
            " [ 7 51 12  4]\n",
            " [ 3  6 53  7]\n",
            " [11  6  6 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 48 12  4]\n",
            " [ 3  4 53  9]\n",
            " [10  6  8 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56  8  2  8]\n",
            " [ 9 48 12  5]\n",
            " [ 4  5 53  7]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[59  8  2  5]\n",
            " [11 48 10  5]\n",
            " [ 4  5 52  8]\n",
            " [10  3 10 60]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[53 11  5  5]\n",
            " [11 49 11  3]\n",
            " [ 2  5 54  8]\n",
            " [ 9  8  6 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8135294117647058\n",
            "0.5166666666666667\n",
            "\n",
            "[[42 11  6 15]\n",
            " [12 37 17  8]\n",
            " [ 6 10 35 18]\n",
            " [14 20  8 41]]\n",
            "estimator: 6\n",
            "0.9641176470588235\n",
            "0.64\n",
            "\n",
            "[[50  9  3 12]\n",
            " [14 39 14  7]\n",
            " [ 9  8 47  5]\n",
            " [10 11  6 56]]\n",
            "estimator: 11\n",
            "0.9882352941176471\n",
            "0.6933333333333334\n",
            "\n",
            "[[55  9  2  8]\n",
            " [ 9 49 14  2]\n",
            " [ 8  5 47  9]\n",
            " [11  9  6 57]]\n",
            "estimator: 16\n",
            "0.9976470588235294\n",
            "0.6833333333333333\n",
            "\n",
            "[[55 10  3  6]\n",
            " [12 41 16  5]\n",
            " [ 5  6 51  7]\n",
            " [ 8 10  7 58]]\n",
            "estimator: 21\n",
            "0.9982352941176471\n",
            "0.6833333333333333\n",
            "\n",
            "[[54 11  2  7]\n",
            " [12 44 11  7]\n",
            " [ 3  6 51  9]\n",
            " [ 7 11  9 56]]\n",
            "estimator: 26\n",
            "0.9982352941176471\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  5  2  9]\n",
            " [12 51  9  2]\n",
            " [ 4  5 52  8]\n",
            " [10  6 10 57]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.6966666666666667\n",
            "\n",
            "[[54  8  7  5]\n",
            " [10 48 10  6]\n",
            " [ 6  6 48  9]\n",
            " [10  8  6 59]]\n",
            "estimator: 36\n",
            "0.9988235294117647\n",
            "0.6866666666666666\n",
            "\n",
            "[[54 10  4  6]\n",
            " [10 46 12  6]\n",
            " [ 5  5 52  7]\n",
            " [10  9 10 54]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 12  2  6]\n",
            " [11 47 10  6]\n",
            " [ 5  5 49 10]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[54 11  3  6]\n",
            " [11 50  8  5]\n",
            " [ 4  7 51  7]\n",
            " [10  8  8 57]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[53 11  4  6]\n",
            " [12 47 11  4]\n",
            " [ 4  6 53  6]\n",
            " [ 8 10  5 60]]\n",
            "estimator: 56\n",
            "0.9994117647058823\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  9  1  8]\n",
            " [ 9 49 11  5]\n",
            " [ 3  5 52  9]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[54 10  4  6]\n",
            " [10 48 10  6]\n",
            " [ 3  4 55  7]\n",
            " [10  7  6 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[58  8  2  6]\n",
            " [10 48 10  6]\n",
            " [ 2  5 54  8]\n",
            " [ 7 10  5 61]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 49 10  4]\n",
            " [ 3  5 52  9]\n",
            " [ 8  6  7 62]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 49 11  4]\n",
            " [ 4  5 52  8]\n",
            " [ 8  7  8 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[57 10  2  5]\n",
            " [12 47 10  5]\n",
            " [ 1  5 54  9]\n",
            " [ 7  8  8 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 46 11  6]\n",
            " [ 4  6 53  6]\n",
            " [11  7  6 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[55 10  4  5]\n",
            " [11 50 10  3]\n",
            " [ 2  5 55  7]\n",
            " [ 9  5  9 60]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[53 13  2  6]\n",
            " [12 50  9  3]\n",
            " [ 5  6 51  7]\n",
            " [10  6  8 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8329411764705882\n",
            "0.5566666666666666\n",
            "\n",
            "[[34 10 15 15]\n",
            " [12 32 14 16]\n",
            " [ 8  6 46  9]\n",
            " [10  7 11 55]]\n",
            "estimator: 6\n",
            "0.9576470588235294\n",
            "0.65\n",
            "\n",
            "[[50 15  3  6]\n",
            " [11 45 11  7]\n",
            " [13  6 43  7]\n",
            " [ 8  9  9 57]]\n",
            "estimator: 11\n",
            "0.9876470588235294\n",
            "0.73\n",
            "\n",
            "[[56 10  2  6]\n",
            " [15 49  7  3]\n",
            " [ 6  5 53  5]\n",
            " [ 5  9  8 61]]\n",
            "estimator: 16\n",
            "0.9952941176470588\n",
            "0.6866666666666666\n",
            "\n",
            "[[54 12  1  7]\n",
            " [10 47 11  6]\n",
            " [ 3  5 53  8]\n",
            " [10  9 12 52]]\n",
            "estimator: 21\n",
            "0.9982352941176471\n",
            "0.69\n",
            "\n",
            "[[55 10  5  4]\n",
            " [11 46 11  6]\n",
            " [ 7  5 49  8]\n",
            " [ 9  9  8 57]]\n",
            "estimator: 26\n",
            "0.9976470588235294\n",
            "0.73\n",
            "\n",
            "[[56 10  3  5]\n",
            " [ 6 51 12  5]\n",
            " [ 4  6 51  8]\n",
            " [10  6  6 61]]\n",
            "estimator: 31\n",
            "0.9994117647058823\n",
            "0.71\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 49 10  5]\n",
            " [ 4  5 51  9]\n",
            " [ 8  9  9 57]]\n",
            "estimator: 36\n",
            "0.9988235294117647\n",
            "0.7366666666666667\n",
            "\n",
            "[[58  8  4  4]\n",
            " [ 9 50  9  6]\n",
            " [ 3  5 53  8]\n",
            " [10  4  9 60]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.73\n",
            "\n",
            "[[57  7  3  7]\n",
            " [10 50  9  5]\n",
            " [ 3  7 51  8]\n",
            " [ 9  5  8 61]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7466666666666667\n",
            "\n",
            "[[58  7  3  6]\n",
            " [10 52  9  3]\n",
            " [ 2  7 52  8]\n",
            " [ 7  8  6 62]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56  9  3  6]\n",
            " [11 47 10  6]\n",
            " [ 5  6 51  7]\n",
            " [ 6  6  9 62]]\n",
            "estimator: 56\n",
            "0.9994117647058823\n",
            "0.7233333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [10 48  9  7]\n",
            " [ 2  6 52  9]\n",
            " [ 9  7  6 61]]\n",
            "estimator: 61\n",
            "0.9994117647058823\n",
            "0.7166666666666667\n",
            "\n",
            "[[60  7  1  6]\n",
            " [11 44 13  6]\n",
            " [ 5  5 51  8]\n",
            " [10  5  8 60]]\n",
            "estimator: 66\n",
            "0.9994117647058823\n",
            "0.7033333333333334\n",
            "\n",
            "[[55 10  3  6]\n",
            " [10 47 10  7]\n",
            " [ 4  6 50  9]\n",
            " [10  8  6 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.6966666666666667\n",
            "\n",
            "[[55 12  3  4]\n",
            " [10 44 13  7]\n",
            " [ 4  6 50  9]\n",
            " [10  7  6 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[54 12  3  5]\n",
            " [10 50  9  5]\n",
            " [ 4  6 52  7]\n",
            " [12  7  7 57]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[58  7  2  7]\n",
            " [ 9 48  9  8]\n",
            " [ 3  4 54  8]\n",
            " [ 7  8 11 57]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[59  8  4  3]\n",
            " [11 47 11  5]\n",
            " [ 4  4 52  9]\n",
            " [ 8  8  6 61]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[57  8  2  7]\n",
            " [11 47 11  5]\n",
            " [ 5  6 51  7]\n",
            " [ 7  9  9 58]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  8  2  7]\n",
            " [ 9 50 10  5]\n",
            " [ 4  6 51  8]\n",
            " [ 8  8  8 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8241176470588235\n",
            "0.55\n",
            "\n",
            "[[33 16 10 15]\n",
            " [12 40 15  7]\n",
            " [ 6 11 42 10]\n",
            " [ 9 14 10 50]]\n",
            "estimator: 6\n",
            "0.971764705882353\n",
            "0.67\n",
            "\n",
            "[[54 11  3  6]\n",
            " [15 48  6  5]\n",
            " [ 7  9 45  8]\n",
            " [10 11  8 54]]\n",
            "estimator: 11\n",
            "0.991764705882353\n",
            "0.7033333333333334\n",
            "\n",
            "[[55  9  5  5]\n",
            " [10 48 12  4]\n",
            " [ 8  4 53  4]\n",
            " [ 9 10  9 55]]\n",
            "estimator: 16\n",
            "0.9958823529411764\n",
            "0.6833333333333333\n",
            "\n",
            "[[54 10  5  5]\n",
            " [11 46 10  7]\n",
            " [ 5  5 50  9]\n",
            " [11  7 10 55]]\n",
            "estimator: 21\n",
            "0.9964705882352941\n",
            "0.7166666666666667\n",
            "\n",
            "[[56 10  4  4]\n",
            " [11 46 11  6]\n",
            " [ 2  6 55  6]\n",
            " [ 8  8  9 58]]\n",
            "estimator: 26\n",
            "0.9994117647058823\n",
            "0.7\n",
            "\n",
            "[[53  8  5  8]\n",
            " [12 49  8  5]\n",
            " [ 5  6 51  7]\n",
            " [12  7  7 57]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55  8  3  8]\n",
            " [ 9 47 14  4]\n",
            " [ 2  4 56  7]\n",
            " [ 9  7  9 58]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7033333333333334\n",
            "\n",
            "[[53  8  5  8]\n",
            " [15 46  8  5]\n",
            " [ 1  6 53  9]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.72\n",
            "\n",
            "[[58  7  2  7]\n",
            " [10 46 12  6]\n",
            " [ 2  6 52  9]\n",
            " [ 7  8  8 60]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[60  6  4  4]\n",
            " [10 51 10  3]\n",
            " [ 4  6 50  9]\n",
            " [10  8  6 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[56  6  4  8]\n",
            " [12 47 11  4]\n",
            " [ 4  5 52  8]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 10  4  6]\n",
            " [10 47 11  6]\n",
            " [ 3  5 53  8]\n",
            " [ 7  5 11 60]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55 11  2  6]\n",
            " [11 47 11  5]\n",
            " [ 4  7 51  7]\n",
            " [ 8  5  7 63]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 10  4  6]\n",
            " [ 9 49 11  5]\n",
            " [ 3  7 52  7]\n",
            " [10  5  8 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55 11  3  5]\n",
            " [12 47 10  5]\n",
            " [ 4  6 50  9]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[52 13  3  6]\n",
            " [10 47 11  6]\n",
            " [ 3  5 53  8]\n",
            " [ 7  7  9 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[53 10  2  9]\n",
            " [11 49 11  3]\n",
            " [ 3  7 52  7]\n",
            " [10  6  7 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[54 12  2  6]\n",
            " [ 8 51 11  4]\n",
            " [ 3  4 53  9]\n",
            " [ 9  9  7 58]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 49  9  5]\n",
            " [ 4  6 52  7]\n",
            " [10  6  8 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[58  7  5  4]\n",
            " [ 8 50 12  4]\n",
            " [ 4  5 51  9]\n",
            " [10  8  6 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8229411764705883\n",
            "0.5666666666666667\n",
            "\n",
            "[[47 10  6 11]\n",
            " [14 35 15 10]\n",
            " [ 4 11 39 15]\n",
            " [12  7 15 49]]\n",
            "estimator: 6\n",
            "0.9641176470588235\n",
            "0.6433333333333333\n",
            "\n",
            "[[55 10  1  8]\n",
            " [15 40 12  7]\n",
            " [ 5  9 46  9]\n",
            " [13  8 10 52]]\n",
            "estimator: 11\n",
            "0.9870588235294118\n",
            "0.6966666666666667\n",
            "\n",
            "[[55  9  6  4]\n",
            " [11 47 10  6]\n",
            " [ 5  6 49  9]\n",
            " [10  8  7 58]]\n",
            "estimator: 16\n",
            "0.9958823529411764\n",
            "0.7233333333333334\n",
            "\n",
            "[[56  7  3  8]\n",
            " [10 49 10  5]\n",
            " [ 6  5 51  7]\n",
            " [ 6  9  7 61]]\n",
            "estimator: 21\n",
            "0.9982352941176471\n",
            "0.7\n",
            "\n",
            "[[55 10  2  7]\n",
            " [11 45 13  5]\n",
            " [ 5  7 49  8]\n",
            " [ 7  8  7 61]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7033333333333334\n",
            "\n",
            "[[54  8  4  8]\n",
            " [12 50 10  2]\n",
            " [ 6  6 48  9]\n",
            " [ 7  9  8 59]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.7266666666666667\n",
            "\n",
            "[[57  7  4  6]\n",
            " [10 48  9  7]\n",
            " [ 2  5 55  7]\n",
            " [ 8  9  8 58]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55  8  6  5]\n",
            " [12 47 12  3]\n",
            " [ 3  4 55  7]\n",
            " [10  8  9 56]]\n",
            "estimator: 41\n",
            "0.9994117647058823\n",
            "0.7033333333333334\n",
            "\n",
            "[[54 10  2  8]\n",
            " [10 48 10  6]\n",
            " [ 4  5 52  8]\n",
            " [10  8  8 57]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  8  2  9]\n",
            " [11 48 11  4]\n",
            " [ 5  6 50  8]\n",
            " [10  6  8 59]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[57  9  3  5]\n",
            " [11 45 12  6]\n",
            " [ 4  5 54  6]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7033333333333334\n",
            "\n",
            "[[56  9  3  6]\n",
            " [10 49 10  5]\n",
            " [ 4  8 49  8]\n",
            " [10  8  8 57]]\n",
            "estimator: 61\n",
            "0.9988235294117647\n",
            "0.69\n",
            "\n",
            "[[51 11  4  8]\n",
            " [10 48 11  5]\n",
            " [ 5  7 49  8]\n",
            " [ 9  7  8 59]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[55  9  3  7]\n",
            " [10 45 14  5]\n",
            " [ 3  7 50  9]\n",
            " [ 8  6  9 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 10  2  7]\n",
            " [ 9 48 11  6]\n",
            " [ 4  5 50 10]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.74\n",
            "\n",
            "[[58  7  3  6]\n",
            " [11 51  8  4]\n",
            " [ 4  6 51  8]\n",
            " [ 8  6  7 62]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[55 11  3  5]\n",
            " [11 48 10  5]\n",
            " [ 4  4 54  7]\n",
            " [ 9  7  6 61]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[58 10  3  3]\n",
            " [10 50  9  5]\n",
            " [ 5  5 52  7]\n",
            " [ 8  7  9 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[55  9  3  7]\n",
            " [12 46 12  4]\n",
            " [ 4  4 52  9]\n",
            " [10  7  7 59]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[56 12  3  3]\n",
            " [ 9 50 10  5]\n",
            " [ 3  5 55  6]\n",
            " [ 9  7  8 59]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.821764705882353\n",
            "0.55\n",
            "\n",
            "[[40 16 11  7]\n",
            " [18 35 12  9]\n",
            " [ 8  9 45  7]\n",
            " [15  9 14 45]]\n",
            "estimator: 6\n",
            "0.9652941176470589\n",
            "0.6433333333333333\n",
            "\n",
            "[[48 11  9  6]\n",
            " [12 44 13  5]\n",
            " [ 4  7 50  8]\n",
            " [ 9  9 14 51]]\n",
            "estimator: 11\n",
            "0.9870588235294118\n",
            "0.6666666666666666\n",
            "\n",
            "[[56  9  2  7]\n",
            " [13 45 10  6]\n",
            " [ 8  8 46  7]\n",
            " [12  9  9 53]]\n",
            "estimator: 16\n",
            "0.9958823529411764\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 10  6  4]\n",
            " [ 9 48 10  7]\n",
            " [ 8  6 49  6]\n",
            " [ 6  7  7 63]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.6833333333333333\n",
            "\n",
            "[[50 14  4  6]\n",
            " [10 45 12  7]\n",
            " [ 1  8 52  8]\n",
            " [ 9  8  8 58]]\n",
            "estimator: 26\n",
            "0.9976470588235294\n",
            "0.71\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 46 12  6]\n",
            " [ 3  7 54  5]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 31\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[57 10  2  5]\n",
            " [11 47 11  5]\n",
            " [ 4  3 55  7]\n",
            " [ 8  7 11 57]]\n",
            "estimator: 36\n",
            "0.9994117647058823\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 11  2  6]\n",
            " [10 49 11  4]\n",
            " [ 5  5 50  9]\n",
            " [ 8  5  7 63]]\n",
            "estimator: 41\n",
            "1.0\n",
            "0.7166666666666667\n",
            "\n",
            "[[54 10  3  7]\n",
            " [11 49 10  4]\n",
            " [ 2  4 54  9]\n",
            " [ 8  9  8 58]]\n",
            "estimator: 46\n",
            "0.9994117647058823\n",
            "0.73\n",
            "\n",
            "[[59  7  2  6]\n",
            " [11 47 11  5]\n",
            " [ 3  5 54  7]\n",
            " [ 8  9  7 59]]\n",
            "estimator: 51\n",
            "0.9994117647058823\n",
            "0.72\n",
            "\n",
            "[[57 10  3  4]\n",
            " [ 9 49 10  6]\n",
            " [ 4  6 53  6]\n",
            " [10  7  9 57]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.7333333333333333\n",
            "\n",
            "[[57 10  2  5]\n",
            " [11 48 11  4]\n",
            " [ 4  6 50  9]\n",
            " [ 7  4  7 65]]\n",
            "estimator: 61\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 11  2  6]\n",
            " [10 49 10  5]\n",
            " [ 3  6 53  7]\n",
            " [ 7  7  9 60]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 11  3  6]\n",
            " [11 47 12  4]\n",
            " [ 3  6 53  7]\n",
            " [10  6  7 60]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 50 10  3]\n",
            " [ 2  6 52  9]\n",
            " [ 8  7  8 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[56  9  2  7]\n",
            " [10 47 11  6]\n",
            " [ 5  7 50  7]\n",
            " [10  8  6 59]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55  9  3  7]\n",
            " [10 51 11  2]\n",
            " [ 5  5 51  8]\n",
            " [ 9  8  7 59]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.7366666666666667\n",
            "\n",
            "[[61  6  2  5]\n",
            " [12 47 10  5]\n",
            " [ 4  4 54  7]\n",
            " [ 9  9  6 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[55 11  1  7]\n",
            " [11 48  9  6]\n",
            " [ 3  6 52  8]\n",
            " [ 8  7  7 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.7266666666666667\n",
            "\n",
            "[[56  9  3  6]\n",
            " [11 47 10  6]\n",
            " [ 3  5 55  6]\n",
            " [10  6  7 60]]\n",
            "Feature: 96\n",
            "estimator: 1\n",
            "0.8129411764705883\n",
            "0.5533333333333333\n",
            "\n",
            "[[45  8 10 11]\n",
            " [13 34 19  8]\n",
            " [ 6 12 39 12]\n",
            " [13 10 12 48]]\n",
            "estimator: 6\n",
            "0.9694117647058823\n",
            "0.65\n",
            "\n",
            "[[52  7  4 11]\n",
            " [12 45  8  9]\n",
            " [ 8  8 47  6]\n",
            " [13 10  9 51]]\n",
            "estimator: 11\n",
            "0.99\n",
            "0.6966666666666667\n",
            "\n",
            "[[54  8  6  6]\n",
            " [ 8 48 11  7]\n",
            " [ 6  4 52  7]\n",
            " [10  6 12 55]]\n",
            "estimator: 16\n",
            "0.9964705882352941\n",
            "0.71\n",
            "\n",
            "[[54 11  2  7]\n",
            " [10 48 10  6]\n",
            " [ 4  2 57  6]\n",
            " [ 9  8 12 54]]\n",
            "estimator: 21\n",
            "0.9976470588235294\n",
            "0.6966666666666667\n",
            "\n",
            "[[58  8  1  7]\n",
            " [11 48  7  8]\n",
            " [ 7  8 47  7]\n",
            " [ 9 13  5 56]]\n",
            "estimator: 26\n",
            "0.9988235294117647\n",
            "0.7133333333333334\n",
            "\n",
            "[[51 12  3  8]\n",
            " [12 50  9  3]\n",
            " [ 4  3 56  6]\n",
            " [11  7  8 57]]\n",
            "estimator: 31\n",
            "0.9988235294117647\n",
            "0.7266666666666667\n",
            "\n",
            "[[55  7  4  8]\n",
            " [11 49  9  5]\n",
            " [ 3  4 53  9]\n",
            " [ 9  7  6 61]]\n",
            "estimator: 36\n",
            "1.0\n",
            "0.6833333333333333\n",
            "\n",
            "[[56 10  2  6]\n",
            " [12 44 11  7]\n",
            " [ 5  7 49  8]\n",
            " [ 9 10  8 56]]\n",
            "estimator: 41\n",
            "0.9988235294117647\n",
            "0.7133333333333334\n",
            "\n",
            "[[54 11  4  5]\n",
            " [10 49 10  5]\n",
            " [ 3  7 51  8]\n",
            " [ 9  7  7 60]]\n",
            "estimator: 46\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[54  9  5  6]\n",
            " [10 47 11  6]\n",
            " [ 3  7 52  7]\n",
            " [ 9  8  9 57]]\n",
            "estimator: 51\n",
            "1.0\n",
            "0.7\n",
            "\n",
            "[[54 10  2  8]\n",
            " [12 45 12  5]\n",
            " [ 4  6 51  8]\n",
            " [ 9  8  6 60]]\n",
            "estimator: 56\n",
            "1.0\n",
            "0.6933333333333334\n",
            "\n",
            "[[56 10  2  6]\n",
            " [11 44 14  5]\n",
            " [ 3  6 51  9]\n",
            " [10  9  7 57]]\n",
            "estimator: 61\n",
            "0.9994117647058823\n",
            "0.7033333333333334\n",
            "\n",
            "[[57  8  2  7]\n",
            " [10 47 12  5]\n",
            " [ 4  5 52  8]\n",
            " [ 9 10  9 55]]\n",
            "estimator: 66\n",
            "1.0\n",
            "0.72\n",
            "\n",
            "[[58  7  2  7]\n",
            " [11 49 10  4]\n",
            " [ 6  4 50  9]\n",
            " [ 9  6  9 59]]\n",
            "estimator: 71\n",
            "1.0\n",
            "0.7233333333333334\n",
            "\n",
            "[[55 10  2  7]\n",
            " [10 50  9  5]\n",
            " [ 4  4 52  9]\n",
            " [10  6  7 60]]\n",
            "estimator: 76\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  9  4  4]\n",
            " [10 48 11  5]\n",
            " [ 2  5 54  8]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 81\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55 10  1  8]\n",
            " [11 45 11  7]\n",
            " [ 4  5 53  7]\n",
            " [ 8  8  7 60]]\n",
            "estimator: 86\n",
            "1.0\n",
            "0.73\n",
            "\n",
            "[[57  8  2  7]\n",
            " [11 50  9  4]\n",
            " [ 3  6 53  7]\n",
            " [10  5  9 59]]\n",
            "estimator: 91\n",
            "1.0\n",
            "0.7066666666666667\n",
            "\n",
            "[[51 12  4  7]\n",
            " [10 50 11  3]\n",
            " [ 4  6 50  9]\n",
            " [ 9  6  7 61]]\n",
            "estimator: 96\n",
            "1.0\n",
            "0.71\n",
            "\n",
            "[[55  9  2  8]\n",
            " [11 46 12  5]\n",
            " [ 4  5 52  8]\n",
            " [ 8  7  8 60]]\n"
          ]
        }
      ],
      "source": [
        "for feature in range(1,120):\n",
        "  mod=SelectKBest(chi2,k=feature)\n",
        "  new1=mod.fit(x,y)\n",
        "  x_1=new1.transform(x)\n",
        "  x_train_, x_test_, y_train, y_test =0,0,0,0\n",
        "  x_train_, x_test_, y_train, y_test =train_test_split(x_1,classes,test_size=0.15,random_state=15)\n",
        "  print(\"Feature:\",estimator)\n",
        "  for estimator in range(1,100,5):\n",
        "    RF_model = RandomForestClassifier(n_estimators=estimator)\n",
        "    RF_model.fit(x_train,y_train)\n",
        "    #for train set\n",
        "    print(\"estimator:\",estimator)\n",
        "    pred_class_train = RF_model.predict(x_train)\n",
        "    # print (classification_report(y_train, pred_class_train))\n",
        "    train_accuracy.append(metrics.accuracy_score(y_train, pred_class_train))\n",
        "    print(metrics.accuracy_score(y_train, pred_class_train))\n",
        "    # #for test class\n",
        "    pred_class_test = RF_model.predict(x_test)\n",
        "    print(metrics.accuracy_score(y_test, pred_class_test))\n",
        "    # print (classification_report(y_test, pred_class_test))\n",
        "    # # print(y_test)\n",
        "    # # print(pred_class_test)\n",
        "    test_accuracy.append(metrics.accuracy_score(y_test, pred_class_test))\n",
        "    print()\n",
        "    # no_of_estimators.append(estimator)\n",
        "    results = confusion_matrix(y_test, pred_class_test)\n",
        "    print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F2BSVMqcXzW",
        "outputId": "b66c3e2c-3f1c-4060-a836-07a0337a0bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8188235294117647, 0.9729411764705882, 0.9923529411764705, 0.9952941176470588, 0.9976470588235294, 1.0, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8123529411764706, 0.9635294117647059, 0.9905882352941177, 0.9941176470588236, 0.9970588235294118, 1.0, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8294117647058824, 0.9705882352941176, 0.9923529411764705, 0.9964705882352941, 0.9958823529411764, 0.9988235294117647, 0.9982352941176471, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8164705882352942, 0.9552941176470588, 0.9876470588235294, 0.9929411764705882, 0.9970588235294118, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 0.9735294117647059, 0.9876470588235294, 0.9952941176470588, 0.9970588235294118, 0.9994117647058823, 0.9994117647058823, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8282352941176471, 0.971764705882353, 0.991764705882353, 0.9923529411764705, 0.9964705882352941, 1.0, 0.9988235294117647, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8117647058823529, 0.9629411764705882, 0.9905882352941177, 0.9923529411764705, 0.9982352941176471, 0.9988235294117647, 1.0, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 0.961764705882353, 0.9911764705882353, 0.9952941176470588, 0.9976470588235294, 0.9976470588235294, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8294117647058824, 0.9647058823529412, 0.991764705882353, 0.9941176470588236, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83, 0.9658823529411765, 0.9911764705882353, 0.9976470588235294, 0.9958823529411764, 0.9994117647058823, 1.0, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8247058823529412, 0.9641176470588235, 0.9905882352941177, 0.9952941176470588, 0.9982352941176471, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8288235294117647, 0.9682352941176471, 0.9911764705882353, 0.9970588235294118, 0.9982352941176471, 0.9970588235294118, 0.9988235294117647, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9676470588235294, 0.9888235294117647, 0.9941176470588236, 0.9964705882352941, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8282352941176471, 0.9664705882352941, 0.9905882352941177, 0.9958823529411764, 0.9970588235294118, 0.9988235294117647, 0.9982352941176471, 1.0, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8152941176470588, 0.9752941176470589, 0.991764705882353, 0.9970588235294118, 0.9982352941176471, 0.9976470588235294, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 0.821764705882353, 0.9676470588235294, 0.9847058823529412, 0.9947058823529412, 0.9970588235294118, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8135294117647058, 0.9594117647058824, 0.9894117647058823, 0.9952941176470588, 0.9976470588235294, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8329411764705882, 0.9652941176470589, 0.99, 0.9958823529411764, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9705882352941176, 0.9911764705882353, 0.9952941176470588, 0.9976470588235294, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.8182352941176471, 0.9658823529411765, 0.9923529411764705, 0.9935294117647059, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9982352941176471, 0.9982352941176471, 1.0, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.821764705882353, 0.9623529411764706, 0.9947058823529412, 0.9952941176470588, 0.9976470588235294, 0.9982352941176471, 0.9994117647058823, 1.0, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8152941176470588, 0.9635294117647059, 0.9876470588235294, 0.9970588235294118, 0.9994117647058823, 0.9976470588235294, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8123529411764706, 0.97, 0.9905882352941177, 0.9929411764705882, 0.9982352941176471, 0.9964705882352941, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.97, 0.991764705882353, 0.9958823529411764, 0.9988235294117647, 0.9982352941176471, 1.0, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 0.8117647058823529, 0.9652941176470589, 0.9923529411764705, 0.9952941176470588, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8288235294117647, 0.9611764705882353, 0.9847058823529412, 0.9964705882352941, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.8270588235294117, 0.9641176470588235, 0.991764705882353, 0.9970588235294118, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.831764705882353, 0.9664705882352941, 0.9923529411764705, 0.9964705882352941, 0.9970588235294118, 0.9994117647058823, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.821764705882353, 0.9623529411764706, 0.9929411764705882, 0.9923529411764705, 0.9964705882352941, 0.9988235294117647, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8382352941176471, 0.9664705882352941, 0.9858823529411764, 0.9976470588235294, 0.9982352941176471, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8211764705882353, 0.9594117647058824, 0.9911764705882353, 0.9958823529411764, 0.9970588235294118, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8294117647058824, 0.9694117647058823, 0.9888235294117647, 0.9952941176470588, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.8088235294117647, 0.9635294117647059, 0.9894117647058823, 0.9982352941176471, 0.9982352941176471, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8252941176470588, 0.9670588235294117, 0.9882352941176471, 0.9970588235294118, 0.9994117647058823, 0.9994117647058823, 0.9988235294117647, 1.0, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 0.821764705882353, 0.9688235294117648, 0.9894117647058823, 0.9964705882352941, 0.9976470588235294, 0.9982352941176471, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8335294117647059, 0.9676470588235294, 0.9923529411764705, 0.9905882352941177, 0.9970588235294118, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.96, 0.9876470588235294, 0.9976470588235294, 0.9982352941176471, 1.0, 0.9994117647058823, 0.9982352941176471, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8276470588235294, 0.9623529411764706, 0.9923529411764705, 0.9970588235294118, 0.9976470588235294, 0.9970588235294118, 0.9994117647058823, 0.9988235294117647, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8205882352941176, 0.9670588235294117, 0.9864705882352941, 0.9947058823529412, 0.9994117647058823, 0.9970588235294118, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8158823529411765, 0.96, 0.9870588235294118, 0.9976470588235294, 0.9988235294117647, 0.9988235294117647, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8164705882352942, 0.9658823529411765, 0.9882352941176471, 0.9947058823529412, 0.9994117647058823, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8152941176470588, 0.9647058823529412, 0.9888235294117647, 0.9976470588235294, 0.9976470588235294, 0.9982352941176471, 0.9982352941176471, 0.9982352941176471, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8211764705882353, 0.9629411764705882, 0.9941176470588236, 0.9964705882352941, 0.9964705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8188235294117647, 0.9611764705882353, 0.9905882352941177, 0.9964705882352941, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8376470588235294, 0.9664705882352941, 0.9947058823529412, 0.9941176470588236, 0.9970588235294118, 0.9976470588235294, 0.9976470588235294, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9711764705882353, 0.9905882352941177, 0.9947058823529412, 0.9964705882352941, 0.9982352941176471, 0.9982352941176471, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.97, 0.9905882352941177, 0.9958823529411764, 0.9982352941176471, 1.0, 0.9988235294117647, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9641176470588235, 0.9835294117647059, 0.9964705882352941, 0.9964705882352941, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 0.8464705882352941, 0.9694117647058823, 0.9894117647058823, 0.9970588235294118, 0.9970588235294118, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 0.9623529411764706, 0.9894117647058823, 0.9952941176470588, 0.9970588235294118, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.9635294117647059, 0.9905882352941177, 0.9952941176470588, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.9564705882352941, 0.9894117647058823, 0.9976470588235294, 0.9988235294117647, 0.9970588235294118, 0.9988235294117647, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8305882352941176, 0.9676470588235294, 0.9894117647058823, 0.9947058823529412, 0.9976470588235294, 0.9988235294117647, 0.9988235294117647, 0.9988235294117647, 1.0, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.8041176470588235, 0.9623529411764706, 0.99, 0.9970588235294118, 0.9982352941176471, 1.0, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8311764705882353, 0.9682352941176471, 0.9911764705882353, 0.9952941176470588, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.831764705882353, 0.9688235294117648, 0.9947058823529412, 0.9976470588235294, 0.9994117647058823, 1.0, 1.0, 1.0, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8305882352941176, 0.9570588235294117, 0.9852941176470589, 0.9947058823529412, 0.9964705882352941, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 0.8170588235294117, 0.9635294117647059, 0.9894117647058823, 0.9947058823529412, 0.9988235294117647, 0.9976470588235294, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8117647058823529, 0.9623529411764706, 0.9911764705882353, 0.9964705882352941, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8211764705882353, 0.9670588235294117, 0.99, 0.9947058823529412, 0.9958823529411764, 0.9982352941176471, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8247058823529412, 0.9611764705882353, 0.9847058823529412, 0.9935294117647059, 0.9988235294117647, 0.9982352941176471, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8117647058823529, 0.9658823529411765, 0.9923529411764705, 0.9952941176470588, 0.9970588235294118, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8382352941176471, 0.9629411764705882, 0.9905882352941177, 0.9970588235294118, 0.9988235294117647, 0.9988235294117647, 0.9994117647058823, 1.0, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8252941176470588, 0.9652941176470589, 0.9923529411764705, 0.9947058823529412, 0.9994117647058823, 0.9970588235294118, 1.0, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8252941176470588, 0.9741176470588235, 0.9894117647058823, 0.9958823529411764, 0.9988235294117647, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8158823529411765, 0.971764705882353, 0.9935294117647059, 0.9970588235294118, 0.9982352941176471, 0.9988235294117647, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8229411764705883, 0.9682352941176471, 0.9935294117647059, 0.9964705882352941, 0.9976470588235294, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8129411764705883, 0.9664705882352941, 0.9905882352941177, 0.9964705882352941, 0.9970588235294118, 0.9988235294117647, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 0.8164705882352942, 0.9664705882352941, 0.9905882352941177, 0.9970588235294118, 0.9988235294117647, 0.9970588235294118, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8294117647058824, 0.9623529411764706, 0.9870588235294118, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 0.9658823529411765, 0.9941176470588236, 0.9941176470588236, 0.9958823529411764, 0.9982352941176471, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8376470588235294, 0.9641176470588235, 0.9888235294117647, 0.9952941176470588, 1.0, 1.0, 0.9988235294117647, 0.9994117647058823, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8288235294117647, 0.9676470588235294, 0.9870588235294118, 0.9970588235294118, 0.9941176470588236, 0.9976470588235294, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8341176470588235, 0.9629411764705882, 0.9905882352941177, 0.9935294117647059, 0.9964705882352941, 0.9988235294117647, 1.0, 0.9988235294117647, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9705882352941176, 0.9923529411764705, 0.9958823529411764, 0.9964705882352941, 0.9988235294117647, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8252941176470588, 0.9664705882352941, 0.9882352941176471, 0.9947058823529412, 0.9988235294117647, 0.9994117647058823, 0.9982352941176471, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8117647058823529, 0.9629411764705882, 0.9864705882352941, 0.9970588235294118, 0.9958823529411764, 0.9982352941176471, 0.9988235294117647, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8258823529411765, 0.9676470588235294, 0.9894117647058823, 0.9935294117647059, 0.9988235294117647, 0.9976470588235294, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8282352941176471, 0.9635294117647059, 0.9888235294117647, 0.9929411764705882, 0.9970588235294118, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8305882352941176, 0.9611764705882353, 0.9923529411764705, 0.9941176470588236, 0.9947058823529412, 0.9982352941176471, 0.9982352941176471, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.9670588235294117, 0.9870588235294118, 0.9982352941176471, 0.9976470588235294, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8329411764705882, 0.9676470588235294, 0.9923529411764705, 0.9958823529411764, 0.9976470588235294, 0.9988235294117647, 0.9994117647058823, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8258823529411765, 0.9588235294117647, 0.9911764705882353, 0.9947058823529412, 0.9958823529411764, 1.0, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 0.8323529411764706, 0.9635294117647059, 0.9905882352941177, 0.9976470588235294, 0.9952941176470588, 0.9976470588235294, 0.9994117647058823, 0.9994117647058823, 0.9976470588235294, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.821764705882353, 0.9611764705882353, 0.9911764705882353, 0.9935294117647059, 0.9952941176470588, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 0.9988235294117647, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9552941176470588, 0.9923529411764705, 0.9958823529411764, 0.9988235294117647, 0.9988235294117647, 1.0, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8229411764705883, 0.9676470588235294, 0.9929411764705882, 0.9958823529411764, 0.9970588235294118, 0.9982352941176471, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 0.9647058823529412, 0.9876470588235294, 0.9964705882352941, 0.9970588235294118, 0.9982352941176471, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.8111764705882353, 0.9652941176470589, 0.9852941176470589, 0.9982352941176471, 0.9970588235294118, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8152941176470588, 0.9594117647058824, 0.9876470588235294, 0.9952941176470588, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8158823529411765, 0.9682352941176471, 0.9870588235294118, 0.9982352941176471, 0.9976470588235294, 0.9982352941176471, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8288235294117647, 0.9629411764705882, 0.9876470588235294, 0.9952941176470588, 0.9976470588235294, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8194117647058824, 0.97, 0.9911764705882353, 0.9947058823529412, 0.9994117647058823, 0.9988235294117647, 0.9976470588235294, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8305882352941176, 0.9729411764705882, 0.991764705882353, 0.9941176470588236, 1.0, 0.9988235294117647, 1.0, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8123529411764706, 0.9582352941176471, 0.9858823529411764, 0.9947058823529412, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8276470588235294, 0.9623529411764706, 0.9888235294117647, 0.9952941176470588, 0.9976470588235294, 0.9976470588235294, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8188235294117647, 0.9758823529411764, 0.9905882352941177, 0.9976470588235294, 0.9970588235294118, 0.9982352941176471, 0.9976470588235294, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8388235294117647, 0.9635294117647059, 0.9911764705882353, 0.9923529411764705, 0.9976470588235294, 0.9988235294117647, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8111764705882353, 0.9658823529411765, 0.9911764705882353, 0.9964705882352941, 0.9988235294117647, 0.9964705882352941, 1.0, 0.9988235294117647, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 0.9564705882352941, 0.9923529411764705, 0.9952941176470588, 0.9994117647058823, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8258823529411765, 0.9647058823529412, 0.9882352941176471, 0.9970588235294118, 0.9952941176470588, 0.9988235294117647, 0.9994117647058823, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8129411764705883, 0.9641176470588235, 0.9870588235294118, 0.9976470588235294, 0.9976470588235294, 0.9988235294117647, 0.9970588235294118, 0.9994117647058823, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8223529411764706, 0.9711764705882353, 0.9894117647058823, 0.9964705882352941, 0.9947058823529412, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8229411764705883, 0.9611764705882353, 0.9858823529411764, 0.991764705882353, 0.9976470588235294, 0.9982352941176471, 0.9988235294117647, 0.9988235294117647, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.831764705882353, 0.9594117647058824, 0.9864705882352941, 0.9952941176470588, 0.9976470588235294, 0.9982352941176471, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8147058823529412, 0.9641176470588235, 0.9888235294117647, 0.9935294117647059, 0.9988235294117647, 1.0, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8164705882352942, 0.9670588235294117, 0.9894117647058823, 0.9941176470588236, 0.9964705882352941, 0.9994117647058823, 0.9994117647058823, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8276470588235294, 0.9735294117647059, 0.9911764705882353, 0.9976470588235294, 0.9952941176470588, 0.9988235294117647, 0.9982352941176471, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8382352941176471, 0.9694117647058823, 0.9888235294117647, 0.9982352941176471, 0.9970588235294118, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8105882352941176, 0.97, 0.991764705882353, 0.9958823529411764, 0.9982352941176471, 0.9994117647058823, 0.9988235294117647, 1.0, 1.0, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8058823529411765, 0.9711764705882353, 0.991764705882353, 0.9964705882352941, 0.9964705882352941, 0.9982352941176471, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8176470588235294, 0.9652941176470589, 0.9888235294117647, 0.9941176470588236, 0.9964705882352941, 0.9988235294117647, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8264705882352941, 0.9647058823529412, 0.9870588235294118, 0.9958823529411764, 0.9964705882352941, 0.9988235294117647, 1.0, 0.9982352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.821764705882353, 0.9647058823529412, 0.9882352941176471, 0.9941176470588236, 0.9976470588235294, 1.0, 1.0, 1.0, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8135294117647058, 0.9641176470588235, 0.9882352941176471, 0.9976470588235294, 0.9982352941176471, 0.9982352941176471, 0.9988235294117647, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8329411764705882, 0.9576470588235294, 0.9876470588235294, 0.9952941176470588, 0.9982352941176471, 0.9976470588235294, 0.9994117647058823, 0.9988235294117647, 0.9994117647058823, 1.0, 1.0, 0.9994117647058823, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8241176470588235, 0.971764705882353, 0.991764705882353, 0.9958823529411764, 0.9964705882352941, 0.9994117647058823, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8229411764705883, 0.9641176470588235, 0.9870588235294118, 0.9958823529411764, 0.9982352941176471, 0.9988235294117647, 0.9988235294117647, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.821764705882353, 0.9652941176470589, 0.9870588235294118, 0.9958823529411764, 0.9976470588235294, 0.9976470588235294, 1.0, 0.9994117647058823, 1.0, 0.9994117647058823, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8129411764705883, 0.9694117647058823, 0.99, 0.9964705882352941, 0.9976470588235294, 0.9988235294117647, 0.9988235294117647, 1.0, 0.9988235294117647, 1.0, 1.0, 1.0, 0.9994117647058823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[0.51, 0.6766666666666666, 0.6733333333333333, 0.7133333333333334, 0.6966666666666667, 0.6966666666666667, 0.7166666666666667, 0.6966666666666667, 0.71, 0.7066666666666667, 0.7133333333333334, 0.7166666666666667, 0.7166666666666667, 0.71, 0.72, 0.7333333333333333, 0.7266666666666667, 0.71, 0.7, 0.7066666666666667, 0.5833333333333334, 0.68, 0.7033333333333334, 0.7, 0.72, 0.7033333333333334, 0.7133333333333334, 0.7166666666666667, 0.7366666666666667, 0.7233333333333334, 0.7166666666666667, 0.72, 0.7166666666666667, 0.7266666666666667, 0.73, 0.7066666666666667, 0.7466666666666667, 0.7166666666666667, 0.7133333333333334, 0.7333333333333333, 0.5566666666666666, 0.6733333333333333, 0.7033333333333334, 0.69, 0.7166666666666667, 0.69, 0.7133333333333334, 0.7, 0.7166666666666667, 0.7033333333333334, 0.72, 0.71, 0.74, 0.69, 0.7266666666666667, 0.72, 0.7333333333333333, 0.71, 0.7366666666666667, 0.72, 0.5933333333333334, 0.6666666666666666, 0.6733333333333333, 0.69, 0.7066666666666667, 0.7, 0.7166666666666667, 0.6866666666666666, 0.71, 0.7033333333333334, 0.72, 0.7033333333333334, 0.72, 0.7166666666666667, 0.72, 0.7066666666666667, 0.7333333333333333, 0.7133333333333334, 0.7166666666666667, 0.7366666666666667, 0.5966666666666667, 0.68, 0.6933333333333334, 0.7, 0.71, 0.7066666666666667, 0.73, 0.72, 0.7166666666666667, 0.7266666666666667, 0.7366666666666667, 0.7033333333333334, 0.72, 0.73, 0.73, 0.7266666666666667, 0.7366666666666667, 0.72, 0.73, 0.7166666666666667, 0.5966666666666667, 0.67, 0.7066666666666667, 0.6933333333333334, 0.7133333333333334, 0.7, 0.6933333333333334, 0.7133333333333334, 0.6933333333333334, 0.7066666666666667, 0.72, 0.7033333333333334, 0.6933333333333334, 0.72, 0.7133333333333334, 0.7266666666666667, 0.7166666666666667, 0.7033333333333334, 0.7233333333333334, 0.74, 0.5066666666666667, 0.6966666666666667, 0.6733333333333333, 0.67, 0.6933333333333334, 0.7166666666666667, 0.7166666666666667, 0.71, 0.7133333333333334, 0.7033333333333334, 0.72, 0.6833333333333333, 0.7266666666666667, 0.73, 0.7266666666666667, 0.7033333333333334, 0.7, 0.7233333333333334, 0.7233333333333334, 0.7433333333333333, 0.5133333333333333, 0.6866666666666666, 0.7066666666666667, 0.7133333333333334, 0.6633333333333333, 0.7166666666666667, 0.7233333333333334, 0.7033333333333334, 0.7066666666666667, 0.73, 0.71, 0.7266666666666667, 0.72, 0.7166666666666667, 0.7333333333333333, 0.72, 0.72, 0.72, 0.7233333333333334, 0.7233333333333334, 0.5366666666666666, 0.6433333333333333, 0.6733333333333333, 0.6833333333333333, 0.7133333333333334, 0.7033333333333334, 0.7133333333333334, 0.6933333333333334, 0.72, 0.71, 0.74, 0.73, 0.69, 0.7166666666666667, 0.7066666666666667, 0.7133333333333334, 0.7166666666666667, 0.7366666666666667, 0.7233333333333334, 0.71, 0.5833333333333334, 0.6666666666666666, 0.6566666666666666, 0.7133333333333334, 0.7266666666666667, 0.7066666666666667, 0.7066666666666667, 0.6966666666666667, 0.7366666666666667, 0.6866666666666666, 0.7266666666666667, 0.7166666666666667, 0.72, 0.71, 0.7066666666666667, 0.7466666666666667, 0.7266666666666667, 0.7233333333333334, 0.7233333333333334, 0.7166666666666667, 0.5966666666666667, 0.6633333333333333, 0.7, 0.69, 0.73, 0.6966666666666667, 0.7133333333333334, 0.7033333333333334, 0.7133333333333334, 0.7266666666666667, 0.7133333333333334, 0.7, 0.7166666666666667, 0.7333333333333333, 0.72, 0.72, 0.7166666666666667, 0.7133333333333334, 0.7366666666666667, 0.7266666666666667, 0.5566666666666666, 0.68, 0.7066666666666667, 0.6933333333333334, 0.71, 0.7266666666666667, 0.71, 0.7166666666666667, 0.7133333333333334, 0.7266666666666667, 0.73, 0.7166666666666667, 0.7133333333333334, 0.7166666666666667, 0.73, 0.7066666666666667, 0.7233333333333334, 0.73, 0.7133333333333334, 0.7433333333333333, 0.5266666666666666, 0.63, 0.69, 0.7066666666666667, 0.7166666666666667, 0.7, 0.7066666666666667, 0.6933333333333334, 0.6966666666666667, 0.7033333333333334, 0.73, 0.7133333333333334, 0.7166666666666667, 0.7133333333333334, 0.73, 0.7133333333333334, 0.7266666666666667, 0.7333333333333333, 0.71, 0.7233333333333334, 0.5533333333333333, 0.6433333333333333, 0.71, 0.6766666666666666, 0.71, 0.71, 0.71, 0.6733333333333333, 0.71, 0.72, 0.7133333333333334, 0.7366666666666667, 0.72, 0.71, 0.7066666666666667, 0.7133333333333334, 0.7133333333333334, 0.72, 0.72, 0.7233333333333334, 0.5633333333333334, 0.6833333333333333, 0.6533333333333333, 0.7133333333333334, 0.6866666666666666, 0.7, 0.71, 0.69, 0.73, 0.69, 0.7366666666666667, 0.72, 0.7166666666666667, 0.7266666666666667, 0.7333333333333333, 0.7033333333333334, 0.72, 0.7066666666666667, 0.7166666666666667, 0.7333333333333333, 0.56, 0.6733333333333333, 0.6933333333333334, 0.72, 0.6766666666666666, 0.7366666666666667, 0.7, 0.74, 0.72, 0.6766666666666666, 0.72, 0.7233333333333334, 0.6966666666666667, 0.74, 0.7333333333333333, 0.7166666666666667, 0.7133333333333334, 0.73, 0.7233333333333334, 0.71, 0.56, 0.6366666666666667, 0.7066666666666667, 0.6766666666666666, 0.6933333333333334, 0.69, 0.7266666666666667, 0.7233333333333334, 0.7166666666666667, 0.7133333333333334, 0.7, 0.7233333333333334, 0.6966666666666667, 0.7133333333333334, 0.7133333333333334, 0.7133333333333334, 0.7133333333333334, 0.7133333333333334, 0.7166666666666667, 0.71, 0.5366666666666666, 0.66, 0.7, 0.73, 0.6966666666666667, 0.6833333333333333, 0.7266666666666667, 0.71, 0.7266666666666667, 0.74, 0.72, 0.7133333333333334, 0.7066666666666667, 0.73, 0.7233333333333334, 0.7266666666666667, 0.72, 0.72, 0.7366666666666667, 0.7266666666666667, 0.56, 0.6433333333333333, 0.6933333333333334, 0.71, 0.72, 0.6866666666666666, 0.7233333333333334, 0.7233333333333334, 0.7366666666666667, 0.7066666666666667, 0.74, 0.7166666666666667, 0.72, 0.7366666666666667, 0.7166666666666667, 0.72, 0.71, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.54, 0.69, 0.67, 0.7266666666666667, 0.71, 0.7066666666666667, 0.6933333333333334, 0.72, 0.7333333333333333, 0.72, 0.7033333333333334, 0.7266666666666667, 0.6966666666666667, 0.7133333333333334, 0.74, 0.6833333333333333, 0.72, 0.7066666666666667, 0.7166666666666667, 0.7266666666666667, 0.5633333333333334, 0.6733333333333333, 0.6866666666666666, 0.6966666666666667, 0.7066666666666667, 0.71, 0.6966666666666667, 0.72, 0.7266666666666667, 0.72, 0.6866666666666666, 0.7066666666666667, 0.71, 0.7166666666666667, 0.7233333333333334, 0.7233333333333334, 0.7266666666666667, 0.7, 0.7266666666666667, 0.72, 0.58, 0.6766666666666666, 0.6866666666666666, 0.6666666666666666, 0.6766666666666666, 0.7066666666666667, 0.7033333333333334, 0.7266666666666667, 0.7166666666666667, 0.7233333333333334, 0.72, 0.74, 0.7233333333333334, 0.7366666666666667, 0.6966666666666667, 0.72, 0.7166666666666667, 0.72, 0.72, 0.7233333333333334, 0.57, 0.67, 0.69, 0.7033333333333334, 0.7133333333333334, 0.7, 0.71, 0.7, 0.72, 0.6933333333333334, 0.7, 0.7266666666666667, 0.7333333333333333, 0.71, 0.7133333333333334, 0.7166666666666667, 0.6966666666666667, 0.7266666666666667, 0.7233333333333334, 0.7333333333333333, 0.57, 0.65, 0.7, 0.7033333333333334, 0.7033333333333334, 0.7166666666666667, 0.73, 0.7266666666666667, 0.7133333333333334, 0.72, 0.73, 0.7166666666666667, 0.7133333333333334, 0.73, 0.7166666666666667, 0.72, 0.7233333333333334, 0.7233333333333334, 0.71, 0.7133333333333334, 0.58, 0.6733333333333333, 0.7166666666666667, 0.7, 0.7, 0.7266666666666667, 0.7133333333333334, 0.7166666666666667, 0.7333333333333333, 0.71, 0.71, 0.72, 0.7133333333333334, 0.73, 0.7133333333333334, 0.7066666666666667, 0.72, 0.72, 0.7266666666666667, 0.72, 0.5833333333333334, 0.6566666666666666, 0.7133333333333334, 0.7, 0.71, 0.7, 0.7433333333333333, 0.6966666666666667, 0.7166666666666667, 0.7233333333333334, 0.7466666666666667, 0.73, 0.72, 0.7466666666666667, 0.72, 0.7166666666666667, 0.7033333333333334, 0.7333333333333333, 0.71, 0.72, 0.5833333333333334, 0.66, 0.68, 0.6766666666666666, 0.7366666666666667, 0.71, 0.7166666666666667, 0.7066666666666667, 0.7033333333333334, 0.7033333333333334, 0.7133333333333334, 0.73, 0.73, 0.7333333333333333, 0.7066666666666667, 0.7233333333333334, 0.7133333333333334, 0.71, 0.7233333333333334, 0.7033333333333334, 0.54, 0.64, 0.7133333333333334, 0.7, 0.7066666666666667, 0.7266666666666667, 0.7, 0.7033333333333334, 0.7333333333333333, 0.7133333333333334, 0.73, 0.7166666666666667, 0.7266666666666667, 0.72, 0.7266666666666667, 0.7266666666666667, 0.73, 0.7266666666666667, 0.7066666666666667, 0.73, 0.5266666666666666, 0.6666666666666666, 0.6633333333333333, 0.7033333333333334, 0.7, 0.69, 0.7166666666666667, 0.71, 0.7, 0.7233333333333334, 0.73, 0.7233333333333334, 0.7066666666666667, 0.7033333333333334, 0.7166666666666667, 0.7033333333333334, 0.7166666666666667, 0.7166666666666667, 0.72, 0.7233333333333334, 0.5366666666666666, 0.63, 0.71, 0.72, 0.7066666666666667, 0.71, 0.72, 0.71, 0.6966666666666667, 0.7233333333333334, 0.72, 0.7266666666666667, 0.7133333333333334, 0.7166666666666667, 0.7366666666666667, 0.7266666666666667, 0.71, 0.7233333333333334, 0.7233333333333334, 0.7166666666666667, 0.5333333333333333, 0.66, 0.6866666666666666, 0.69, 0.7066666666666667, 0.72, 0.7033333333333334, 0.73, 0.7133333333333334, 0.71, 0.7166666666666667, 0.71, 0.73, 0.72, 0.7266666666666667, 0.71, 0.7166666666666667, 0.73, 0.71, 0.73, 0.5333333333333333, 0.6433333333333333, 0.6933333333333334, 0.7133333333333334, 0.6966666666666667, 0.7066666666666667, 0.6966666666666667, 0.72, 0.7066666666666667, 0.7033333333333334, 0.71, 0.71, 0.7066666666666667, 0.71, 0.7233333333333334, 0.7233333333333334, 0.7233333333333334, 0.72, 0.7333333333333333, 0.7233333333333334, 0.5366666666666666, 0.6333333333333333, 0.6766666666666666, 0.7066666666666667, 0.7133333333333334, 0.7066666666666667, 0.6866666666666666, 0.7066666666666667, 0.7033333333333334, 0.7233333333333334, 0.7166666666666667, 0.7266666666666667, 0.7233333333333334, 0.72, 0.73, 0.7333333333333333, 0.7166666666666667, 0.73, 0.71, 0.7133333333333334, 0.5633333333333334, 0.6566666666666666, 0.6833333333333333, 0.6833333333333333, 0.71, 0.7333333333333333, 0.7066666666666667, 0.7, 0.7133333333333334, 0.7133333333333334, 0.7066666666666667, 0.71, 0.7033333333333334, 0.73, 0.71, 0.7133333333333334, 0.72, 0.7333333333333333, 0.7133333333333334, 0.72, 0.5666666666666667, 0.65, 0.6933333333333334, 0.7, 0.7233333333333334, 0.7233333333333334, 0.7266666666666667, 0.7333333333333333, 0.71, 0.73, 0.73, 0.7066666666666667, 0.73, 0.7366666666666667, 0.7233333333333334, 0.73, 0.7266666666666667, 0.7166666666666667, 0.7166666666666667, 0.7133333333333334, 0.5666666666666667, 0.6633333333333333, 0.6966666666666667, 0.67, 0.7166666666666667, 0.7033333333333334, 0.7333333333333333, 0.6766666666666666, 0.7, 0.72, 0.72, 0.69, 0.7133333333333334, 0.72, 0.7266666666666667, 0.7333333333333333, 0.72, 0.7433333333333333, 0.7366666666666667, 0.7166666666666667, 0.54, 0.6433333333333333, 0.68, 0.6833333333333333, 0.6933333333333334, 0.7133333333333334, 0.73, 0.7266666666666667, 0.7266666666666667, 0.69, 0.7066666666666667, 0.72, 0.72, 0.7, 0.73, 0.72, 0.7233333333333334, 0.7233333333333334, 0.7166666666666667, 0.75, 0.5433333333333333, 0.6533333333333333, 0.6733333333333333, 0.7, 0.73, 0.6966666666666667, 0.7066666666666667, 0.7066666666666667, 0.72, 0.74, 0.7166666666666667, 0.72, 0.72, 0.74, 0.72, 0.73, 0.7233333333333334, 0.73, 0.7266666666666667, 0.7066666666666667, 0.5866666666666667, 0.6533333333333333, 0.68, 0.7033333333333334, 0.6733333333333333, 0.7033333333333334, 0.73, 0.7033333333333334, 0.7033333333333334, 0.72, 0.7366666666666667, 0.7433333333333333, 0.7066666666666667, 0.7133333333333334, 0.7166666666666667, 0.7333333333333333, 0.7133333333333334, 0.7366666666666667, 0.7233333333333334, 0.7166666666666667, 0.55, 0.66, 0.6766666666666666, 0.7133333333333334, 0.7233333333333334, 0.7133333333333334, 0.71, 0.72, 0.71, 0.7266666666666667, 0.7333333333333333, 0.71, 0.72, 0.72, 0.7266666666666667, 0.7233333333333334, 0.73, 0.7133333333333334, 0.72, 0.7066666666666667, 0.5966666666666667, 0.6333333333333333, 0.6566666666666666, 0.69, 0.7033333333333334, 0.71, 0.7366666666666667, 0.6933333333333334, 0.7433333333333333, 0.7166666666666667, 0.6933333333333334, 0.72, 0.7366666666666667, 0.7133333333333334, 0.71, 0.71, 0.7266666666666667, 0.72, 0.7333333333333333, 0.7066666666666667, 0.52, 0.6233333333333333, 0.6866666666666666, 0.6833333333333333, 0.71, 0.71, 0.7133333333333334, 0.6933333333333334, 0.71, 0.7166666666666667, 0.72, 0.7, 0.7133333333333334, 0.7233333333333334, 0.7133333333333334, 0.71, 0.7133333333333334, 0.7333333333333333, 0.7166666666666667, 0.7, 0.5566666666666666, 0.66, 0.6666666666666666, 0.7166666666666667, 0.7133333333333334, 0.71, 0.7033333333333334, 0.7166666666666667, 0.7166666666666667, 0.6933333333333334, 0.71, 0.7233333333333334, 0.7066666666666667, 0.7066666666666667, 0.72, 0.7266666666666667, 0.72, 0.7266666666666667, 0.72, 0.72, 0.5633333333333334, 0.6633333333333333, 0.6933333333333334, 0.7066666666666667, 0.71, 0.71, 0.7233333333333334, 0.6966666666666667, 0.6966666666666667, 0.71, 0.7233333333333334, 0.7433333333333333, 0.7266666666666667, 0.7266666666666667, 0.7133333333333334, 0.7233333333333334, 0.7066666666666667, 0.7066666666666667, 0.71, 0.7266666666666667, 0.54, 0.6566666666666666, 0.6933333333333334, 0.6966666666666667, 0.67, 0.6933333333333334, 0.7133333333333334, 0.7, 0.7, 0.71, 0.7233333333333334, 0.7133333333333334, 0.7, 0.72, 0.7166666666666667, 0.7233333333333334, 0.7066666666666667, 0.7133333333333334, 0.7033333333333334, 0.7266666666666667, 0.54, 0.6633333333333333, 0.6966666666666667, 0.6933333333333334, 0.68, 0.7033333333333334, 0.7133333333333334, 0.7266666666666667, 0.7066666666666667, 0.7233333333333334, 0.71, 0.7066666666666667, 0.72, 0.7233333333333334, 0.7066666666666667, 0.7133333333333334, 0.7066666666666667, 0.72, 0.7133333333333334, 0.72, 0.5066666666666667, 0.67, 0.7, 0.7133333333333334, 0.7233333333333334, 0.6866666666666666, 0.6733333333333333, 0.6966666666666667, 0.7333333333333333, 0.7166666666666667, 0.73, 0.7133333333333334, 0.7233333333333334, 0.7266666666666667, 0.7266666666666667, 0.6966666666666667, 0.73, 0.7266666666666667, 0.7, 0.72, 0.5633333333333334, 0.63, 0.7066666666666667, 0.6866666666666666, 0.7133333333333334, 0.7266666666666667, 0.7233333333333334, 0.7333333333333333, 0.7266666666666667, 0.7, 0.71, 0.7166666666666667, 0.7166666666666667, 0.71, 0.71, 0.72, 0.7133333333333334, 0.7366666666666667, 0.7066666666666667, 0.73, 0.5933333333333334, 0.6166666666666667, 0.6833333333333333, 0.6966666666666667, 0.73, 0.69, 0.7233333333333334, 0.6966666666666667, 0.7066666666666667, 0.74, 0.7033333333333334, 0.72, 0.7266666666666667, 0.7266666666666667, 0.7, 0.6933333333333334, 0.72, 0.7166666666666667, 0.73, 0.7133333333333334, 0.5433333333333333, 0.6733333333333333, 0.6733333333333333, 0.7066666666666667, 0.71, 0.72, 0.7066666666666667, 0.71, 0.72, 0.6933333333333334, 0.7166666666666667, 0.7133333333333334, 0.7133333333333334, 0.7166666666666667, 0.7166666666666667, 0.72, 0.71, 0.7066666666666667, 0.7133333333333334, 0.72, 0.5566666666666666, 0.6366666666666667, 0.6766666666666666, 0.6933333333333334, 0.7133333333333334, 0.7, 0.7133333333333334, 0.73, 0.7266666666666667, 0.7233333333333334, 0.72, 0.7333333333333333, 0.7366666666666667, 0.7066666666666667, 0.7266666666666667, 0.7133333333333334, 0.7266666666666667, 0.7133333333333334, 0.7233333333333334, 0.7333333333333333, 0.5633333333333334, 0.6833333333333333, 0.7033333333333334, 0.6833333333333333, 0.7233333333333334, 0.72, 0.68, 0.7266666666666667, 0.71, 0.71, 0.73, 0.7233333333333334, 0.7266666666666667, 0.7166666666666667, 0.7166666666666667, 0.7133333333333334, 0.7233333333333334, 0.7366666666666667, 0.74, 0.72, 0.5533333333333333, 0.6366666666666667, 0.6833333333333333, 0.7233333333333334, 0.7166666666666667, 0.71, 0.69, 0.7266666666666667, 0.7266666666666667, 0.7133333333333334, 0.7366666666666667, 0.73, 0.7233333333333334, 0.72, 0.73, 0.7266666666666667, 0.7266666666666667, 0.71, 0.71, 0.7133333333333334, 0.5233333333333333, 0.6566666666666666, 0.6966666666666667, 0.6966666666666667, 0.6833333333333333, 0.6966666666666667, 0.71, 0.7033333333333334, 0.7066666666666667, 0.72, 0.6966666666666667, 0.7033333333333334, 0.7266666666666667, 0.7233333333333334, 0.71, 0.7233333333333334, 0.7, 0.71, 0.7166666666666667, 0.7133333333333334, 0.5333333333333333, 0.6733333333333333, 0.6866666666666666, 0.71, 0.72, 0.72, 0.7233333333333334, 0.7266666666666667, 0.7233333333333334, 0.7166666666666667, 0.7266666666666667, 0.73, 0.7266666666666667, 0.7, 0.71, 0.7266666666666667, 0.72, 0.7166666666666667, 0.7266666666666667, 0.7433333333333333, 0.59, 0.6466666666666666, 0.6933333333333334, 0.7033333333333334, 0.7266666666666667, 0.7266666666666667, 0.72, 0.7, 0.7333333333333333, 0.6933333333333334, 0.7166666666666667, 0.72, 0.7166666666666667, 0.7233333333333334, 0.7366666666666667, 0.72, 0.74, 0.71, 0.7166666666666667, 0.7333333333333333, 0.5733333333333334, 0.6833333333333333, 0.71, 0.6966666666666667, 0.71, 0.7, 0.72, 0.7166666666666667, 0.7066666666666667, 0.72, 0.71, 0.71, 0.7266666666666667, 0.73, 0.7333333333333333, 0.7133333333333334, 0.6933333333333334, 0.7266666666666667, 0.72, 0.69, 0.5733333333333334, 0.66, 0.6866666666666666, 0.7033333333333334, 0.71, 0.7066666666666667, 0.6766666666666666, 0.6933333333333334, 0.7033333333333334, 0.7033333333333334, 0.7333333333333333, 0.74, 0.7233333333333334, 0.73, 0.73, 0.7033333333333334, 0.72, 0.7333333333333333, 0.7166666666666667, 0.7266666666666667, 0.5166666666666667, 0.6533333333333333, 0.66, 0.6866666666666666, 0.69, 0.7066666666666667, 0.71, 0.7066666666666667, 0.7233333333333334, 0.73, 0.7366666666666667, 0.71, 0.7266666666666667, 0.7066666666666667, 0.7066666666666667, 0.6866666666666666, 0.7266666666666667, 0.7433333333333333, 0.72, 0.7066666666666667, 0.5166666666666667, 0.69, 0.6633333333333333, 0.68, 0.7133333333333334, 0.7066666666666667, 0.7266666666666667, 0.7133333333333334, 0.7066666666666667, 0.7166666666666667, 0.7166666666666667, 0.7, 0.7166666666666667, 0.7366666666666667, 0.72, 0.73, 0.73, 0.7333333333333333, 0.72, 0.73, 0.5733333333333334, 0.6333333333333333, 0.6733333333333333, 0.7166666666666667, 0.69, 0.7033333333333334, 0.7133333333333334, 0.7233333333333334, 0.73, 0.7, 0.72, 0.7066666666666667, 0.7133333333333334, 0.73, 0.7266666666666667, 0.7233333333333334, 0.7066666666666667, 0.7166666666666667, 0.71, 0.7166666666666667, 0.56, 0.66, 0.6866666666666666, 0.7133333333333334, 0.6933333333333334, 0.7166666666666667, 0.6933333333333334, 0.71, 0.7066666666666667, 0.7233333333333334, 0.7033333333333334, 0.7333333333333333, 0.7266666666666667, 0.7266666666666667, 0.7066666666666667, 0.7266666666666667, 0.7133333333333334, 0.7333333333333333, 0.7133333333333334, 0.7233333333333334, 0.55, 0.6633333333333333, 0.68, 0.71, 0.7033333333333334, 0.7066666666666667, 0.7266666666666667, 0.71, 0.72, 0.6933333333333334, 0.6966666666666667, 0.72, 0.7133333333333334, 0.7166666666666667, 0.71, 0.7233333333333334, 0.74, 0.71, 0.71, 0.7266666666666667, 0.5133333333333333, 0.6366666666666667, 0.71, 0.7033333333333334, 0.68, 0.7, 0.7033333333333334, 0.72, 0.73, 0.7, 0.71, 0.73, 0.7266666666666667, 0.7333333333333333, 0.7166666666666667, 0.73, 0.72, 0.72, 0.7233333333333334, 0.7266666666666667, 0.5866666666666667, 0.6466666666666666, 0.6733333333333333, 0.7, 0.7133333333333334, 0.7166666666666667, 0.7133333333333334, 0.7166666666666667, 0.6933333333333334, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.7166666666666667, 0.73, 0.7166666666666667, 0.7166666666666667, 0.7266666666666667, 0.71, 0.7066666666666667, 0.72, 0.5533333333333333, 0.66, 0.6966666666666667, 0.7133333333333334, 0.72, 0.6933333333333334, 0.7133333333333334, 0.6966666666666667, 0.7066666666666667, 0.7166666666666667, 0.7233333333333334, 0.7333333333333333, 0.72, 0.7366666666666667, 0.74, 0.7166666666666667, 0.7266666666666667, 0.7066666666666667, 0.73, 0.7433333333333333, 0.5566666666666666, 0.6866666666666666, 0.6966666666666667, 0.7033333333333334, 0.71, 0.67, 0.7166666666666667, 0.71, 0.7266666666666667, 0.7066666666666667, 0.72, 0.7133333333333334, 0.7266666666666667, 0.7166666666666667, 0.7066666666666667, 0.7333333333333333, 0.7133333333333334, 0.7166666666666667, 0.7366666666666667, 0.72, 0.57, 0.67, 0.67, 0.71, 0.6666666666666666, 0.6933333333333334, 0.6833333333333333, 0.7, 0.71, 0.7133333333333334, 0.7033333333333334, 0.7266666666666667, 0.7166666666666667, 0.7133333333333334, 0.71, 0.7233333333333334, 0.7166666666666667, 0.7266666666666667, 0.7233333333333334, 0.72, 0.5766666666666667, 0.6766666666666666, 0.68, 0.6766666666666666, 0.6866666666666666, 0.7133333333333334, 0.7233333333333334, 0.7, 0.72, 0.72, 0.75, 0.7066666666666667, 0.7266666666666667, 0.73, 0.7333333333333333, 0.7266666666666667, 0.73, 0.71, 0.7, 0.7233333333333334, 0.58, 0.6733333333333333, 0.68, 0.71, 0.7066666666666667, 0.7233333333333334, 0.6966666666666667, 0.7033333333333334, 0.7333333333333333, 0.7166666666666667, 0.7166666666666667, 0.7266666666666667, 0.7166666666666667, 0.71, 0.73, 0.7033333333333334, 0.7133333333333334, 0.7133333333333334, 0.7233333333333334, 0.7133333333333334, 0.5866666666666667, 0.6666666666666666, 0.71, 0.7266666666666667, 0.7066666666666667, 0.71, 0.71, 0.71, 0.7133333333333334, 0.7133333333333334, 0.72, 0.7033333333333334, 0.73, 0.74, 0.7333333333333333, 0.7333333333333333, 0.72, 0.7233333333333334, 0.7333333333333333, 0.72, 0.5266666666666666, 0.6366666666666667, 0.68, 0.68, 0.73, 0.71, 0.7133333333333334, 0.7066666666666667, 0.7033333333333334, 0.7466666666666667, 0.6966666666666667, 0.7133333333333334, 0.7233333333333334, 0.7133333333333334, 0.7233333333333334, 0.7033333333333334, 0.72, 0.7, 0.7166666666666667, 0.7333333333333333, 0.56, 0.6766666666666666, 0.72, 0.7233333333333334, 0.6966666666666667, 0.7066666666666667, 0.7, 0.7133333333333334, 0.7166666666666667, 0.7033333333333334, 0.7033333333333334, 0.7066666666666667, 0.72, 0.73, 0.7433333333333333, 0.7166666666666667, 0.7233333333333334, 0.7166666666666667, 0.72, 0.7233333333333334, 0.5966666666666667, 0.6633333333333333, 0.6766666666666666, 0.6833333333333333, 0.7066666666666667, 0.72, 0.72, 0.73, 0.7333333333333333, 0.7133333333333334, 0.73, 0.7266666666666667, 0.71, 0.7333333333333333, 0.7133333333333334, 0.71, 0.7133333333333334, 0.71, 0.73, 0.7233333333333334, 0.57, 0.6866666666666666, 0.6733333333333333, 0.6966666666666667, 0.68, 0.73, 0.7233333333333334, 0.69, 0.7133333333333334, 0.71, 0.7133333333333334, 0.72, 0.73, 0.7333333333333333, 0.73, 0.7033333333333334, 0.7366666666666667, 0.7233333333333334, 0.71, 0.74, 0.5566666666666666, 0.6633333333333333, 0.65, 0.6733333333333333, 0.7033333333333334, 0.72, 0.7133333333333334, 0.72, 0.7233333333333334, 0.73, 0.71, 0.72, 0.7133333333333334, 0.7266666666666667, 0.7, 0.7166666666666667, 0.7133333333333334, 0.72, 0.72, 0.7233333333333334, 0.5333333333333333, 0.62, 0.66, 0.68, 0.6933333333333334, 0.72, 0.7133333333333334, 0.72, 0.72, 0.7, 0.73, 0.7233333333333334, 0.7166666666666667, 0.71, 0.7133333333333334, 0.7366666666666667, 0.7266666666666667, 0.7133333333333334, 0.7133333333333334, 0.7566666666666667, 0.54, 0.6566666666666666, 0.7133333333333334, 0.6866666666666666, 0.6966666666666667, 0.6966666666666667, 0.7, 0.7166666666666667, 0.7233333333333334, 0.7, 0.6833333333333333, 0.7266666666666667, 0.7, 0.71, 0.7066666666666667, 0.7333333333333333, 0.7033333333333334, 0.7266666666666667, 0.7233333333333334, 0.7266666666666667, 0.5033333333333333, 0.65, 0.6933333333333334, 0.7133333333333334, 0.67, 0.6833333333333333, 0.7133333333333334, 0.7166666666666667, 0.7233333333333334, 0.6966666666666667, 0.7033333333333334, 0.7366666666666667, 0.7166666666666667, 0.72, 0.73, 0.7066666666666667, 0.72, 0.7166666666666667, 0.7166666666666667, 0.7133333333333334, 0.5333333333333333, 0.7166666666666667, 0.7233333333333334, 0.7166666666666667, 0.7, 0.6866666666666666, 0.7266666666666667, 0.72, 0.7166666666666667, 0.7, 0.7333333333333333, 0.7133333333333334, 0.7333333333333333, 0.72, 0.7266666666666667, 0.7333333333333333, 0.7033333333333334, 0.7166666666666667, 0.73, 0.7266666666666667, 0.5233333333333333, 0.6733333333333333, 0.7033333333333334, 0.7, 0.6966666666666667, 0.7, 0.7066666666666667, 0.6966666666666667, 0.7166666666666667, 0.6833333333333333, 0.7166666666666667, 0.73, 0.7066666666666667, 0.72, 0.7166666666666667, 0.7266666666666667, 0.7333333333333333, 0.7033333333333334, 0.7366666666666667, 0.7166666666666667, 0.59, 0.6333333333333333, 0.7033333333333334, 0.7066666666666667, 0.7033333333333334, 0.6866666666666666, 0.69, 0.7133333333333334, 0.74, 0.7233333333333334, 0.7233333333333334, 0.7, 0.71, 0.7433333333333333, 0.74, 0.7333333333333333, 0.7166666666666667, 0.7466666666666667, 0.7133333333333334, 0.7333333333333333, 0.5366666666666666, 0.6633333333333333, 0.6666666666666666, 0.6966666666666667, 0.7, 0.6966666666666667, 0.71, 0.69, 0.7233333333333334, 0.7233333333333334, 0.74, 0.72, 0.74, 0.7266666666666667, 0.7366666666666667, 0.7233333333333334, 0.7033333333333334, 0.7, 0.7233333333333334, 0.7266666666666667, 0.5833333333333334, 0.6433333333333333, 0.71, 0.71, 0.7033333333333334, 0.7366666666666667, 0.72, 0.7266666666666667, 0.7, 0.72, 0.73, 0.71, 0.7233333333333334, 0.71, 0.7266666666666667, 0.7133333333333334, 0.6966666666666667, 0.7066666666666667, 0.73, 0.73, 0.5633333333333334, 0.68, 0.7066666666666667, 0.6966666666666667, 0.71, 0.7266666666666667, 0.7066666666666667, 0.7133333333333334, 0.7233333333333334, 0.7133333333333334, 0.7366666666666667, 0.7233333333333334, 0.73, 0.7033333333333334, 0.7266666666666667, 0.7233333333333334, 0.7266666666666667, 0.7233333333333334, 0.7233333333333334, 0.7166666666666667, 0.5566666666666666, 0.6766666666666666, 0.73, 0.6833333333333333, 0.68, 0.7, 0.7366666666666667, 0.7066666666666667, 0.7366666666666667, 0.7166666666666667, 0.7233333333333334, 0.7133333333333334, 0.7233333333333334, 0.72, 0.7166666666666667, 0.7466666666666667, 0.7366666666666667, 0.7266666666666667, 0.7133333333333334, 0.7333333333333333, 0.59, 0.59, 0.7166666666666667, 0.6866666666666666, 0.7133333333333334, 0.69, 0.7033333333333334, 0.7166666666666667, 0.72, 0.7266666666666667, 0.71, 0.7433333333333333, 0.7066666666666667, 0.71, 0.7166666666666667, 0.7266666666666667, 0.6933333333333334, 0.7133333333333334, 0.7333333333333333, 0.7166666666666667, 0.5666666666666667, 0.6666666666666666, 0.6666666666666666, 0.6966666666666667, 0.7133333333333334, 0.7066666666666667, 0.7066666666666667, 0.73, 0.7333333333333333, 0.71, 0.6966666666666667, 0.71, 0.72, 0.7166666666666667, 0.73, 0.7233333333333334, 0.72, 0.6966666666666667, 0.7066666666666667, 0.74, 0.5433333333333333, 0.6466666666666666, 0.7033333333333334, 0.6766666666666666, 0.71, 0.6933333333333334, 0.7166666666666667, 0.7233333333333334, 0.7233333333333334, 0.7133333333333334, 0.7133333333333334, 0.7033333333333334, 0.73, 0.7266666666666667, 0.7166666666666667, 0.7266666666666667, 0.7, 0.7233333333333334, 0.73, 0.72, 0.56, 0.65, 0.7066666666666667, 0.68, 0.7033333333333334, 0.69, 0.7133333333333334, 0.7366666666666667, 0.7166666666666667, 0.7066666666666667, 0.7066666666666667, 0.7, 0.7233333333333334, 0.7366666666666667, 0.73, 0.7266666666666667, 0.7366666666666667, 0.7133333333333334, 0.7233333333333334, 0.7266666666666667, 0.5766666666666667, 0.6266666666666667, 0.7166666666666667, 0.71, 0.7, 0.6966666666666667, 0.7166666666666667, 0.7033333333333334, 0.7333333333333333, 0.7066666666666667, 0.7266666666666667, 0.7166666666666667, 0.73, 0.7266666666666667, 0.71, 0.7333333333333333, 0.7233333333333334, 0.73, 0.7166666666666667, 0.73, 0.5666666666666667, 0.6633333333333333, 0.66, 0.6866666666666666, 0.6933333333333334, 0.7033333333333334, 0.7366666666666667, 0.7066666666666667, 0.7033333333333334, 0.7233333333333334, 0.7133333333333334, 0.7, 0.6966666666666667, 0.71, 0.7333333333333333, 0.7233333333333334, 0.7233333333333334, 0.7033333333333334, 0.72, 0.7233333333333334, 0.55, 0.68, 0.7066666666666667, 0.7066666666666667, 0.6866666666666666, 0.72, 0.71, 0.71, 0.7166666666666667, 0.7233333333333334, 0.7333333333333333, 0.74, 0.7166666666666667, 0.7133333333333334, 0.71, 0.7266666666666667, 0.7066666666666667, 0.71, 0.73, 0.71, 0.5866666666666667, 0.6666666666666666, 0.7, 0.73, 0.7033333333333334, 0.69, 0.7133333333333334, 0.7166666666666667, 0.74, 0.7266666666666667, 0.71, 0.72, 0.7266666666666667, 0.7233333333333334, 0.7266666666666667, 0.72, 0.7266666666666667, 0.7266666666666667, 0.7133333333333334, 0.7233333333333334, 0.5833333333333334, 0.6366666666666667, 0.6633333333333333, 0.7133333333333334, 0.71, 0.71, 0.6833333333333333, 0.7066666666666667, 0.7266666666666667, 0.7233333333333334, 0.7166666666666667, 0.73, 0.7266666666666667, 0.7133333333333334, 0.7233333333333334, 0.72, 0.7266666666666667, 0.7066666666666667, 0.7133333333333334, 0.7, 0.54, 0.6733333333333333, 0.6766666666666666, 0.7066666666666667, 0.7266666666666667, 0.7066666666666667, 0.7333333333333333, 0.6833333333333333, 0.7166666666666667, 0.7333333333333333, 0.7066666666666667, 0.7366666666666667, 0.7333333333333333, 0.72, 0.7233333333333334, 0.72, 0.7333333333333333, 0.75, 0.7366666666666667, 0.7266666666666667, 0.57, 0.65, 0.6633333333333333, 0.6833333333333333, 0.6833333333333333, 0.7033333333333334, 0.71, 0.7133333333333334, 0.7233333333333334, 0.7166666666666667, 0.7166666666666667, 0.7266666666666667, 0.7266666666666667, 0.71, 0.7333333333333333, 0.7166666666666667, 0.7166666666666667, 0.7233333333333334, 0.7233333333333334, 0.7166666666666667, 0.5766666666666667, 0.6733333333333333, 0.7033333333333334, 0.6966666666666667, 0.6966666666666667, 0.7066666666666667, 0.7033333333333334, 0.71, 0.7233333333333334, 0.7133333333333334, 0.7233333333333334, 0.6833333333333333, 0.7333333333333333, 0.7066666666666667, 0.7066666666666667, 0.72, 0.7266666666666667, 0.71, 0.7166666666666667, 0.7233333333333334, 0.5533333333333333, 0.6333333333333333, 0.6866666666666666, 0.6866666666666666, 0.69, 0.7, 0.71, 0.7266666666666667, 0.7166666666666667, 0.73, 0.7133333333333334, 0.74, 0.7133333333333334, 0.7066666666666667, 0.7433333333333333, 0.74, 0.73, 0.7233333333333334, 0.7166666666666667, 0.72, 0.5533333333333333, 0.6733333333333333, 0.69, 0.71, 0.7, 0.72, 0.7133333333333334, 0.72, 0.7333333333333333, 0.7066666666666667, 0.74, 0.71, 0.7166666666666667, 0.7133333333333334, 0.7133333333333334, 0.7033333333333334, 0.7233333333333334, 0.7166666666666667, 0.7166666666666667, 0.7233333333333334, 0.5766666666666667, 0.65, 0.6533333333333333, 0.7333333333333333, 0.7, 0.72, 0.7066666666666667, 0.72, 0.73, 0.7266666666666667, 0.69, 0.7133333333333334, 0.73, 0.72, 0.7333333333333333, 0.7266666666666667, 0.7333333333333333, 0.7166666666666667, 0.7233333333333334, 0.73, 0.5066666666666667, 0.6766666666666666, 0.7433333333333333, 0.71, 0.71, 0.7133333333333334, 0.7066666666666667, 0.7066666666666667, 0.71, 0.7166666666666667, 0.73, 0.72, 0.73, 0.7166666666666667, 0.73, 0.7, 0.71, 0.7266666666666667, 0.72, 0.7133333333333334, 0.5166666666666667, 0.6566666666666666, 0.67, 0.68, 0.6933333333333334, 0.7166666666666667, 0.7133333333333334, 0.7033333333333334, 0.7266666666666667, 0.72, 0.71, 0.7266666666666667, 0.71, 0.7266666666666667, 0.7233333333333334, 0.7166666666666667, 0.72, 0.73, 0.72, 0.71, 0.56, 0.67, 0.6833333333333333, 0.7133333333333334, 0.7033333333333334, 0.7333333333333333, 0.73, 0.7066666666666667, 0.7, 0.7266666666666667, 0.7133333333333334, 0.7066666666666667, 0.7233333333333334, 0.7233333333333334, 0.7033333333333334, 0.7133333333333334, 0.7133333333333334, 0.7233333333333334, 0.72, 0.7166666666666667, 0.5333333333333333, 0.6566666666666666, 0.7066666666666667, 0.7266666666666667, 0.7166666666666667, 0.69, 0.6933333333333334, 0.6966666666666667, 0.6966666666666667, 0.7066666666666667, 0.7166666666666667, 0.7333333333333333, 0.7166666666666667, 0.7033333333333334, 0.71, 0.7233333333333334, 0.7333333333333333, 0.7166666666666667, 0.7333333333333333, 0.7133333333333334, 0.55, 0.67, 0.69, 0.6766666666666666, 0.7266666666666667, 0.7, 0.7166666666666667, 0.7433333333333333, 0.72, 0.72, 0.7133333333333334, 0.71, 0.72, 0.7166666666666667, 0.7233333333333334, 0.7233333333333334, 0.72, 0.7233333333333334, 0.71, 0.7266666666666667, 0.5466666666666666, 0.6633333333333333, 0.6666666666666666, 0.7266666666666667, 0.6866666666666666, 0.7066666666666667, 0.7333333333333333, 0.7066666666666667, 0.73, 0.7333333333333333, 0.7133333333333334, 0.7, 0.7066666666666667, 0.74, 0.7266666666666667, 0.7266666666666667, 0.71, 0.7166666666666667, 0.7133333333333334, 0.7366666666666667, 0.58, 0.6833333333333333, 0.7033333333333334, 0.72, 0.69, 0.7166666666666667, 0.7033333333333334, 0.7133333333333334, 0.7133333333333334, 0.7166666666666667, 0.7166666666666667, 0.7233333333333334, 0.7366666666666667, 0.7066666666666667, 0.7166666666666667, 0.72, 0.7333333333333333, 0.7133333333333334, 0.71, 0.7233333333333334, 0.57, 0.69, 0.6733333333333333, 0.7066666666666667, 0.7166666666666667, 0.69, 0.7, 0.73, 0.7233333333333334, 0.73, 0.7166666666666667, 0.7066666666666667, 0.7333333333333333, 0.7166666666666667, 0.7066666666666667, 0.7166666666666667, 0.72, 0.72, 0.7333333333333333, 0.7133333333333334, 0.5266666666666666, 0.6433333333333333, 0.7133333333333334, 0.6966666666666667, 0.6833333333333333, 0.6966666666666667, 0.71, 0.6966666666666667, 0.71, 0.7233333333333334, 0.7, 0.72, 0.7166666666666667, 0.7166666666666667, 0.71, 0.7466666666666667, 0.73, 0.7333333333333333, 0.71, 0.71, 0.55, 0.66, 0.6666666666666666, 0.7266666666666667, 0.6933333333333334, 0.7066666666666667, 0.6866666666666666, 0.7233333333333334, 0.71, 0.7333333333333333, 0.7166666666666667, 0.7266666666666667, 0.71, 0.71, 0.7166666666666667, 0.7066666666666667, 0.72, 0.71, 0.7333333333333333, 0.7233333333333334, 0.56, 0.6266666666666667, 0.68, 0.6833333333333333, 0.73, 0.71, 0.71, 0.7133333333333334, 0.69, 0.71, 0.7166666666666667, 0.7333333333333333, 0.74, 0.7233333333333334, 0.73, 0.7166666666666667, 0.73, 0.73, 0.72, 0.72, 0.5733333333333334, 0.6966666666666667, 0.6733333333333333, 0.69, 0.6966666666666667, 0.7166666666666667, 0.7, 0.71, 0.7033333333333334, 0.7066666666666667, 0.7133333333333334, 0.71, 0.7166666666666667, 0.7133333333333334, 0.73, 0.7166666666666667, 0.75, 0.7033333333333334, 0.7166666666666667, 0.72, 0.5766666666666667, 0.6133333333333333, 0.6833333333333333, 0.6833333333333333, 0.7266666666666667, 0.7133333333333334, 0.71, 0.7133333333333334, 0.71, 0.7, 0.71, 0.7166666666666667, 0.72, 0.71, 0.7233333333333334, 0.7333333333333333, 0.7133333333333334, 0.72, 0.73, 0.72, 0.5166666666666667, 0.64, 0.6933333333333334, 0.6833333333333333, 0.6833333333333333, 0.7266666666666667, 0.6966666666666667, 0.6866666666666666, 0.7033333333333334, 0.7066666666666667, 0.71, 0.7233333333333334, 0.7233333333333334, 0.7366666666666667, 0.7266666666666667, 0.7233333333333334, 0.7266666666666667, 0.7133333333333334, 0.7333333333333333, 0.71, 0.5566666666666666, 0.65, 0.73, 0.6866666666666666, 0.69, 0.73, 0.71, 0.7366666666666667, 0.73, 0.7466666666666667, 0.72, 0.7233333333333334, 0.7166666666666667, 0.7033333333333334, 0.6966666666666667, 0.71, 0.7233333333333334, 0.73, 0.71, 0.7233333333333334, 0.55, 0.67, 0.7033333333333334, 0.6833333333333333, 0.7166666666666667, 0.7, 0.72, 0.7033333333333334, 0.72, 0.7333333333333333, 0.7166666666666667, 0.7133333333333334, 0.72, 0.7166666666666667, 0.7066666666666667, 0.7066666666666667, 0.7133333333333334, 0.72, 0.72, 0.7266666666666667, 0.5666666666666667, 0.6433333333333333, 0.6966666666666667, 0.7233333333333334, 0.7, 0.7033333333333334, 0.7266666666666667, 0.71, 0.7033333333333334, 0.7066666666666667, 0.7233333333333334, 0.7033333333333334, 0.69, 0.7, 0.71, 0.74, 0.7266666666666667, 0.73, 0.7066666666666667, 0.7333333333333333, 0.55, 0.6433333333333333, 0.6666666666666666, 0.7133333333333334, 0.6833333333333333, 0.71, 0.72, 0.7233333333333334, 0.7166666666666667, 0.73, 0.72, 0.7333333333333333, 0.7233333333333334, 0.7133333333333334, 0.7266666666666667, 0.7066666666666667, 0.72, 0.7366666666666667, 0.72, 0.7266666666666667, 0.5533333333333333, 0.65, 0.6966666666666667, 0.71, 0.6966666666666667, 0.7133333333333334, 0.7266666666666667, 0.6833333333333333, 0.7133333333333334, 0.7, 0.7, 0.6933333333333334, 0.7033333333333334, 0.72, 0.7233333333333334, 0.73, 0.71, 0.73, 0.7066666666666667, 0.71]\n"
          ]
        }
      ],
      "source": [
        "print(train_accuracy)\n",
        "print(test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgf17ev7yV0G",
        "outputId": "cae28951-8880-49fc-c417-177eee1153a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7566666666666667\n"
          ]
        }
      ],
      "source": [
        "print(max(test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2FlBReCpF_Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vY3fxxfs_GD",
        "outputId": "d4d013b5-1f02-4af3-b963-9d562c785279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for depth : 10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.94       426\n",
            "           1       0.94      0.98      0.96       426\n",
            "           2       0.97      0.91      0.94       431\n",
            "           3       0.95      0.94      0.95       417\n",
            "\n",
            "    accuracy                           0.95      1700\n",
            "   macro avg       0.95      0.95      0.95      1700\n",
            "weighted avg       0.95      0.95      0.95      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.68      0.70        74\n",
            "           1       0.72      0.77      0.75        74\n",
            "           2       0.63      0.59      0.61        69\n",
            "           3       0.70      0.73      0.72        83\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.69      0.69      0.69       300\n",
            "weighted avg       0.70      0.70      0.70       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 3 1 0 1 0 3 3 3 1 3 0 0 3 1 2 2 0 0 2 1 3 3 2 3 2 1 3 1 2 0 0 0 1\n",
            " 3 3 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 2 2 2 3 1 2 1 1 3 1 2 1 2 0 1 1 3 2 2 1\n",
            " 3 3 0 0 3 1 1 3 0 1 3 3 1 3 2 3 2 3 3 0 3 2 1 3 2 1 1 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 2 2 0 2 0 1 2 2 2 3 2 0 3 3 2 2 2 3 3 1 3 2 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 3 2 0 3 1 3 1 3 0 0 3 2 1 1 1 0 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 1 2 1 0 0 2 1 3 0 1 1 1 3 1 0 0 0 3 1 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 0 3 2 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 2 0 0 0 1 2 3 1 2 2 3 3 2 0 0 1 3 1 1 1 0 3 2 0 2 2 3 1 3 0 0 3 0 2 1\n",
            " 0 0 3 0]\n",
            "for depth : 15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.65      0.66        74\n",
            "           1       0.78      0.73      0.76        74\n",
            "           2       0.58      0.64      0.61        69\n",
            "           3       0.75      0.75      0.75        83\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.69      0.69      0.69       300\n",
            "weighted avg       0.70      0.69      0.69       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 1 3 0 0 3 1 2 2 0 2 2 1 3 3 2 2 2 1 3 1 0 0 0 0 1\n",
            " 3 3 3 2 1 2 0 3 0 0 3 1 0 3 0 2 1 2 2 0 3 1 2 1 1 3 1 2 1 2 0 0 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 3 2 3 2 3 3 0 3 2 1 3 2 1 3 3 0 1 3 1 0 1 0 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 3 2 0 2 0 1 2 2 2 3 2 0 3 2 2 2 2 2 3 0 3 0 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 2 0 2 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 0 2 1 3 0 2 1 3 0 1 1 1 2 1 0 3 0 3 2 2 1 2 2 3 2 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 3 2 3 0 3 2 0 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 2 0 0 0 1 2 0 2 2 2 3 3 2 0 0 1 3 2 1 1 0 3 2 0 2 2 3 1 3 0 3 3 0 2 1\n",
            " 2 0 0 0]\n",
            "for depth : 20\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.66      0.67        74\n",
            "           1       0.75      0.77      0.76        74\n",
            "           2       0.62      0.59      0.61        69\n",
            "           3       0.72      0.75      0.73        83\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.69      0.69      0.69       300\n",
            "weighted avg       0.70      0.70      0.70       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 0 0 0 1 3 1 2 2 0 0 0 1 0 3 2 2 2 1 3 1 0 1 0 0 1\n",
            " 3 3 3 3 0 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 2 1 3 1 2 2 2 0 1 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 3 2 2 2 2 3 2 3 2 1 3 2 1 2 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 0 2 1 3 3 0 1 3 1 2 2 0 2 0 1 2 2 3 3 2 0 3 3 0 2 2 3 3 2 3 0 1 2\n",
            " 3 3 1 3 3 3 3 1 1 2 3 3 2 0 3 1 2 1 3 0 0 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 0 2 1 3 0 3 1 3 0 1 1 1 2 1 0 2 0 3 1 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 3 2 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 2 3 0 0 1 2 3 1 2 2 3 0 2 0 0 1 3 1 1 1 0 3 2 0 2 2 3 1 3 0 0 3 1 2 1\n",
            " 2 0 0 0]\n",
            "for depth : 25\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.68      0.70        74\n",
            "           1       0.75      0.77      0.76        74\n",
            "           2       0.59      0.61      0.60        69\n",
            "           3       0.73      0.73      0.73        83\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.70      0.70      0.70       300\n",
            "weighted avg       0.70      0.70      0.70       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 0 3 0 1 3 1 2 2 0 0 0 1 3 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 3 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 3 2 1 2 0 1 1 3 0 2 1\n",
            " 3 2 3 0 3 1 1 3 0 1 3 3 1 3 2 3 2 0 3 0 3 2 1 3 2 1 2 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 1 1 3 2 0 2 0 1 2 2 2 3 2 0 3 3 0 2 2 3 3 2 3 2 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 3 3 1 2 1 2 0 0 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 0 2 1 3 0 3 1 3 0 1 1 1 2 1 0 2 0 2 2 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 3 0 0 3 3 2 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 2 0 0 0 1 2 1 1 2 2 3 3 2 0 0 1 3 1 1 1 0 3 2 0 2 2 3 1 3 0 0 3 1 2 1\n",
            " 2 0 0 0]\n",
            "for depth : 30\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.65      0.70        74\n",
            "           1       0.74      0.78      0.76        74\n",
            "           2       0.59      0.59      0.59        69\n",
            "           3       0.72      0.77      0.74        83\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.70      0.70      0.70       300\n",
            "weighted avg       0.70      0.70      0.70       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 1 3 0 0 3 1 2 2 0 2 3 1 0 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 2 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 2 2 0 3 1 2 1 1 3 1 2 2 2 0 0 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 2 1 3 2 3 2 2 3 0 3 2 1 3 2 1 3 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 3 2 0 2 1 1 1 2 3 3 2 0 3 3 2 2 2 3 3 2 3 1 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 3 2 2 3 1 2 1 3 0 0 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 0 2 1 3 0 3 1 3 0 1 1 1 2 1 0 3 0 2 1 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 0 3 1 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 3 2 0 0 1 2 3 1 2 2 3 3 2 0 0 1 3 2 1 1 0 3 2 3 2 2 3 1 3 0 3 3 0 2 1\n",
            " 3 0 0 0]\n",
            "for depth : 35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.65      0.67        74\n",
            "           1       0.74      0.76      0.75        74\n",
            "           2       0.56      0.58      0.57        69\n",
            "           3       0.74      0.75      0.74        83\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.68      0.68      0.68       300\n",
            "weighted avg       0.69      0.69      0.69       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 1 3 0 0 3 1 2 2 0 0 2 1 3 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 3 3 2 1 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 2 2 2 2 0 1 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 0 2 3 2 2 3 0 3 2 1 3 2 1 1 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 0 2 1 3 3 0 1 1 1 2 2 0 2 0 1 2 2 2 3 2 0 3 3 2 2 2 2 3 2 2 0 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 3 0 2 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 1 2 1 3 0 3 1 3 0 1 1 1 2 1 0 3 0 0 2 2 1 3 2 3 0 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 0 3 1 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 2 0 0 0 1 2 3 1 2 2 3 3 0 0 0 1 3 2 1 1 0 3 2 3 2 2 3 1 3 0 3 3 0 2 1\n",
            " 3 0 0 0]\n",
            "for depth : 40\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.62      0.66        74\n",
            "           1       0.78      0.76      0.77        74\n",
            "           2       0.57      0.62      0.60        69\n",
            "           3       0.75      0.80      0.77        83\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.70      0.70      0.70       300\n",
            "weighted avg       0.71      0.70      0.70       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 3 3 3 0 1 3 0 0 3 1 2 2 0 2 2 1 0 3 2 2 2 1 3 1 2 0 0 0 1\n",
            " 3 3 3 2 1 2 0 3 3 0 3 1 0 3 0 2 1 3 2 0 2 1 2 1 1 3 2 2 1 2 0 0 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 2 3 2 3 2 1 2 0 3 2 1 3 2 1 2 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 3 2 0 2 0 1 2 2 3 3 2 0 3 3 2 2 2 3 3 0 3 2 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 2 0 2 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 1 2 1 3 0 3 1 3 0 1 1 1 2 1 0 3 0 3 2 2 1 3 2 3 0 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 3 2 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 3 2 0 0 1 2 1 1 2 2 3 3 2 0 0 1 3 2 1 1 0 3 2 2 2 2 3 1 3 0 0 3 0 2 1\n",
            " 3 0 0 0]\n",
            "for depth : 45\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.61      0.64        74\n",
            "           1       0.78      0.77      0.78        74\n",
            "           2       0.56      0.59      0.58        69\n",
            "           3       0.67      0.71      0.69        83\n",
            "\n",
            "    accuracy                           0.67       300\n",
            "   macro avg       0.67      0.67      0.67       300\n",
            "weighted avg       0.68      0.67      0.67       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 3 1 3 0 1 3 1 2 2 0 0 2 1 3 3 2 2 2 1 3 1 0 0 0 0 1\n",
            " 3 3 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 2 2 1 2 0 1 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 2 2 3 2 2 2 0 3 0 3 2 1 3 2 1 3 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 0 2 1 3 3 0 1 3 1 3 2 0 2 1 1 2 2 2 3 2 0 3 2 2 2 2 2 3 2 3 0 1 3\n",
            " 3 0 1 3 3 3 3 1 1 2 2 2 2 0 3 1 2 1 2 0 2 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 0 2 1 3 0 2 1 3 1 1 1 1 2 1 0 3 0 3 1 2 1 3 2 3 0 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 2 1 3 0 2 2 2 3 1 0 0 3 3 3 3 0 3 2 0 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 3 3 0 0 1 2 0 1 2 2 3 3 3 0 0 1 3 2 1 1 0 3 2 2 2 2 3 1 3 0 3 3 1 2 1\n",
            " 3 0 3 0]\n",
            "for depth : 50\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.64      0.66        74\n",
            "           1       0.74      0.78      0.76        74\n",
            "           2       0.65      0.65      0.65        69\n",
            "           3       0.74      0.75      0.74        83\n",
            "\n",
            "    accuracy                           0.71       300\n",
            "   macro avg       0.70      0.70      0.70       300\n",
            "weighted avg       0.71      0.71      0.71       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 3 3 3 0 1 0 0 1 3 1 2 2 0 0 2 1 0 3 2 2 2 1 3 1 0 0 0 0 1\n",
            " 3 3 3 2 1 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 2 3 1 2 3 1 0 0 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 3 2 3 2 3 3 0 3 2 1 3 2 1 2 3 0 1 3 1 0 1 0 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 1 1 1 2 0 2 1 1 2 2 3 3 2 0 3 2 0 2 2 2 3 2 3 0 1 2\n",
            " 3 3 1 3 3 3 3 1 1 2 2 2 2 0 3 1 2 1 2 0 0 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 0 2 1 3 0 3 1 3 0 1 1 1 2 1 0 3 0 2 2 2 1 3 2 3 1 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 3 2 3 0 3 2 0 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 2 2 0 0 1 2 3 1 2 2 3 0 2 0 0 1 3 1 1 1 0 3 2 3 2 2 3 1 3 0 3 3 1 2 1\n",
            " 2 0 0 0]\n",
            "for depth : 55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.65      0.68        74\n",
            "           1       0.77      0.76      0.76        74\n",
            "           2       0.59      0.59      0.59        69\n",
            "           3       0.69      0.75      0.72        83\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.69      0.69      0.69       300\n",
            "weighted avg       0.69      0.69      0.69       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 3 1 3 0 0 3 1 2 2 0 0 2 1 3 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 3 3 3 0 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 2 2 3 2 0 1 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 2 0 2 0 2 3 3 0 3 2 1 3 2 1 2 3 3 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 3 2 0 2 0 1 2 2 2 3 2 0 3 3 2 2 2 2 3 0 3 0 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 3 0 0 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 1 2 1 3 0 2 1 3 0 1 1 1 2 1 0 2 0 3 2 2 1 3 2 3 0 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 0 3 1 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 2 3 0 0 1 2 3 1 2 2 3 3 3 0 0 1 3 1 1 1 0 3 2 0 2 2 3 1 3 0 0 3 0 2 1\n",
            " 2 0 3 0]\n",
            "for depth : 60\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.61      0.66        74\n",
            "           1       0.77      0.80      0.78        74\n",
            "           2       0.59      0.61      0.60        69\n",
            "           3       0.74      0.80      0.77        83\n",
            "\n",
            "    accuracy                           0.71       300\n",
            "   macro avg       0.70      0.70      0.70       300\n",
            "weighted avg       0.71      0.71      0.71       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 3 3 3 3 1 3 0 0 3 1 2 2 0 2 2 1 3 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 3 3 2 1 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 1 2 1 0 0 1 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 2 3 2 3 2 3 3 0 3 2 1 3 2 1 2 3 0 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 0 2 0 1 2 2 3 3 2 0 3 3 2 2 2 2 3 2 2 0 1 2\n",
            " 3 3 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 2 0 2 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 0 2 1 3 0 3 1 3 1 1 1 1 2 1 0 2 0 2 2 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 1 2 3 2 3 1 0 0 3 3 3 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 2 0 0 0 1 2 0 1 2 2 3 3 2 0 0 1 3 1 1 1 0 3 2 0 2 2 3 1 3 0 3 3 0 2 1\n",
            " 0 0 3 0]\n",
            "for depth : 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.59      0.64        74\n",
            "           1       0.77      0.80      0.78        74\n",
            "           2       0.57      0.57      0.57        69\n",
            "           3       0.69      0.75      0.72        83\n",
            "\n",
            "    accuracy                           0.68       300\n",
            "   macro avg       0.68      0.68      0.68       300\n",
            "weighted avg       0.68      0.68      0.68       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 1 3 0 1 3 1 2 2 0 2 2 1 3 3 2 2 2 1 3 1 0 1 0 0 1\n",
            " 3 2 3 2 1 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 2 3 2 2 1 2 0 0 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 3 2 2 2 3 3 0 3 2 1 3 2 1 1 3 0 1 3 1 0 1 0 1 2\n",
            " 3 0 3 0 0 2 1 3 3 0 1 1 1 3 2 0 3 0 1 2 2 2 3 2 0 3 3 2 2 2 3 3 2 3 0 1 2\n",
            " 3 3 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 3 0 2 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 0 2 1 3 0 3 1 3 0 1 1 1 2 1 0 0 0 3 1 2 1 3 2 3 1 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 3 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 3 2 0 2 1 2 3 1 2 2 3 3 3 0 0 1 3 2 1 1 0 3 2 0 2 2 3 1 3 0 3 3 1 2 1\n",
            " 0 0 0 0]\n",
            "for depth : 70\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.61      0.64        74\n",
            "           1       0.73      0.77      0.75        74\n",
            "           2       0.56      0.58      0.57        69\n",
            "           3       0.73      0.75      0.74        83\n",
            "\n",
            "    accuracy                           0.68       300\n",
            "   macro avg       0.68      0.68      0.68       300\n",
            "weighted avg       0.68      0.68      0.68       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 3 1 0 0 1 3 1 2 2 0 2 0 1 3 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 3 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 2 2 0 2 1 2 1 1 3 2 2 2 2 0 1 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 0 2 3 2 3 3 2 3 2 1 3 2 1 1 3 3 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 0 2 1 3 3 0 1 1 1 1 2 0 3 0 1 2 2 3 3 2 0 3 3 2 2 2 2 3 0 3 0 1 3\n",
            " 3 3 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 3 0 0 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 0 2 1 3 0 2 1 3 0 1 1 1 2 1 0 2 0 2 1 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 3 3 3 0 3 2 0 3 0 2 3 1 2 3 2 3 0 0 3\n",
            " 3 3 2 2 0 0 1 2 1 1 2 2 3 0 2 0 0 1 3 2 1 1 0 3 2 0 2 2 3 1 3 0 0 3 1 2 1\n",
            " 0 0 3 0]\n",
            "for depth : 75\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.62      0.67        74\n",
            "           1       0.77      0.76      0.76        74\n",
            "           2       0.56      0.64      0.60        69\n",
            "           3       0.73      0.75      0.74        83\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.69      0.69      0.69       300\n",
            "weighted avg       0.70      0.69      0.69       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 3 1 0 1 0 3 3 0 1 3 0 0 3 1 2 2 0 2 2 1 3 3 2 2 2 1 3 1 0 0 0 0 1\n",
            " 3 3 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 2 2 0 2 1 2 1 1 3 2 2 1 2 0 1 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 3 2 3 2 3 3 2 3 2 1 3 2 1 2 3 3 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 0 2 1 3 3 0 1 3 1 1 2 0 2 1 1 2 2 2 3 2 0 3 2 2 2 2 3 3 2 2 2 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 3 2 0 3 1 2 1 2 0 2 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 1 2 1 3 0 2 1 3 0 1 1 1 2 1 0 0 0 2 2 2 1 3 2 3 0 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 3 3 2 0 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 3 0 0 0 1 2 3 1 2 2 3 3 2 0 0 1 3 2 1 1 0 3 2 3 2 2 3 1 3 0 3 3 0 2 1\n",
            " 2 0 0 0]\n",
            "for depth : 80\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.62      0.65        74\n",
            "           1       0.78      0.77      0.78        74\n",
            "           2       0.54      0.57      0.55        69\n",
            "           3       0.66      0.70      0.68        83\n",
            "\n",
            "    accuracy                           0.67       300\n",
            "   macro avg       0.67      0.66      0.66       300\n",
            "weighted avg       0.67      0.67      0.67       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 3 0 0 0 0 3 1 2 2 0 2 0 1 3 3 2 2 2 1 3 1 2 1 0 0 1\n",
            " 3 3 3 2 1 2 0 3 3 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 3 2 3 2 0 0 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 2 3 2 2 2 0 3 0 3 2 1 3 2 1 2 3 0 1 3 1 0 1 0 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 1 1 2 2 0 2 0 1 2 2 2 3 2 0 3 3 2 2 2 2 3 0 3 0 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 3 3 1 2 1 3 0 2 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 0 0 3 3 1 2 1 3 0 3 1 3 0 1 1 1 2 1 0 3 0 3 2 2 1 3 2 3 2 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 2 3 0 0 1 2 3 1 2 2 3 0 3 0 0 1 3 1 1 1 0 3 2 0 2 2 3 1 3 0 0 3 0 2 1\n",
            " 2 0 3 0]\n",
            "for depth : 85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.62      0.65        74\n",
            "           1       0.74      0.78      0.76        74\n",
            "           2       0.59      0.59      0.59        69\n",
            "           3       0.71      0.72      0.71        83\n",
            "\n",
            "    accuracy                           0.68       300\n",
            "   macro avg       0.68      0.68      0.68       300\n",
            "weighted avg       0.68      0.68      0.68       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 0 3 3 0 1 3 0 0 3 1 2 2 0 0 2 1 0 3 2 2 2 1 3 1 0 0 0 0 1\n",
            " 3 3 3 3 1 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 1 1 3 2 2 1 2 0 1 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 0 2 3 2 2 2 2 3 2 1 3 2 1 1 3 3 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 2 0 1 3 1 1 2 0 3 1 1 1 2 2 3 2 0 3 3 0 2 2 3 3 2 3 0 1 2\n",
            " 3 3 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 3 0 2 3 2 1 1 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 1 2 1 3 0 3 1 3 0 1 1 1 2 1 0 2 0 0 2 2 1 2 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 3 2 3 0 3 2 0 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 2 2 0 0 1 2 3 1 2 2 3 3 0 0 0 1 3 2 1 1 0 3 2 3 2 2 3 1 3 0 3 3 0 2 1\n",
            " 0 0 0 0]\n",
            "for depth : 90\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.68      0.71        74\n",
            "           1       0.77      0.76      0.76        74\n",
            "           2       0.62      0.65      0.64        69\n",
            "           3       0.74      0.78      0.76        83\n",
            "\n",
            "    accuracy                           0.72       300\n",
            "   macro avg       0.72      0.72      0.72       300\n",
            "weighted avg       0.72      0.72      0.72       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 0 1 0 1 3 3 3 3 1 3 0 0 3 1 2 2 0 0 0 1 3 3 2 2 2 1 3 1 0 0 0 0 1\n",
            " 3 3 3 2 1 2 0 3 0 0 3 1 0 3 0 2 1 2 2 0 2 1 2 1 2 3 1 2 2 0 0 0 1 3 2 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 3 2 3 2 3 3 2 3 2 1 3 2 1 3 3 0 2 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 2 2 0 2 0 1 1 2 2 3 2 0 3 3 2 2 2 3 3 2 3 0 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 2 0 2 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 1 2 1 3 0 3 1 3 0 1 1 1 2 1 0 2 0 3 1 2 1 3 2 3 0 1 1 1 0 0 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 3 2 3 1 0 0 3 1 2 3 0 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 1 3 0 0 0 1 2 3 1 2 2 3 3 2 0 0 1 3 2 1 1 0 3 2 0 2 2 3 1 3 0 0 3 0 2 1\n",
            " 3 0 3 0]\n",
            "for depth : 95\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       426\n",
            "           1       1.00      1.00      1.00       426\n",
            "           2       1.00      1.00      1.00       431\n",
            "           3       1.00      1.00      1.00       417\n",
            "\n",
            "    accuracy                           1.00      1700\n",
            "   macro avg       1.00      1.00      1.00      1700\n",
            "weighted avg       1.00      1.00      1.00      1700\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.68      0.70        74\n",
            "           1       0.75      0.74      0.75        74\n",
            "           2       0.58      0.61      0.60        69\n",
            "           3       0.72      0.75      0.73        83\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.70      0.69      0.69       300\n",
            "weighted avg       0.70      0.70      0.70       300\n",
            "\n",
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n",
            "[2 2 2 1 3 1 0 1 3 3 3 0 1 3 0 0 3 1 2 2 0 0 0 1 0 3 2 2 2 1 3 1 0 1 0 0 1\n",
            " 3 3 3 2 0 2 0 3 0 0 3 1 0 3 0 2 1 3 2 0 3 1 2 2 1 3 1 2 2 2 0 0 1 3 0 2 1\n",
            " 3 3 3 0 3 1 1 3 0 1 3 3 1 0 2 3 2 3 2 2 3 2 1 3 2 1 2 3 3 1 3 1 0 1 1 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 1 1 3 2 0 2 0 1 1 2 3 3 2 0 3 3 0 2 2 3 3 2 2 0 1 2\n",
            " 3 0 1 3 3 3 3 1 1 2 3 2 2 0 3 1 2 1 3 0 0 3 2 1 2 1 2 0 3 1 1 2 1 1 0 1 0\n",
            " 1 2 0 3 3 0 2 1 3 0 2 1 3 0 1 1 1 2 1 0 0 0 3 2 2 1 3 2 3 0 1 1 1 0 1 0 3\n",
            " 0 3 2 1 1 3 3 1 3 0 2 2 2 3 1 0 0 3 3 2 3 3 3 2 1 3 0 2 3 1 2 3 0 3 0 0 3\n",
            " 3 3 2 2 0 0 1 2 1 1 2 2 3 3 2 0 0 1 3 2 1 1 0 3 2 0 2 2 3 1 3 0 3 3 0 2 1\n",
            " 2 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "train_accuracy = list()\n",
        "test_accuracy = list()\n",
        "max_depth = list()\n",
        "\n",
        "for depth in range(10,100,5):\n",
        "    imageTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = depth)\n",
        "    imageTree.fit(x_train,y_train)\n",
        "    print(\"for depth :\",depth)\n",
        "    #for train\n",
        "    pred_class_train = imageTree.predict(x_train)\n",
        "    print (classification_report(y_train, pred_class_train))\n",
        "    train_accuracy.append(metrics.accuracy_score(y_train, pred_class_train))\n",
        "    \n",
        "    #for test class\n",
        "    pred_class_test = imageTree.predict(x_test)\n",
        "    print (classification_report(y_test, pred_class_test))\n",
        "    \n",
        "    print(y_test)\n",
        "    print(pred_class_test)\n",
        "    test_accuracy.append(metrics.accuracy_score(y_test, pred_class_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V5AF7J2tCTw",
        "outputId": "e38d7322-2237-4c3a-e4ca-4cf1ef25b052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9452941176470588, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[0.6966666666666667, 0.6933333333333334, 0.6966666666666667, 0.7, 0.7033333333333334, 0.6866666666666666, 0.7033333333333334, 0.6733333333333333, 0.7066666666666667, 0.69, 0.7066666666666667, 0.68, 0.68, 0.6933333333333334, 0.6666666666666666, 0.6833333333333333, 0.72, 0.6966666666666667]\n"
          ]
        }
      ],
      "source": [
        "print(train_accuracy)\n",
        "print(test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBuoDUTataMO"
      },
      "outputs": [],
      "source": [
        "#svm\n",
        "from sklearn import svm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i1ROYwgqFOW"
      },
      "outputs": [],
      "source": [
        "train_begin = time.time()\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(x_train, y_train) \n",
        "train_end = time.time()\n",
        "train_time = train_end - train_begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-Jm5JnuqKZ4",
        "outputId": "074c66dd-77f2-4482-8f37-381dfdda698f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 2 2 1 2 1 0 1 3 3 3 0 0 3 0 1 3 1 2 2 0 0 2 1 1 3 2 2 2 1 2 0 1 1 0 0 1\n",
            " 3 0 3 0 1 3 0 3 0 0 3 1 0 3 3 0 1 2 3 2 2 1 2 1 2 3 0 2 1 0 0 0 1 3 1 0 2\n",
            " 0 3 3 0 3 1 1 3 0 3 3 3 3 2 2 3 0 3 3 2 3 2 1 3 3 0 0 3 2 1 3 1 0 1 2 1 2\n",
            " 3 0 3 0 2 2 1 3 3 0 1 3 1 1 2 3 2 1 2 1 0 3 3 1 1 3 2 1 2 2 2 3 2 3 0 1 2\n",
            " 3 0 1 3 2 3 0 1 1 3 3 3 2 2 3 2 3 1 2 0 0 3 2 1 3 1 0 0 3 1 1 2 0 1 1 1 0\n",
            " 1 2 2 0 3 0 1 1 3 0 3 3 0 0 1 1 1 2 1 0 0 0 1 1 2 1 3 2 3 0 1 1 1 0 2 0 3\n",
            " 0 3 2 1 3 3 3 1 3 0 2 3 2 2 1 0 0 3 3 2 3 0 0 3 1 3 0 2 3 1 2 2 0 3 0 0 3\n",
            " 2 3 2 0 0 0 1 2 2 1 0 2 3 3 2 0 0 1 3 1 1 1 0 2 2 2 2 2 2 1 3 0 0 3 0 2 0\n",
            " 3 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEkYkDTTqMRQ",
        "outputId": "4b6ec048-f392-408d-c851-6d4e2352936f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 2 2 0 0 1 0 1 3 3 3 0 1 3 1 3 3 3 2 2 1 2 2 1 1 3 2 2 2 1 2 0 1 1 3 0 1\n",
            " 3 0 3 2 1 3 0 3 0 0 3 1 0 3 3 0 1 2 2 2 0 0 0 0 0 0 1 2 1 0 2 1 2 3 2 2 2\n",
            " 0 3 0 0 3 1 1 3 2 1 3 2 0 2 2 2 1 3 3 2 3 2 0 2 2 0 0 3 2 0 1 1 0 0 2 0 0\n",
            " 3 1 3 3 2 0 2 0 2 2 1 1 0 1 2 0 2 0 2 2 1 0 3 1 1 3 2 1 2 2 0 3 1 2 1 2 2\n",
            " 0 0 1 0 3 3 0 1 0 2 3 2 2 1 2 1 0 1 0 1 3 1 0 1 3 1 2 0 3 1 1 2 1 0 3 1 2\n",
            " 0 2 2 0 0 0 1 1 0 0 3 0 3 1 1 1 0 2 1 0 0 0 1 0 2 0 3 2 3 1 0 1 0 0 2 0 3\n",
            " 1 3 2 2 1 3 3 1 3 1 2 3 2 2 0 0 0 0 3 2 3 0 3 3 1 3 0 2 3 1 2 0 0 2 0 0 3\n",
            " 3 1 2 0 0 1 2 2 0 2 2 2 0 3 2 0 0 1 3 1 3 0 0 2 2 2 2 2 3 1 3 0 0 3 0 2 0\n",
            " 1 1 3 0]\n",
            "0.63\n",
            "[[44 16  9  5]\n",
            " [19 42  8  5]\n",
            " [11  3 52  3]\n",
            " [14  7 11 51]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.59      0.54        74\n",
            "           1       0.62      0.57      0.59        74\n",
            "           2       0.65      0.75      0.70        69\n",
            "           3       0.80      0.61      0.69        83\n",
            "\n",
            "    accuracy                           0.63       300\n",
            "   macro avg       0.64      0.63      0.63       300\n",
            "weighted avg       0.65      0.63      0.63       300\n",
            "\n",
            "744.5380992889404\n",
            "0.028526782989501953\n"
          ]
        }
      ],
      "source": [
        "test_begin = time.time()\n",
        "yhat = clf.predict(x_test)\n",
        "test_end = time.time()\n",
        "test_time = test_end - test_begin\n",
        "print(yhat)\n",
        "\n",
        "print(metrics.accuracy_score(y_test, yhat))\n",
        "print(confusion_matrix(y_test, yhat))\n",
        "print(classification_report(y_test, yhat))\n",
        "print(train_time)\n",
        "print(test_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CcGUIqZqMkV"
      },
      "outputs": [],
      "source": [
        "#Classifier implementing the k-nearest neighbors vote.\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9QdD9wosEZK",
        "outputId": "52ee33d8-dca1-4194-ce6f-5126c7d5711e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=4)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take k=4\n",
        "k = 4\n",
        "#Train Model and Predict  \n",
        "neigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\n",
        "neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XavxndesJAD",
        "outputId": "5d99b2a9-e4f6-47bc-b5a2-1d75030cdc11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 1, 2, 1, 0, 0, 0, 0, 3, 3, 2, 0, 1, 3, 0, 0, 3, 1, 2, 2, 0, 2,\n",
              "       2, 1, 3, 3, 2, 2, 2, 1, 2, 0, 0, 2, 3, 0, 0, 3, 0, 0, 2, 0, 1, 1,\n",
              "       3, 0, 0, 3, 1, 0, 3, 3, 2, 0, 0, 2, 2, 3, 1, 3, 0, 1, 3, 3, 2, 2,\n",
              "       0, 1, 2, 2, 1, 2, 0, 0, 0, 3, 0, 0, 3, 1, 2, 1, 0, 3, 1, 2, 3, 2,\n",
              "       2, 2, 0, 1, 3, 1, 3, 2, 1, 2, 2, 0, 0, 3, 2, 1, 0, 1, 0, 0, 2, 1,\n",
              "       2, 3, 0, 0, 1, 2, 2, 2, 3, 2, 0, 1, 3, 1, 3, 2, 2, 0, 0, 2, 1, 0,\n",
              "       3, 0, 1, 0, 3, 2, 1, 2, 1, 3, 3, 0, 2, 1, 0, 2, 0, 0, 1, 3, 3, 3,\n",
              "       0, 1, 1, 2, 3, 2, 2, 3, 1, 1, 3, 1, 2, 1, 0, 3, 2, 1, 2, 1, 1, 0,\n",
              "       3, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 1, 0, 2, 1, 0, 0, 3, 2, 0,\n",
              "       0, 0, 1, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 3, 0, 3, 1, 0, 2, 0, 0, 1,\n",
              "       0, 3, 1, 3, 2, 2, 3, 1, 3, 1, 3, 3, 2, 3, 2, 2, 1, 0, 0, 0, 3, 2,\n",
              "       3, 0, 3, 2, 0, 3, 0, 2, 3, 1, 2, 0, 0, 2, 0, 0, 3, 2, 1, 2, 0, 0,\n",
              "       0, 3, 2, 2, 1, 0, 2, 3, 3, 2, 0, 0, 1, 3, 3, 0, 1, 0, 2, 2, 2, 2,\n",
              "       2, 2, 0, 3, 0, 0, 0, 0, 2, 0, 3, 1, 3, 0])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yhat = neigh.predict(x_test)\n",
        "yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJspbUKNsNtQ",
        "outputId": "ad268055-70df-410e-cb6c-41fa41fd4b5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 2, 2, 1, 2, 1, 0, 1, 3, 3, 3, 0, 0, 3, 0, 1, 3, 1, 2, 2, 0, 0,\n",
              "       2, 1, 1, 3, 2, 2, 2, 1, 2, 0, 1, 1, 0, 0, 1, 3, 0, 3, 0, 1, 3, 0,\n",
              "       3, 0, 0, 3, 1, 0, 3, 3, 0, 1, 2, 3, 2, 2, 1, 2, 1, 2, 3, 0, 2, 1,\n",
              "       0, 0, 0, 1, 3, 1, 0, 2, 0, 3, 3, 0, 3, 1, 1, 3, 0, 3, 3, 3, 3, 2,\n",
              "       2, 3, 0, 3, 3, 2, 3, 2, 1, 3, 3, 0, 0, 3, 2, 1, 3, 1, 0, 1, 2, 1,\n",
              "       2, 3, 0, 3, 0, 2, 2, 1, 3, 3, 0, 1, 3, 1, 1, 2, 3, 2, 1, 2, 1, 0,\n",
              "       3, 3, 1, 1, 3, 2, 1, 2, 2, 2, 3, 2, 3, 0, 1, 2, 3, 0, 1, 3, 2, 3,\n",
              "       0, 1, 1, 3, 3, 3, 2, 2, 3, 2, 3, 1, 2, 0, 0, 3, 2, 1, 3, 1, 0, 0,\n",
              "       3, 1, 1, 2, 0, 1, 1, 1, 0, 1, 2, 2, 0, 3, 0, 1, 1, 3, 0, 3, 3, 0,\n",
              "       0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 2, 1, 3, 2, 3, 0, 1, 1, 1, 0, 2,\n",
              "       0, 3, 0, 3, 2, 1, 3, 3, 3, 1, 3, 0, 2, 3, 2, 2, 1, 0, 0, 3, 3, 2,\n",
              "       3, 0, 0, 3, 1, 3, 0, 2, 3, 1, 2, 2, 0, 3, 0, 0, 3, 2, 3, 2, 0, 0,\n",
              "       0, 1, 2, 2, 1, 0, 2, 3, 3, 2, 0, 0, 1, 3, 1, 1, 1, 0, 2, 2, 2, 2,\n",
              "       2, 2, 1, 3, 0, 0, 3, 0, 2, 0, 3, 1, 1, 0])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04vfaFeUsQIU",
        "outputId": "f4c9d36b-3439-47bd-e966-a508399a8a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set Accuracy:  0.7494117647058823\n",
            "Test set Accuracy:  0.64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#accuracy\n",
        "from sklearn import metrics\n",
        "print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(x_train)))\n",
        "print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9xET-LMseok"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RF_GLCM_120 Wrapper method Forward selection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}